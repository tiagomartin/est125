{
  "hash": "34b6c26f20c03fc242f69c23bb7692cd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Análise Preditiva\"\nformat: \n  revealjs:\n    width: 1600\n    height: 900\n    footer: \"\"\n    theme: quartomonothemer.scss\n    slide-number: c/t\n    show-slide-number: all\nincremental: false\ncode-link: true\nbibliography: references.bib\ntitle-slide-attributes:\n    data-background-image: /images/back.jpeg\n    data-background-size: cover\n    data-background-opacity: \"0.3\"\n---\n\n\n# Machine Learning\n\n## O que é Inteligência Artificial (IA)?\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n<br>\n\n![](/images/IA.png){fig.align=\"right\" width=120%}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\n<p align=\"center\"> \nA capacidade de um **sistema computacional** simular **habilidades cognitivas humanas** \n</p>\n\n<br>\n\n\n- Visão Computacional\n\n- Processamento de Linguagem Natural\n\n- Robótica \n\n- Machine Learning\n:::\n::::\n\n\n## O que é Inteligência Artificial (IA)?\n\n<br>\n\n![](/images/linha_IA.png){fig.align=\"center\" width=120%}\n\n\n## O que é Machine Learning?\n\n<p align=\"center\">\nUm subcampo da IA que permite que sistemas aprendam a partir de dados, sem serem explicitamente programados\n</p>\n\n<br>\n\n![](/images/PT_ML.png){fig.align=\"center\" width=80%}\n\n## O que é Machine Learning?\n\n:::: {.columns}\n\n::: {.column width=\"20%\"}\n<span style='font-size:200px;'>&#128202;</span> \n:::\n\n::: {.column width=\"80%\"}\n<p align=\"center\">\nDo ponto de vista da **Estatística**, Machine Learning (ML) é uma extensão e uma aplicação computacional de métodos estatísticos com um forte **foco em predição** e **descoberta de padrões em dados**, muitas vezes em larga escala e com **menor ênfase na inferência causal tradicional**.\n</p>\n:::\n\n::::\n\n\n. . .\n\n\n:::: {.columns}\n\n::: {.column width=\"20%\"}\n<span style='font-size:200px;'>&#128187;</span> \n:::\n\n::: {.column width=\"80%\"}\n<p align=\"center\">\nDo ponto de vista da **Ciência da Computação**, Machine Learning (ML) é um campo que se concentra no **desenvolvimento de algoritmos** e sistemas computacionais que podem **aprender a partir de dados** para realizar tarefas **sem serem explicitamente programados** para cada uma delas.\n</p>\n:::\n\n::::\n\n\n## O que é Machine Learning?\n\n![](/images/est_cc_ML.png){fig.align=\"center\"}\n\n## As duas culturas...\n\n<br>\n\n- **Data Modeling Culture:** Domina a comunidade estatística. Nela se **assume que o modelo utilizado é correto**. Testar suposições é fundamental. **Foco em inferência e na interpretação dos parâmetros**.\n\n. . .\n\n- **Algorithmic Modeling Culture:** Domina a comunidade de machine learning. Nela **não se assume que o modelo utilizado é correto**; **o modelo é utilizado apenas para criar bons algoritmos preditivos**. Podemos interpretar os resultados, mas esse, em geral, não é o foco.\n\n\n## Exemplos práticos de aplicações de ML no dia a dia.\n\n<br>\n\n\n-\tSistemas de recomendação (Netflix, Amazon).\n-\tFiltros de spam (Gmail).\n-\tCarros autônomos (Tesla).\n-\tDiagnóstico médico (detecção de tumores).\n-\tReconhecimento facial (smartphones).\n\n\n## Como as Máquinas Aprendem? \n\n<br>\n\n::: {#fig layout-ncol=2}\n\n![](/images/bike.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n\n![](/images/aprendizadoRN.gif){.nostretch fig-align=\"center\" width=\"1200px\"}\n:::\n\n\n## Componentes do aprendizado de máquina\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n![https://vas3k.com/blog/machine_learning/](/images/fig07.png){.nostretch fig-align=\"center\" width=\"700px\"}\n:::\n\n::: {.column width=\"40%\"}\n\n<br>\n\n<p></p>\n\n- Dados\n\n- Características/Features\n\n- Algoritmos\n\n:::\n\n::::\n\n## O mapa da aprendizagem de máquina\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n![](/images/fig02a.png){fig.align=\"center\"}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\n<p align=\"center\">\n&#128073; Nunca há uma única maneira de resolver um problema no mundo do aprendizado de máquina. \n\n&#128073; Sempre existem vários algoritmos que se encaixam, e você deve escolher qual deles se encaixa melhor. \n\n\n&#128073; Tudo pode ser resolvido com uma rede neural? Sim, mas quem pagará por todo esse custo?\n</p>\n\n\n\n:::\n\n::::\n\n\n## O mapa da aprendizagem de máquina\n\n![](/images/fig03a.png){.nostretch fig-align=\"center\" width=\"1200px\"}\n\n\n\n## Aprendizado de Máquina Clássico\n\n\n![](/images/tipos_aprend.png){.nostretch fig-align=\"center\" width=\"1150px\"}\n\n\n\n## Aprendizado de Máquina Clássico\n\n\n\n&#128073; O aprendizado de máquina clássico é frequentemente dividido em duas categorias – **Aprendizado Supervisionado** e **Não Supervisionado**.\n\n. . .\n\n\n- **Aprendizado Supervisionado:** usa um algoritmo que precisa de exemplos rotulados para desempenhar suas tarefas. \n\n\n. . .\n\n\n- **Aprendizado Não-supervisionado:** os dados não são rotulados, não há professor e a máquina está tentando encontrar padrões por conta própria.\n\n\n\n## Aprendizado Supervisionado\n\n<br>\n\n<span style='font-size:80px;'>&#128161;</span> Claramente, a máquina aprenderá mais rápido com um professor. Por isso, é mais comum encontrarmos esse caso nas tarefas da vida real.\n\n. . .\n\n\n- Existem dois tipos de tarefas: \n\n    - **classificação:** predição de categoria de um objeto e \n    - **regressão:** predição de um ponto específico em um eixo numérico.\n    \n    \n## Tarefa de classificação\n\n<p align=\"center\">\nOs algoritmos de classificação dividem os objetos com base em um dos atributos conhecidos de antemão. \n</p>\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n<br>\n\n- Usados nos dias de hoje para:\n\n  - Filtragem de spam;\n  - Detecção de idioma;\n  - Pesquisa por documentos semelhantes;\n  - Análise de sentimentos;\n  - Reconhecimento de caracteres;\n  - Detecção de fraude.\n\n:::\n\n::: {.column width=\"40%\"}\n\n![](/images/fig09.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n::::\n\n\n## Tarefa de classificação\n\n<br>\n<br>\n\n\n<p align=\"center\">\nAlgoritmos populares: **Naive Bayes**, **Decision Tree**, **Logistic Regression**, **K-Nearest Neighbours**, **Support Vector Machine**.\n</p>\n\n\n## Tarefa de regressão\n\n<p align=\"center\">\nSe a variável resposta é quantitativa, temos um problema de análise de regressão \n</p>\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n<br>\n\n- Usados nos dias de hoje para:\n\n  - Previsões do preço das ações;\n  - Análise de demanda e volume de vendas;\n  - Diagnóstico médico.\n\n:::\n\n::: {.column width=\"40%\"}\n\n![](/images/fig10.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n::::\n\n## Tarefa de regressão\n\n<br>\n<br>\n\n\n<p align=\"center\">\nAlgoritmos populares: **Decision Tree**, **Regressão Linear** e **Regressão Polinomial**, **K-Nearest Neighbours**, **Support Vector Machine**.\n</p>\n\n\n\n\n## Avaliação de modelos\n\n\n<span style='font-size:80px;'>&#128161;</span>  Independente do modelo escolhido, é importante saber se um modelo de machine learning está realmente funcionando. É aí que entra a **avaliação de modelos**!\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](/images/detetive.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\n<p align=\"center\">\nA **avaliação de modelos** de machine learning é como um **detetive investigando um caso**.\n</p>\n\n:::\n::::\n\n\n## Avaliação de modelos\n\n<br>\n\n<br>\n\n::: {layout-ncol=2}\n\n![](/images/over_under.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/erro_over_under.png){.fragment fig-align=\"center\" width=\"1000px\"}\n\n\n:::\n\n\n\n## Avaliação de modelos\n\n\n\n<p align=\"center\" >\n<span style='font-size:70px;'>&#129300;</span> O nosso modelo é um **herói** ou um **impostor**?\n</p>\n\n![](https://media4.giphy.com/media/ek4CUx2FONgHaMz9V5/giphy-downsized-medium.gif){fig-align=\"center\" width=\"1000px\"}\n\n\n\n\n## Matriz de confusão\n\n\nPermite a visualização do **desempenho** de um **algoritmo de classificação**\n\n&nbsp;\n\n![](/images/mc.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n## Matriz de confusão\n\n<br>\n\n- **VP (Verdadeiro Positivo):** objeto da classe positiva classificado como positivo \n\n. . .\n\n\n- **VN (Verdadeiro Negativo):** objeto da classe negativa classificado como negativo\n\n. . .\n\n\n- **FP (Falso Positivo):** objeto da classe negativa classificado como positivo. Também conhecido como **alarme falso** ou **Erro tipo 1**\n\n\n. . .\n\n\n- **FN (Falso Negativo):** objeto da classe positiva classificado como negativo. É também conhecido como **Erro Tipo 2**\n\n\n## Matriz de confusão\n\n<br> \n\n**Exemplo:** Sejam as seguintes matrizes de confusão, obtidas de dois classificadores quaisquer.\n\n<br>\n\n::: {layout-ncol=2}\n\n![](/images/mc_cliente.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/mc_paciente.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n\n:::\n\n\n# Métricas derivadas da matriz de confusão\n\n## Acurácia\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](/images/acc.jpg){.nostretch fig-align=\"center\" width=\"400px\"}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\n\n<p align=\"center\">\nMede a **proporção de previsões corretas** do modelo em relação ao total de previsões feitas.\n</p>\n\n:::\n::::\n\n\n<p align=\"center\">\n**É como sua nota em uma prova!**\n</p>\n\n\n## Acurácia\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](/images/mc.png){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n![](/images/acc_form.jpg){.nostretch fig-align=\"center\" width=\"800px\"}\n\n:::\n::::\n\n\nA **Taxa de Erro Aparente** do classificador é dada por\n\n$$TEA = 1 - ACC$$\n\n\n\n## Acurácia\n\n::: {layout-nrow=2}\n\n![](/images/mc_cliente_acc.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/acc_tea_cliente.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/mc_paciente_acc.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/acc_tea_paciente.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n\n\n## Acurácia\n\n\n<br>\n\n<br>\n\n\n<p align=\"center\">\nMas será que a **acurácia** é suficiente para avaliar nossos modelos de forma **precisa**? \n</p>\n\n\n. . .\n\n\n<p align=\"center\">\nÀs vezes, uma **única métrica** não é capaz de nos contar toda a história.\n</p>\n\n\n\n## Precisão\n\n<br>\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](/images/precision.jpg){.nostretch fig-align=\"center\" width=\"400px\"}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\n\n<p align=\"center\">\nEla nos diz quantas das **previsões positivas** foram realmente **corretas**.\n</p>\n\n:::\n::::\n\n\n## Precisão\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](/images/mc.png){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n![](/images/precision_form.jpg){fig-align=\"center\" width=\"800px\"}\n\n:::\n::::\n\n\n\n<p align=\"center\">\nPorcentagem de verdadeiros positivos dentre todos os objetos classificados como positivos\n</p>\n\n\n\n## Precisão\n\n\n::: {layout-nrow=2 layout-valign=\"bottom\"}\n\n![](/images/mc_cliente_prec.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/prec_form_clientes.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/mc_paciente_prec.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/prec_form_pacientes.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n\n\n\n## Sensibilidade\n\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](/images/recall.jpg){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n<p align=\"center\">\nMede a **proporção de casos positivos reais** que foram encontrados pelo modelo \n</p>\n\n\n:::\n::::\n\n## Sensibilidade\n\n<br>\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](/images/mc.png){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n![](/images/sens_form.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n::::\n\n<p align=\"center\">\nTambém conhecida por **Recall** ou **Taxa de Verdadeiros Positivos (TVP)**\n</p>\n\n## Sensibilidade\n\n\n::: {layout-nrow=2 layout-valign=\"bottom\"}\n\n![](/images/mc_cliente_sens.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/sens_form_clientes.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/mc_paciente_sens.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/sens_form_pacientes.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n\n\n## Especificidade\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](/images/espec.jpg){.nostretch fig-align=\"center\" width=\"600px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<br>\n\n<p align=\"center\">\nAjuda a identificar a capacidade do modelo em reconhecer **corretamente as amostras negativas**   \n</p>\n\n\n:::\n::::\n\n\n## Especificidade\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](/images/mc.png){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n![](/images/espec_form.jpg){.nostretch fig-align=\"center\" width=\"800px\"}\n\n:::\n::::\n\n<p align=\"center\">\nTambém conhecida por **Taxa de Verdadeiros Negativos (TVN)**\n</p>\n\n\n## Especificidade\n\n::: {layout-nrow=2 layout-valign=\"bottom\"}\n\n![](/images/mc_cliente_espec.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/espec_form_clientes.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/mc_paciente_espec.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/espec_form_pacientes.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n\n\n## Taxa de Falso Positivo\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n![](/images/fpr.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<br>\n\n<p align=\"center\">\nEla mede a **proporção de amostras negativas** classificadas como positivas pelo modelo  \n</p>\n\n\n:::\n::::\n\n\n## Taxa de Falso Positivo\n\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](/images/mc.png){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n![](/images/fpr_form.jpg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n:::\n::::\n\n\n## Taxa de Falso Positivo\n\n::: {layout-nrow=2 layout-valign=\"bottom\"}\n\n![](/images/mc_cliente_espec.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/fpr_form_clientes.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/mc_paciente_espec.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n![](/images/fpr_form_pacientes.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n\n\n## $F_1$-Score\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n![](/images/f1.jpg){.nostretch fig-align=\"center\" width=\"500px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<br>\n\n<br>\n\n<p align=\"center\">\n Ele leva em consideração tanto **precisão** quanto a **sensibilidade**, dando uma medida balanceada do desempenho do modelo.  \n</p>\n\n\n:::\n::::\n\n\n## $F_1$-Score\n\n<br>\n\n<br>\n\n$$F_1 \\text{-score} = 2 \\times \\dfrac{\\text{Precisão} \\times \\text{Sensibilidade}}{\\text{Precisão} + \\text{Sensibilidade}}$$\n\n<br>\n\n<p align=\"center\">\nO $F_1$-Score é como um **equilibrista** em uma corda bamba.\n</p>\n\n\n## Curva ROC\n\n- A **Curva ROC** é como um mapa que nos guia pela **sensibilidade** e pelos **falsos positivos** do modelo em diferentes configurações. \n\n. . .\n\n- Ela nos mostra o quão bem nosso modelo pode **distinguir** entre as classes.\n\n\n![](/images/roc.jpg){.fragment fig-align=\"center\" width=\"700px\"}\n\n\n## Curva ROC\n\n\n- Representa o número de vezes que o classificador **acertou a predição** contra o número de vezes que o classificador **errou a predição**\n\n. . .\n\n\n- A área sob a curva ROC, conhecida como **AUC-ROC**, é uma métrica comumente utilizada para avaliar o desempenho global do modelo. \n\n. . . \n\n- Quanto **maior a AUC-ROC**, **melhor** é o desempenho do modelo em discriminar corretamente as classes.\n\n![](/images/auc.jpg){.fragment fig-align=\"center\" width=\"1400px\"}\n\n\n# E como avaliar modelos de predição?\n\n## Avaliação de modelos de predição\n\n<br>\n\n- Seja $d_j$, $j = 1,\\cdots, n$, a resposta desejada para o objeto $j$ e $y_j$ a resposta estimada (predita) do algoritmo, obtida a partir de uma entrada $\\mathbf{x_j}$ apresentada ao algoritmo.\n\n. . .\n\n- Seja então, $e_j = d_j - y_j$ a diferença entre o valor observado e o valor predito para o objeto $j$.\n\n\n. . .\n\n- Podemos definir as seguintes métricas para **avaliação de modelos preditivos.**\n\n## Avaliação de modelos de predição\n\n**Erro Quadrático Médio (MSE - Mean Squared Error):**\n\n$$MSE =\\dfrac{1}{n} \\displaystyle{\\sum_{j=1}^n e_j^2}$$\n\n - **Ponto forte:** Penaliza fortemente erros maiores devido ao termo quadrático. Isso significa que o MSE é sensível a outliers (valores discrepantes).\n\n\n - **Ponto fraco:** Como eleva os erros ao quadrado, a unidade da métrica resultante não é a mesma da variável original, o que dificulta a interpretação direta da magnitude do erro.\n\n\n## Avaliação de modelos de predição\n\n**Raiz do Erro Quadrático Médio (RMSE - Root Mean Squared Error):**\n\n$$RMSE =\\sqrt{\\dfrac{1}{n} \\displaystyle{\\sum_{j=1}^n e_j^2}}$$\n\n\n - **Ponto forte:** Mantém a propriedade de penalizar erros maiores, mas retorna o erro na mesma unidade da variável original, facilitando a interpretação. É uma métrica amplamente utilizada.\n\n\n - **Ponto fraco:** Ainda é sensível a outliers, pois se baseia no MSE.\n\n\n\n## Avaliação de modelos de predição\n\n**Erro Médio Absoluto (MAE - Mean Absolute Error):**\n\n$$MAE = \\dfrac{1}{n} \\displaystyle{\\sum_{j=1}^n |e_j|}$$\n\n - **Ponto forte:** É mais robusto a outliers em comparação com MSE e RMSE, pois não eleva os erros ao quadrado. Fornece uma medida direta da magnitude média dos erros na unidade original da variável.\n\n\n - **Ponto fraco:** Não penaliza erros maiores de forma tão intensa quanto o MSE e RMSE. Pode não ser ideal se erros grandes tiverem um impacto significativamente maior no seu problema.\n\n\n\n\n## Avaliação de modelos de predição\n\n**Erro Percentual Absoluto Médio (MAPE - Mean Absolute Percentage Error):**\n\n$$MAPE = \\dfrac{1}{n} \\displaystyle{\\sum_{j=1}^n |e_j/d_j|}\\times 100$$\n\n - **Ponto forte:** É fácil de interpretar, pois expressa o erro em termos percentuais. Isso pode ser útil para comparar o desempenho de modelos em diferentes escalas.\n\n\n - **Ponto fraco:** Não é definido quando os valores reais são zero. Além disso, pode ser assimétrico, penalizando mais os erros de previsão abaixo do valor real do que acima. Pode ser instável se houver valores reais muito pequenos.\n\n\n## Avaliação de modelos de predição\n\n<p align=\"center\">\n<span style='font-size:70px;'>&#129300;</span> Qual a melhor métrica?\n</p>\n\n\n\n**Em resumo:**\n\n\n- Se você se preocupa muito com grandes erros: MSE e RMSE são boas opções.\n\n. . .\n\n- Se você quer uma métrica robusta a outliers: MAE é uma boa escolha.\n\n. . .\n\n- Se a interpretabilidade em termos percentuais é importante (com cuidado com valores zero/pequenos): MAPE pode ser útil.\n\n. . .\n\n\n<p align=\"center\">\n<span style='font-size:60px;'>&#128161;</span> O ideal é analisar todas as métricas em conjunto, considerando o contexto do seu problema e utilizando validação cruzada.\n</p>\n\n\n## Qual o melhor modelo?\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n<span style='font-size:70px;'>&#128073;</span> Suponha que tenhamos dados simulados utilizando o seguinte modelo:\n\n![](/images/predicao/fig01.png){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n![](/images/predicao/fig02.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n\n:::\n::::\n\n\n## Qual o melhor modelo?\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n<span style='font-size:70px;'>&#128073;</span> Podemos estimar diversos modelos $y$ que predizem o verdadeiro valor de $d$\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](/images/predicao/fig03.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n\n:::\n::::\n\n\n\n## Qual o melhor modelo?\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n<span style='font-size:70px;'>&#128073;</span> Nosso interesse está em treinar o modelo e avaliar a sua capacidade de generalização\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](/images/predicao/fig04.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n\n:::\n::::\n\n\n\n## Validação holdout\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n<span style='font-size:70px;'>&#128073;</span> **Dados originais:** treinamento e teste\n\n\n<span style='font-size:70px;'>&#128073;</span> **Dados de treinamento:** treinamento e validação\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](/images/predicao/houlout.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n\n:::\n::::\n\n\n\n\n## Validação holdout\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n**Exemplo:** Vamos avaliar a relação entre Frequência Cardíaca e Idade de 270 pacientes\n\n\n![](/images/predicao/freq.png){.nostretch fig-align=\"center\" width=\"600px\"}\n\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n![](/images/predicao/exemplo.png){.nostretch fig-align=\"center\" width=\"1200px\"}\n\n\n:::\n::::\n\n\n\n## Validação holdout\n\n<p align=\"center\">\n<span style='font-size:60px;'>&#129300;</span> Qual o melhor modelo nesse caso?\n</p>\n\n\n![](/images/predicao/fig05.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\n\n## Validação holdout\n\n\n<p align=\"center\">\n<span style='font-size:60px;'>&#128073;</span> 70% da base para treino e 30% para validação\n</p>\n\n::: {layout-ncol=2}\n![](/images/predicao/fig06.png){.nostretch fig-align=\"center\" width=\"900px\"}\n\n![](/images/predicao/fig07.png){.nostretch fig-align=\"center\" width=\"900px\"}\n:::\n\n\n\n\n## Validação cruzada k-fold\n\n<span style='font-size:70px;'>&#128073;</span> **Dados de treinamento:** $k$ partes iguais. **Treina** com $k-1$ partes, e **valida** com uma\n\n![](/images/predicao/cross_validation.png){.nostretch fig-align=\"center\" width=\"1400px\"}\n\n\n\n## Validação cruzada k-fold\n\n<br>\n\n::: {layout-ncol=2}\n![](/images/predicao/fig08.png){.nostretch fig-align=\"center\" width=\"900px\"}\n\n![](/images/predicao/fig09.png){.nostretch fig-align=\"center\" width=\"900px\"}\n:::\n\n\n## Trade-off bias-variância\n\n\nImagine que você está tentando **acertar** um alvo com dardos. Seus arremessos podem ser agrupados de três maneiras diferentes:\n\n\n- **Grupo de alto viés (bias):** Seus arremessos são **consistentemente agrupados longe do alvo**, mas **próximos uns dos outros**. Isso indica um **alto viés**, pois você está fazendo arremessos incorretos, mas de forma **consistente**.\n\n\n. . .\n\n- **Grupo de alta variância:** Seus arremessos estão **espalhados por toda a área**, **longe do alvo e uns dos outros**. Isso indica **alta variância**, pois seus arremessos são **inconsistentes e imprevisíveis**. \n\n\n\n. . .\n\n\n- **Grupo equilibrado:** Seus arremessos estão **agrupados próximo ao alvo** e também estão **próximos uns dos outros**. Isso é o **equilíbrio** entre **viés** e **variância**, onde você está acertando o alvo de forma **consistente** e **precisa**.\n\n\n\n## Trade-off bias-variância\n\n![](/images/predicao/bias_variance.png){.nostretch fig-align=\"center\" width=\"900px\"}\n\n\n## Trade-off bias-variância\n\n\n<br>\n\n\nQueremos construir modelos que não apenas performem bem nos dados de treinamento, mas que também façam previsões precisas em dados novos e não vistos.\n\n\n. . .\n\n\nO **erro total** de um **modelo preditivo** em dados não vistos pode ser **decomposto em três componentes principais**, ou seja,\n\n\n\n$$\\text{Erro Total} = \\text{Bias}^2 + \\text{Variância} + \\text{Erro Irredutível}$$\n\n\n\n## Trade-off bias-variância\n\n<br>\n\n- **Bias (Viés):** Erro devido a suposições simplificadoras no modelo. Um modelo com alto bias tende a subajustar (underfit) os dados de treinamento, perdendo relações importantes entre as variáveis.\n\n\n. . .\n\n\n- **Variância:** Sensibilidade do modelo a pequenas variações nos dados de treinamento. Um modelo com alta variância tende a sobreajustar (overfit) os dados de treinamento, aprendendo até mesmo o ruído presente neles.\n\n\n. . .\n\n- **Erro Irredutível:** Ruído inerente aos dados que não pode ser reduzido por nenhum modelo.\n\n\n## O Problema do Subajuste (Underfitting)\n\n<br>\n\nModelos com **alto bias** fazem suposições fortes sobre a forma da função que mapeia as entradas para as saídas. São tipicamente modelos mais simples, com poucos parâmetros.\n\n. . .\n\n\n- Consequências:\n\n  - Desempenho ruim tanto nos dados de treinamento quanto nos dados de teste.\n  - Incapacidade de capturar a complexidade real dos dados.\n  \n  \n## O Problema do Sobreajuste (Overfitting)\n\n<br>\n\nModelos com **alta variância** aprendem os dados de treinamento muito bem, incluindo o ruído presente neles. São tipicamente modelos mais complexos, com muitos parâmetros.\n\n\n- Consequências:\n\n  - Excelente desempenho nos dados de treinamento.\n  - Desempenho significativamente pior em dados de teste não vistos. O modelo \"decorou\" os dados de treinamento em vez de aprender padrões gerais.\n\n\n## Underfitting e Overfitting\n\n<br>\n\n![](/images/predicao/over_under_good.png){.nostretch fig-align=\"center\" width=\"1800px\"}\n\n\n## Bias vs. Variância\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<br>\n\nVoltemos ao nosso modelo simulado:\n\n\n![](/images/predicao/fig01.png){.nostretch fig-align=\"center\" width=\"600px\"}\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](/images/predicao/fig02.png){.nostretch fig-align=\"center\" width=\"1200px\"}\n\n\n:::\n::::\n\n\n\n## Bias vs. Variância\n\n<p align=\"center\">\n<span style='font-size:60px;'>&#128073;</span> Vamos ajustar um modelo polinomial de **grau 2**\n</p>\n\n\n\n\n![](/images/predicao/fig12.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n\n## Bias vs. Variância\n\n::: {layout-ncol=2}\n![](/images/predicao/fig13.png){.nostretch fig-align=\"center\" width=\"900px\"}\n\n![](/images/predicao/fig14.png){.nostretch fig-align=\"center\" width=\"900px\"}\n:::\n\n\n\n\n\n## Bias vs. Variância\n\n<p align=\"center\">\n<span style='font-size:60px;'>&#128073;</span> Vamos ajustar um modelo polinomial de **grau 10**\n</p>\n\n\n\n\n![](/images/predicao/fig15.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n\n## Bias vs. Variância\n\n::: {layout-ncol=2}\n![](/images/predicao/fig16.png){.nostretch fig-align=\"center\" width=\"900px\"}\n\n![](/images/predicao/fig17.png){.nostretch fig-align=\"center\" width=\"900px\"}\n:::\n\n\n\n\n\n\n\n\n\n\n## O Tradeoff - A Balança Delicada\n\n<span style='font-size:80px;'>&#128073;</span> **Bias** e **variância** estão **inversamente relacionados**. Tentar reduzir um geralmente aumenta o outro.\n\n\n. . .\n\n- **Modelos simples** tendem a ter **alto bias e baixa variância**.\n\n- **Modelos complexos** tendem a ter **baixo bias e alta variância**.\n\n\n. . .\n\n<span style='font-size:80px;'>&#128161;</span> O objetivo é encontrar um modelo com um **bom equilíbrio** entre **bias** e **variância**, que **minimize o erro de generalização**.\n\n\n\n## O Tradeoff - A Balança Delicada\n\n<br>\n\n![](/images/predicao/bias-variance_tradeOff.png){.nostretch fig-align=\"center\" width=\"1800px\"}\n\n## Como Lidar com o Tradeoff?\n\n<br>\n\n- **Seleção de Modelos:** Experimentar diferentes tipos de modelos (lineares, não lineares, árvores, redes neurais, etc.).\n\n. . .\n\n- **Ajuste de Hiperparâmetros:**  Controlar a complexidade do modelo ajustando seus hiperparâmetros (ex: profundidade máxima de uma árvore, número de neurônios em uma camada).\n\n. . .\n\n\n- **Validação Cruzada:** Usar técnicas de validação cruzada para estimar o desempenho do modelo em dados não vistos de forma mais robusta e ajudar a identificar overfitting.\n\n. . .\n\n- **Engenharia de Features:** Criar features mais informativas pode reduzir o bias.\n\n\n## Como Lidar com o Tradeoff?\n\n<br>\n\n- **Regularização:** Técnicas como L1 e L2 adicionam uma penalidade à complexidade do modelo, ajudando a reduzir a variância (overfitting).\n\n\n. . .\n\n\n- **Mais Dados:** Em alguns casos, aumentar a quantidade de dados de treinamento pode ajudar a reduzir a variância.\n\n. . .\n\n\n- **Ensemble Methods:** Combinar múltiplos modelos (ex: Random Forest, Gradient Boosting) pode ajudar a reduzir tanto o bias quanto a variância.\n\n\n\n# Algoritmos de predição\n\n## Algoritmos de predição\n\n<br>\n\n- Algoritmos mais comuns usados para problemas de predição:\n\n  - K-Nearest Neighbors (KNN)\n  - Naive Bayes\n  - Árvores de decisão\n  - Random Forests\n  - Máquinas de Vetores de Suporte (SVM)\n  - Redes Neurais Artificiais (ANN)\n  \n\n##  K-Nearest Neighbors (KNN) \n\n<br>\n\nO KNN é um algoritmo de aprendizado supervisionado de classificação e regressão, que usa a proximidade dos objetos para classificar novas instâncias.\n\n\n. . .\n\nÉ um dos algoritmos mais simples e intuitivos de aprendizado de máquina. Utiliza a ideia do **vizinho mais próximo**, o que significa que ele determina a classe de uma instância com base nas classes de seus vizinhos mais próximos.\n\n. . .\n\nEm outras palavras, se a maioria dos vizinhos mais próximos de uma instância pertence a uma classe específica, então a instância também é classificada como pertencente a essa classe.\n\n\n## Algoritmo KNN\n\n![](/images/knn/knn.png){.nostretch fig-align=\"center\" width=\"900px\"}\n\n\n##  Algoritmo KNN\n\n<br>\n\nKNN é usado para problemas de classificação e regressão.\n\n\n. . .\n\n\n  - Em problemas de **classificação**, a classe mais comum entre os K vizinhos mais próximos é escolhida como a classe da nova instância.\n\n. . .\n\n  - Em problemas de **regressão**, a média ou mediana dos valores alvo dos K vizinhos mais próximos é escolhida como o valor alvo da nova instância.\n\n\n## A escolha do valor de K\n\n<br>\n\nO valor de K é um **parâmetro** importante em KNN. Ele determina o **número de vizinhos mais próximos** que são usados para classificar uma nova instância.\n\n. . .\n\n\nUm valor de K pequeno pode levar a um modelo muito **sensível** ao **ruído** nos dados (overfitting), enquanto um valor grande de K pode levar a uma **perda** de detalhes importantes nos dados (underfitting).\n\n\n. . .\n\nA escolha do valor K ideal é frequentemente realizada por meio de técnicas de **validação cruzada**. \n\n. . .\n\n\nK ímpar evita empates.\n\n\n\n  \n\n\n## Métricas de Distância\n\n- **Distância Euclidiana:** A mais comum. \n\n$$d_{ij} = \\displaystyle{\\sqrt{(\\mathbf{x}_i - \\mathbf{x}_j)^t(\\mathbf{x}_i - \\mathbf{x}_j)}} = \\sqrt{\\displaystyle{\\sum_{k=1}^p(x_{ik} - x_{jk})^2}}$$\n\n- **Distância de Minkowski:** Soma das diferenças absolutas entre as coordenadas. \n\n$$d_{ij} = \\left( \\displaystyle{\\sum_{k=1}^P} |X_{ik} - X_{jk}|^{\\lambda}\\right)^{\\frac{1}{\\lambda}}$$\n\n## Métricas de Distância\n\n<br>\n\n- Para a distância de Minkowski:\n\n  - Se $\\lambda = 1$, temos a chamada **métrica de Manhattan**. É também conhecida como *city block*.  \n  - Se $\\lambda = 2$, temos a distância euclidiana.\n  - A métrica de Minkowski é menos afetada pela presença de valores discrepantes na amostra do que a distância Euclidiana.\n\n. . .\n\n\n- Se os dados forem textuais: usar cosseno\n\n\n## Vantagens e Desvantagens\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n<span style='font-size:130px;'>&#128077;</span>\n:::\n\n::: {.column width=\"90%\"}\n  -\tSimples de entender e implementar.\n  -\tNão faz muitas suposições sobre os dados (não paramétrico).\n  -\tÚtil para dados complexos e não lineares.\n  -\tPode ser usado tanto para classificação quanto para regressão.\n:::\n\n::::\n\n## Vantagens e Desvantagens\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<span style='font-size:130px;'>&#128078;</span>\n:::\n\n::: {.column width=\"90%\"}\n  -\tComputacionalmente caro para grandes conjuntos de dados (cálculo de distância para todos os pontos).\n  -\tSensível à escala dos dados (variáveis com escalas maiores podem dominar o cálculo da distância) - importância da normalização/padronização.\n  -\tDesempenho pode degradar em dados com muitas dimensões (maldição da dimensionalidade).\n  - Escolha do valor de k pode ser crucial e não é trivial.\n\n:::\n\n::::\n\n\n\n## Pré-processamento de Dados para KNN\n\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<span style='font-size:130px;'>&#128187;</span>\n:::\n\n::: {.column width=\"90%\"}\n  -\t**Escalonamento de features:** Padronização (média zero, desvio padrão um) ou normalização (escala entre 0 e 1) para garantir que todas as features contribuam igualmente para o cálculo da distância.\n  -\t**Tratamento de valores ausentes:** Imputação ou remoção.\n  - **Seleção de features:** Reduzir a dimensionalidade para melhorar o desempenho.\n\n\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador\n\n\n![](/images/knn/tabela.jpg){.nostretch fig-align=\"center\" width=\"1300px\"}\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](/images/joao.png){.nostretch fig-align=\"center\" width=\"1300px\"}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\nJoão possui as seguintes características\n\n- menos de 30 anos\n- renda média\n- é estudante\n- possuí um bom crédito na praça!\n\n\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador \n\n<p align=\"center\">\nJoão **compraria** ou **não compraria** o computador?\n</p>\n\n\n![](https://media2.giphy.com/media/XeH1MFu4x3etVsllUN/giphy.gif){.nostretch fig-align=\"center\" width=\"1100px\"}  \n\n\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](/images/joao.png){.nostretch fig-align=\"center\" width=\"1300px\"}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\n<p align=\"center\" style=\"font-size: 76px\">**Quem é o João?**</p>\n\n&nbsp;\n&nbsp;\n\n::: {.fragment}\n<p align=\"center\" style=\"font-size: 56px\">$x_0 = (\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Bom})$</p>\n:::\n\n\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador \n\n<p align=\"center\">\nUsando o classificador KNN com k = 5\n</p>\n\n\n![](/images/knn/tab_dist.jpg){.nostretch fig-align=\"center\" width=\"1100px\"}\n\n\n\n## Exemplo: Compra de um computador \n\n<br>\n\nTemos então que os **5 vizinhos** mais próximos a João são\n\n \\s\\s\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n<span style='font-size:130px;'>&#x274C;</span>\n:::\n\n::: {.column width=\"90%\"}\n\n \\s\\s\n\n- <p style=\"color: red\">$x_2 = \\{\\leq 30, \\text{ Alta}, \\text{ Sim}, \\text{ Bom}\\}$</p>\n- <p style=\"color: red\">$x_8 = \\{\\leq 30, \\text{Média}, \\text{ Não}, \\text{ Bom}\\}$</p>\n\n:::\n\n::::\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<span style='font-size:130px;'>&#x2705;</span>\n:::\n\n::: {.column width=\"90%\"}\n\n\n \n- <p style=\"color: green\">$x_9 = \\{\\leq 30, \\text{ Baixa}, \\text{ Sim}, \\text{ Bom}\\}$</p>\n- <p style=\"color: green\">$x_{10} = \\{> 40, \\text{ Média}, \\text{ Sim}, \\text{ Excelente}\\}$</p>\n- <p style=\"color: green\">$x_{11} = \\{\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Excelente}\\}$</p>\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador \n\n<br>\n\n<br>\n\n\n<details>\n  <summary align=\"center\" style='font-size:80px;'>**João comprará o computador?**</summary>\n  <p align=\"center\" style=\"color: green; font-size:80px;\"> **De acordo com o KNN: SIM!** </p>\n</details>\n\n\n## KNN para classificação no R e Python\n\n\n\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris_split <- initial_split(iris, strata = Species)\ntrain_data <- training(iris_split)\ntest_data <- testing(iris_split)\n\nknn_spec <- nearest_neighbor(neighbors = 5) %>% \n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\nknn_fit <- workflow() %>%\n  add_model(knn_spec) %>%\n  add_formula(Species ~ .) %>%\n  fit(data = train_data)\n\nsp_predict <- predict(knn_fit, test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.974359\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, stratify=iris.target, random_state=12345\n)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = knn.predict(X_test)\naccuracy_score(y_test, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9473684210526315\n```\n\n\n:::\n:::\n\n\n:::\n\n## KNN para regressão no R e Python\n\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split <- initial_split(mtcars, strata = mpg)\ntrain_data <- training(mtcars_split)\ntest_data <- testing(mtcars_split)\n\nknn_spec <- nearest_neighbor(neighbors = 5) %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"regression\")\n\nknn_fit <- workflow() %>%\n  add_model(knn_spec) %>%\n  add_formula(mpg ~ .) %>%\n  fit(data = train_data)\n\nmpg_predict <- predict(knn_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.069807\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = knn_reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.1408296168142682\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## Algoritmo Naive Bayes\n\n<br>\n\n\nBaseia-se no **Teorema de Bayes**, que afirma que a probabilidade de um evento ocorrer dado que outro evento já ocorreu é proporcional à probabilidade deste último evento ocorrer dado o primeiro.\n\n. . .\n\nA \"ingenuidade\" (Naive): assume que os preditores são independentes entre si, dado o valor da classe.\n\n. . . \n\nAplicado principalmente em problemas de classificação, especialmente com dados categóricos ou textuais.\n\n\n##  Revisão do Teorema de Bayes\n\nSejam A e B dois eventos\n\n$$P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)},\\,\\,\\,\\,\\,\\,\\, \\text{para } P(B)>0$$\n\n\n. . .\n\nO Teorema de Bayes Aplicado ao Naive Bayes:\n\n$$P(\\text{Classe}|\\text{Características}) = \\dfrac{P(\\text{Características}|\\text{Classe})P(\\text{Classe})}{P(\\text{Características})}$$\n\n\n. . . \n\n<p align=\"center\">\n<span style='font-size:80px;'>&#128073;</span>O objetivo é encontrar a classe com a **maior probabilidade posterior**.\n</p>\n\n\n## O Teorema de Bayes Aplicado ao Naive Bayes\n\nDevido a essa suposição de independência condicional, a probabilidade das características conjuntas dado a classe pode ser simplificada para o produto das probabilidades de cada característica individual dado a classe:\n\n$$P(X_1, X_2,\\cdots, X_n|\\text{Classe}) = P(X_1|\\text{Classe}) \\times P(X_2|\\text{Classe}) \\times \\cdots \\times P(X_n|\\text{Classe})$$\n\n. . .\n\nSubstituindo na fórmula de Bayes, nos leva a\n\n$$P(\\text{Classe}|\\text{Características}) \\propto P(\\text{Classe}) \\times \\prod_{i=1}^n P(X_i|\\text{Classe})$$\n\n\n**Observação importante:** O denominador $P(\\text{Características})$ é constante para todas as classes, então podemos ignorá-lo para fins de comparação\n\n\n## Tipos de Naive Bayes\n\n<br>\n\n- **Gaussian Naive Bayes:** Para características contínuas. Assume que as características são distribuídas de acordo com uma distribuição Gaussiana (Normal).\n\n. . . \n\n- **Multinomial Naive Bayes:** Ideal para contagens de ocorrências (ex: contagem de palavras em documentos, usado em classificação de texto).\n\n. . .\n\n- **Bernoulli Naive Bayes:** Adequado para dados binários (presença/ausência de uma característica).\n\n\n\n\n## Vantagens e Desvantagens\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<span style='font-size:160px;'>&#128077;</span>\n:::\n\n::: {.column width=\"90%\"}\n  -\t**Simplicidade:** Fácil de entender e implementar.\n  - **Eficiência:** Muito rápido para treinar e prever, especialmente em grandes conjuntos de dados.\n  - **Bom desempenho:** Surpreendentemente eficaz em muitos problemas reais, mesmo com a suposição de independência.\n  - **Processamento de texto:** Excelente para classificação de documentos e filtragem de spam.\n  - **Não requer muitos dados:** Pode funcionar bem com conjuntos de dados menores.\n:::\n\n::::\n\n## Vantagens e Desvantagens\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<span style='font-size:160px;'>&#128078;</span>\n:::\n\n::: {.column width=\"90%\"}\n  -\t**Suposição de independência:** A suposição de independência entre as características raramente é verdadeira no mundo real, o que pode limitar a precisão.\n  - **Problema de \"zero frequência\":** Se uma categoria de característica não aparecer no conjunto de treinamento para uma determinada classe, a probabilidade para essa característica será zero, o que leva a uma probabilidade posterior zero. (para resolver: Laplace Smoothing)\n  - **Estimativa de Probabilidades:** Pode ter um desempenho ruim quando há características numéricas com distribuições complexas (a suposição Gaussiana pode não se aplicar).\n\n:::\n\n::::\n\n\n\n## Pré-processamento de Dados para Naive Bayes\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<span style='font-size:160px;'>&#128187;</span>\n:::\n\n::: {.column width=\"90%\"}\n  -\t**Dados Categóricos:** Transformar para formato numérico (One-Hot Encoding ou Label Encoding, dependendo do tipo de NB).\n  - **Dados Numéricos:**\n    - **GaussianNB:** Assumir distribuição normal. Normalização/padronização pode ajudar, mas não é estritamente necessária como em KNN, já que não se baseia em distância.\n    - **Discretização:** Converter dados numéricos em categóricos (bins) para usar Multinomial ou Bernoulli NB.\n  - **Tratamento de Valores Ausentes:** Imputação ou remoção.\n  - **Textuais:** Tokenização, remoção de stop words, stemização/lema, TF-IDF, CountVectorizer.\n\n\n:::\n\n::::\n\n\n\n\n\n\n## Exemplo: Compra de um computador\n\n\n![](/images/knn/tabela.jpg){.nostretch fig-align=\"center\" width=\"1300px\"}\n\n\n\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](/images/joao.png){.nostretch fig-align=\"center\" width=\"1300px\"}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\n<p align=\"center\" style=\"font-size: 76px\">**Voltando ao Joãozinho...**</p>\n\n&nbsp;\n&nbsp;\n\n::: {.fragment}\n<p align=\"center\" style=\"font-size: 56px\">$x_0 = (\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Bom})$</p>\n:::\n\n\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n![](/images/knn/tabela.jpg){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">**Probabilidade de ocorrência das classes**</p>\n\n::: {.fragment}\n$$ P(\\text{classe = Sim}) = \\dfrac{9}{14}$$ \n$$P(\\text{classe = Não}) = \\dfrac{5}{14}$$\n:::\n\n\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n![](/images/knn/tabela.jpg){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">$x_0 = (\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Bom})$</p>\n\n::: {.fragment}\nPara o atributo **Idade**:\n\n$$ P(\\text{Idade} \\leq 30 | \\text{classe = Sim}) = \\dfrac{2}{9} \\\\ P(\\text{Idade} \\leq 30 | \\text{classe = Não}) = \\dfrac{3}{5}$$\n:::\n\n\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n![](/images/knn/tabela.jpg){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">$x_0 = (\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Bom})$</p>\n\n::: {.fragment}\nPara o atributo **Renda**:\n\n$$ P(\\text{Renda = Média} | \\text{classe = Sim}) = \\dfrac{4}{9} \\\\ P(\\text{Renda = Média} | \\text{classe = Não}) = \\dfrac{2}{5}$$\n:::\n\n\n:::\n\n::::\n\n\n\n## Exemplo: Compra de um computador\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n![](/images/knn/tabela.jpg){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">$x_0 = (\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Bom})$</p>\n\n::: {.fragment}\nPara o atributo **Estudante**:\n\n$$ P(\\text{Estudante = Sim} | \\text{classe = Sim}) = \\dfrac{6}{9} \\\\ P(\\text{Estudante = Sim} | \\text{classe = Não}) = \\dfrac{2}{5}$$\n:::\n\n\n:::\n\n::::\n\n\n\n## Exemplo: Compra de um computador\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n![](/images/knn/tabela.jpg){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">$x_0 = (\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Bom})$</p>\n\n::: {.fragment}\nPara o atributo **Crédito**:\n\n$$ P(\\text{Crédito = Bom} | \\text{classe = Sim}) = \\dfrac{6}{9} \\\\ P(\\text{Crédito = Bom} | \\text{classe = Não}) = \\dfrac{3}{5}$$\n:::\n\n\n:::\n\n::::\n\n\n\n## Exemplo: Compra de um computador\n\nTemos então, sob independência:\n\n\n$$P(x_0|\\text{Sim}) = P(\\leq 30 \\cap \\text{Média} \\cap \\text{Sim} \\cap \\text{Bom}|\\text{Sim} ) \\\\ = \\dfrac{2}{9} \\times \\dfrac{4}{9}\\times \\dfrac{6}{9}\\times \\dfrac{6}{9} = \\dfrac{288}{729} = 0,0439\\\\$$\n\n\n. . .\n\n\ne, \n\n\n$$P(x_0|\\text{Não}) = P(\\leq 30 \\cap \\text{Média} \\cap \\text{Sim} \\cap \\text{Bom}|\\text{Não} ) \\\\ = \\dfrac{3}{5} \\times \\dfrac{2}{5}\\times \\dfrac{2}{5}\\times \\dfrac{3}{5} = \\dfrac{36}{625} = 0,0576\\\\$$\n\n\n\n## Exemplo: Compra de um computador\n\n\nPelo Teorema da Probabilidade Total:\n\n$$\n\\begin{eqnarray*}\nP(x_0) &=& P(x_0|\\text{Sim}) \\times P(\\text{Sim}) + P(x_0|\\text{Não}) \\times P(\\text{Não}) \\\\ &=& 0,0439 \\times 0,6429 + 0,0576 \\times 0,3571 \\\\ &=& 0,0488\n\\end{eqnarray*}\n$$\n\n. . .\n\n\nAssim, pelo Teorema de Bayes:\n\n$$P(\\text{Sim}|x_0) = \\dfrac{P(x_0|\\text{Sim}) \\times P(\\text{Sim})}{P(x_0)} = \\dfrac{0,0439 \\times 0,6429}{0,0488} = 0,5783$$\n\n## Exemplo: Compra de um computador\n\ne, \n\n$$P(\\text{Não}|x_0) = \\dfrac{P(x_0|\\text{Não}) \\times P(\\text{Não})}{P(x_0)} = \\dfrac{0,0576 \\times 0,3571}{0,0488} = 0,4215 \\\\ $$\n\n\n## Exemplo: Compra de um computador \n\n<br>\n\n<br>\n\n\n<details>\n  <summary align=\"center\" style='font-size:80px;'>**João comprará o computador?**</summary>\n  <p align=\"center\" style=\"color: green; font-size:80px;\"> **De acordo com o Naive Bayes: SIM!** </p>\n</details>\n\n\n## Naive Bayes para classificação no R e Python\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris_split <- initial_split(iris, strata = Species)\ntrain_data <- training(iris_split)\ntest_data <- testing(iris_split)\n\nnb_model <- naive_Bayes() %>% \n  set_engine(\"klaR\") %>%\n  set_mode(\"classification\")\n\nnb_fit <- workflow() %>%\n  add_model(nb_model) %>%\n  add_formula(Species ~ .) %>%\n  fit(data = train_data)\n\nsp_predict <- predict(nb_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9487179\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB # Para dados contínuos como Iris\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.datasets import load_iris # Para carregar o dataset Iris\n\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target_names[iris.target], name='species') # Mapear números para nomes de espécies\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\nmodel_nb = GaussianNB()\nmodel_nb.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = model_nb.predict(X_test)\naccuracy_score(y_test, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9555555555555556\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n## Árvores de decisão\n\n<br>\n\nUma **árvore de decisão** é um modelo de aprendizado de máquina que utiliza uma estrutura de árvore para tomar decisões com base em condições nos dados de entrada. \n\n\n. . .\n\nEssa estrutura hierárquica consiste em nós que representam **testes** sobre atributos e arestas que conectam os nós, indicando os resultados desses testes.\n\n\n. . .\n\n\nCada **nó interno** da árvore representa um teste em um atributo específico, enquanto as **folhas** representam as classes ou valores de saída. \n\n\n\n\n\n## Árvores de decisão\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](/images/ad/ad.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">Ao percorrer a árvore da raiz até uma folha, os dados de entrada são avaliados de acordo com os testes em cada nó, seguindo o caminho apropriado até alcançar uma folha, onde é tomada a decisão final.</p>\n:::\n::::\n\n\n## Principais Algoritmos \n\n\n<br>\n\n\n<p align=\"center\">\n**ID3 (Iterative Dichotomiser 3)** \n</p>\n\n- É um dos primeiros algoritmos de árvore de decisão e utiliza o **ganho de informação** como critério para selecionar a melhor divisão em cada nó. No entanto, o ID3 não lida diretamente com **atributos numéricos**.\n\n. . .\n\n\n- Favorece características com muitos valores distintos.\n\n\n## Principais Algoritmos \n\n\n<br>\n\n\n<p align=\"center\">\n**C4.5**\n</p>\n\n- É uma extensão do ID3 e possui melhorias, incluindo a capacidade de lidar com **atributos numéricos** e **valores ausentes**. Além disso, o C4.5 utiliza a **razão de ganho** como métrica de seleção de atributos, em vez do ganho de informação utilizado pelo ID3.\n\n. . .\n\n- Corrige o viés em relação a características com muitos valores.\n\n\n\n## Principais Algoritmos \n\n\n<br>\n\n\n<p align=\"center\">\n**CART (Classification and Regression Trees)**\n</p>\n\n- Pode ser usado tanto para classificação quanto para regressão.\n\n. . .\n\n- Para classificação, usa o índice de Gini como critério de divisão.\n\n. . .\n\n- Gera árvores binárias (cada nó tem exatamente dois filhos). \n\n\n. . .\n\n- Ele busca minimizar a **impureza** nos nós da árvore.\n\n\n\n\n## Principais Algoritmos \n\n\n<br>\n\n\n<p align=\"center\">\n**CART (Classification and Regression Trees)**\n</p>\n\n- Para regressão, busca dividir os dados de forma a minimizar o erro quadrático médio (MSE) ou o desvio absoluto médio (MAE) nos nós folha.\n\n\n## Processo de preparação de um modelo de Árvore de Decisão\n\n<br>\n\n![](/images/ad/processo.jpg){.nostretch fig-align=\"center\" width=\"1800px\"}\n\n\n\n## Critérios de Divisão para Classificação\n\n<br>\n\n<br>\n\n<p align=\"center\">\nO objetivo dos critérios de divisão é encontrar a melhor característica para separar os dados em subconjuntos mais \"puros\" em relação à variável alvo (classe).\n</p>\n\n\n## Critérios de Divisão para Classificação\n\n<br>\n\n- **Entropia**\n\nA **entropia** é uma **medida de impureza** ou aleatoriedade dos dados em um algoritmo de árvore de decisão. Ela é utilizada para avaliar o quão homogêneos ou heterogêneos são os exemplos de uma determinada classe em um conjunto de dados.\n\n. . .\n\nÉ calculada a partir da distribuição das classes no conjunto de dados. Quanto **maior** a entropia, **maior a incerteza** sobre a classe de um exemplo. Quanto **menor** a entropia, mais **homogêneos** são os exemplos em relação à classe.\n\n\n## Critérios de Divisão para Classificação\n\n<br>\n\n- **Entropia**\n\n$$ \\text{E(S)} = \\sum_{i=1}^c -p_i\\log_2 p_i$$\n\n\nem que $p_i$ é a proporção de exemplos na classe $i$ da amostra de treinamento S.\n\n\n## Critérios de Divisão para Classificação\n\n<br>\n\n- **Ganho de Informação (G)**\n\nO **ganho de informação** é uma métrica utilizada para medir a relevância de um atributo na divisão dos dados. Ele indica a **quantidade de informação** que um atributo fornece sobre a **classe** ou **variável** de saída.\n\n. . .\n\nMede a **redução na entropia** após a divisão do conjunto de dados por uma característica.\n\n. . .\n\nO ID3 escolhe a característica com o **maior ganho de informação**.\n\n\n\n## Critérios de Divisão para Classificação\n\n<br>\n\n- **Ganho de Informação (G)**\n\n\n$$ \\text{G}(S, A) = \\text{E}(S) - \\text{E}(S,A)$$\n\nem que $E(S,A) = \\sum_{v\\in \\text{valores de } A} \\dfrac{|S_v|}{|S|} \\times \\text{E}(S_v)$ é a entropia do atributo A.\n\n\n\n\n\n## Critérios de Divisão para Classificação\n\n<br>\n\n- **Razão de ganho**\n\nTambém conhecida como **gain ratio**, é uma métrica utilizada para selecionar os melhores atributos de divisão, levando em consideração o viés por atributos com muitos valores possíveis.\n\n. . .\n\nEnquanto o **ganho de informação** mede a redução da entropia dos dados após a divisão com base em um atributo, a **razão de ganho** ajusta esse valor, levando em consideração o número de valores distintos do atributo. \n\n. . .\n\nEssa correção é importante para evitar que atributos com **muitos valores** possíveis tenham vantagem sobre atributos com **menos valores**.\n\n\n\n## Critérios de Divisão para Classificação\n\n<br>\n\n- **Razão de ganho**\n\n\n$$ \\text{Gain ratio}(S, A) = \\dfrac{G(S, A)}{E(S, A)}$$\n\n\n\n\n\n\n\n## Critérios de Divisão para Classificação\n\n\n\n- **Índice de Gini**\n\nO índice de Gini é outra medida de **impureza** utilizada para avaliar a heterogeneidade dos dados em relação à classe ou variável de saída. \n\n\n. . .\n\nMedida da probabilidade de uma amostra ser classificada incorretamente se fosse aleatoriamente rotulada de acordo com a distribuição das classes no subconjunto.\n\n. . .\n\nUm valor menor indica maior pureza.\n\n\n. . .\n\n\nUsado pelo CART para classificação.\n\n\n\n## Critérios de Divisão para Classificação\n\n\n\n- **Índice de Gini**\n\n$$ \\text{Gini}(S) = 1 - \\sum_{i=1}^c p_i^2$$\nem que $p_i$ é a proporção de exemplos na classe $i$ da amostra de treinamento S.\n\n\n. . .\n\n\nO CART busca a divisão que resulta na maior redução no índice de Gini.\n\n\n. . .\n\nO índice de Gini varia de 0 a 1, sendo 0 quando todos os exemplos pertencem à mesma classe (alta pureza) e 1 quando os exemplos estão igualmente distribuídos entre as classes (alta impureza).\n\n\n## Critérios de Divisão para Regressão\n\n<br>\n\n<br>\n\n\n<p align=\"center\">\nO objetivo é dividir os dados de forma a minimizar a variabilidade da variável alvo dentro de cada nó folha.\n</p>\n\n\n\n## Critérios de Divisão para Regressão\n\n- **Redução do Erro Quadrático Médio (MSE):**\n\n\nO CART para regressão geralmente busca a divisão que maximiza a redução no MSE.\n\n. . .\n\nCalcula-se o MSE no nó pai e o MSE ponderado nos nós filhos após a divisão. A divisão que proporciona a maior redução é escolhida.\n\n. . .\n\n$$MSE = \\dfrac{1}{|S|} \\displaystyle{\\sum_{i\\in S} (y_i - \\bar{y})^2}$$\n\n\n\nem que $y_i$ é o valor da variável alvo para a amostra $i$ e $\\bar{y}$ é a média dos valores da variável alvo em S.\n\n\n\n## Critérios de Divisão para Regressão\n\n- **Redução do Desvio Absoluto Médio (MAE):**\n\n\nAlternativamente, alguns algoritmos de árvores de regressão podem usar o MAE.\n\n\n. . .\n\n\n$$MAE = \\dfrac{1}{|S|} \\displaystyle{\\sum_{i\\in S} |y_i - \\text{mediana}(y)|}$$\n\n\n## Vantagens e Desvantagens\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<span style='font-size:160px;'>&#128077;</span>\n:::\n\n::: {.column width=\"90%\"}\n  -\tFáceis de entender e interpretar (modelos \"caixa branca\").\n  - Podem lidar com dados numéricos e categóricos.\n  - Não exigem muita preparação dos dados (não sensíveis a escala ou transformações monótonas).\n  - Capazes de capturar relações não lineares entre as características e a variável alvo.\n  - Úteis para seleção de características (as características mais próximas da raiz são consideradas mais importantes).\n:::\n\n::::\n\n## Vantagens e Desvantagens\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<span style='font-size:160px;'>&#128078;</span>\n:::\n\n::: {.column width=\"90%\"}\n  -\tTendência ao overfitting (ajustar-se demais aos dados de treinamento, resultando em mau desempenho em dados novos).\n  - Podem ser sensíveis a pequenas variações nos dados de treinamento.\n  - Podem criar árvores complexas que não generalizam bem.\n  - Para algumas tarefas, podem não ser tão precisas quanto outros algoritmos.\n  - O algoritmo guloso de construção da árvore nem sempre encontra a melhor árvore possível.\n\n:::\n\n::::\n\n\n\n## Controle de Overfitting (Podas)\n\n<br>\n\nA **poda** é o processo de **reduzir o tamanho de uma árvore de decisão**, removendo seções da árvore que podem estar superajustando os dados de treinamento.\n\n. . .\n\nO objetivo da poda é **simplificar** a árvore, tornando-a mais generalizável e menos propensa a memorizar ruídos específicos do conjunto de treinamento.\n\n\n## Controle de Overfitting (Podas)\n\n<br>\n\nTipos de Poda:\n\n\n- **Pré-poda:** Parar o crescimento da árvore antecipadamente com base em critérios (ex: profundidade máxima, número mínimo de amostras por nó).\n\n  - Mais eficiente computacionalmente; pode levar ao underfitting\n  \n. . .\n\n- **Pós-poda:** Construir a árvore completa e depois remover nós que não melhoram o desempenho em um conjunto de validação.\n\n  - Árvores com melhor capacidade de generalização; mais intensiva computacionalmente\n  \n\n\n## Exemplo: Compra de um computador\n\n\n![](/images/knn/tabela.jpg){.nostretch fig-align=\"center\" width=\"1300px\"}\n\n\n\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n![](/images/knn/tabela.jpg){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">**Cálculo da Entropia**</p>\n\n$$ \\text{E(S)} = \\sum_{i=1}^c -p_i\\log_2 p_i$$\n\n::: {.fragment}\n$$ \\text{E(S)} = -\\dfrac{9}{14}\\log_2 (\\dfrac{9}{14}) - \\dfrac{5}{14}\\log_2 (\\dfrac{5}{14}) = 0,940$$\n:::\n\n\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<br>\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;}\n.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-cw6g\"></th>\n    <th class=\"tg-cw6g\"></th>\n    <th class=\"tg-c3ow\" colspan=\"2\">Classe</th>\n    <th class=\"tg-c3ow\"></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-cw6g\"></td>\n    <td class=\"tg-oao4\"></td>\n    <td class=\"tg-c3ow\">Sim</td>\n    <td class=\"tg-c3ow\">Não</td>\n    <td class=\"tg-c3ow\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-itrc\" rowspan=\"3\">Idade<br></td>\n    <td class=\"tg-oao4\">=&lt; 30</td>\n    <td class=\"tg-c3ow\">2</td>\n    <td class=\"tg-c3ow\">3</td>\n    <td class=\"tg-c3ow\">5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-oao4\">31...40</td>\n    <td class=\"tg-c3ow\">4</td>\n    <td class=\"tg-c3ow\">0</td>\n    <td class=\"tg-c3ow\">4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-oao4\">&gt;40</td>\n    <td class=\"tg-c3ow\">3</td>\n    <td class=\"tg-c3ow\">2</td>\n    <td class=\"tg-c3ow\">5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-cw6g\"></td>\n    <td class=\"tg-oao4\"></td>\n    <td class=\"tg-c3ow\"></td>\n    <td class=\"tg-c3ow\"></td>\n  </tr>\n</tbody>\n</table>\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">**Ganho de informação para o atributo idade**</p>\n\n$$ \\text{E(S, Idade)} = \\dfrac{|\\leq 30|}{|S|}\\times \\text{E}(\\leq 30)\\\\\n+ \\dfrac{|31...40|}{|S|}\\times \\text{E}(31...40) \\\\ +  \\dfrac{|>40|}{|S|}\\times \\text{E}(>40)$$\n\n<!-- ::: {.fragment} -->\n<!-- $$ \\text{E(S)} = -\\dfrac{9}{14}\\log_2 (\\dfrac{9}{14}) - \\dfrac{5}{14}\\log_2 (\\dfrac{5}{14}) = 0,940$$ -->\n<!-- ::: -->\n\n\n:::\n\n::::\n\n\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<br>\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;}\n.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-cw6g\"></th>\n    <th class=\"tg-cw6g\"></th>\n    <th class=\"tg-c3ow\" colspan=\"2\">Classe</th>\n    <th class=\"tg-c3ow\"></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-cw6g\"></td>\n    <td class=\"tg-oao4\"></td>\n    <td class=\"tg-c3ow\">Sim</td>\n    <td class=\"tg-c3ow\">Não</td>\n    <td class=\"tg-c3ow\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-itrc\" rowspan=\"3\">Idade<br></td>\n    <td class=\"tg-oao4\">=&lt; 30</td>\n    <td class=\"tg-c3ow\">2</td>\n    <td class=\"tg-c3ow\">3</td>\n    <td class=\"tg-c3ow\">5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-oao4\">31...40</td>\n    <td class=\"tg-c3ow\">4</td>\n    <td class=\"tg-c3ow\">0</td>\n    <td class=\"tg-c3ow\">4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-oao4\">&gt;40</td>\n    <td class=\"tg-c3ow\">3</td>\n    <td class=\"tg-c3ow\">2</td>\n    <td class=\"tg-c3ow\">5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-cw6g\"></td>\n    <td class=\"tg-oao4\"></td>\n    <td class=\"tg-c3ow\"></td>\n    <td class=\"tg-c3ow\"></td>\n  </tr>\n</tbody>\n</table>\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">**Ganho de informação para o atributo idade**</p>\n\n$$ \\text{E(S, Idade)} = \\dfrac{5}{14}\\times \\text{E}(\\leq 30) \\\\ + \\dfrac{4}{14}\\times \\text{E}(31...40) \\\\ +  \\dfrac{5}{14}\\times \\text{E}(>40)$$\n\n\n\n\n:::\n\n::::\n\n\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<br>\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;}\n.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-cw6g\"></th>\n    <th class=\"tg-cw6g\"></th>\n    <th class=\"tg-c3ow\" colspan=\"2\">Classe</th>\n    <th class=\"tg-c3ow\"></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-cw6g\"></td>\n    <td class=\"tg-oao4\"></td>\n    <td class=\"tg-c3ow\">Sim</td>\n    <td class=\"tg-c3ow\">Não</td>\n    <td class=\"tg-c3ow\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-itrc\" rowspan=\"3\">Idade<br></td>\n    <td class=\"tg-oao4\">=&lt; 30</td>\n    <td class=\"tg-c3ow\">2</td>\n    <td class=\"tg-c3ow\">3</td>\n    <td class=\"tg-c3ow\">5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-oao4\">31...40</td>\n    <td class=\"tg-c3ow\">4</td>\n    <td class=\"tg-c3ow\">0</td>\n    <td class=\"tg-c3ow\">4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-oao4\">&gt;40</td>\n    <td class=\"tg-c3ow\">3</td>\n    <td class=\"tg-c3ow\">2</td>\n    <td class=\"tg-c3ow\">5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-cw6g\"></td>\n    <td class=\"tg-oao4\"></td>\n    <td class=\"tg-c3ow\"></td>\n    <td class=\"tg-c3ow\"></td>\n  </tr>\n</tbody>\n</table>\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">**Ganho de informação para o atributo idade**</p>\n\n$$ \\text{E}(\\leq 30) = -\\dfrac{2}{5}\\log_2 (\\dfrac{2}{5}) - \\dfrac{3}{5}\\log_2 (\\dfrac{3}{5}) = 0,971$$\n$$ \\text{E}(31...40) = 0\\,\\,\\,\\,\\,\\,\\,\\, \\text{e} \\,\\,\\,\\,\\,\\,\\,\\, \\text{E}(>40) = 0,971$$\n\n\n\n:::\n\n::::\n\n\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<br>\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;}\n.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-cw6g\"></th>\n    <th class=\"tg-cw6g\"></th>\n    <th class=\"tg-c3ow\" colspan=\"2\">Classe</th>\n    <th class=\"tg-c3ow\"></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-cw6g\"></td>\n    <td class=\"tg-oao4\"></td>\n    <td class=\"tg-c3ow\">Sim</td>\n    <td class=\"tg-c3ow\">Não</td>\n    <td class=\"tg-c3ow\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-itrc\" rowspan=\"3\">Idade<br></td>\n    <td class=\"tg-oao4\">=&lt; 30</td>\n    <td class=\"tg-c3ow\">2</td>\n    <td class=\"tg-c3ow\">3</td>\n    <td class=\"tg-c3ow\">5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-oao4\">31...40</td>\n    <td class=\"tg-c3ow\">4</td>\n    <td class=\"tg-c3ow\">0</td>\n    <td class=\"tg-c3ow\">4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-oao4\">&gt;40</td>\n    <td class=\"tg-c3ow\">3</td>\n    <td class=\"tg-c3ow\">2</td>\n    <td class=\"tg-c3ow\">5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-cw6g\"></td>\n    <td class=\"tg-oao4\"></td>\n    <td class=\"tg-c3ow\"></td>\n    <td class=\"tg-c3ow\"></td>\n  </tr>\n</tbody>\n</table>\n:::\n\n::: {.column width=\"50%\"}\n\n<br>\n\n<p align=\"center\">**Ganho de informação para o atributo idade**</p>\n\n$$ \\text{E(S, Idade)} = \\dfrac{5}{|14|}\\times 0,971  + \\dfrac{4}{|14|}\\times 0 \\\\ +  \\dfrac{5}{14}\\times 0,971 = 0,693$$\n\n$$ \\text{G}(S, \\text{Idade}) = \\text{E}(S) - \\text{E}(S,\\text{Idade})\\\\ = 0,940 - 0,693 = 0,247$$\n\n\n\n:::\n\n::::\n\n\n\n\n\n\n\n\n## Exemplo: Compra de um computador\n\n\n\n![](/images/ad/ganhos.png){.nostretch fig-align=\"center\" width=\"1800px\"}\n\n\n\n## Exemplo: Compra de um computador\n\n<br>\n\nEscolhemos o atributo com maior ganho de informação como nó de decisão. No nosso caso, o atributo **Idade**. A partir daí, dividimos o conjunto de dados a partir das categorias da variável idade e repetimos o mesmo processo em todos os ramos. \n\n. . .\n\nUm ramo com entropia de 0 é um **nó folha**. Um ramo com entropia maior que 0 precisa de mais divisão.\n\n\n## Exemplo: Compra de um computador\n\n\n<p align=\"center\">**Árvore estimada**</p>\n\n\n![](/images/ad/arv_est.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n\n\n\n## Exemplo: Compra de um computador\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![](/images/joao.png){.nostretch fig-align=\"center\" width=\"1300px\"}\n:::\n\n::: {.column width=\"60%\"}\n\n<br>\n\n<p align=\"center\" style=\"font-size: 76px\">**Voltando ao Joãozinho...**</p>\n\n&nbsp;\n&nbsp;\n\n::: {.fragment}\n<p align=\"center\" style=\"font-size: 56px\">$x_0 = (\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Bom})$</p>\n:::\n\n\n:::\n\n::::\n\n\n## Exemplo: Compra de um computador\n\n<p align=\"center\" style=\"font-size: 56px\">$x_0 = (\\leq 30, \\text{ Média}, \\text{ Sim}, \\text{ Bom})$</p>\n\n\n![](/images/ad/arv_est_joao.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n\n\n## Exemplo: Compra de um computador \n\n<br>\n\n<br>\n\n\n<details>\n  <summary align=\"center\" style='font-size:80px;'>**João comprará o computador?**</summary>\n  <p align=\"center\" style=\"color: green; font-size:80px;\"> **De acordo com Árvores de Decisão: SIM!** </p>\n</details>\n\n\n## Árvores de  Decisão para classificação no R e Python\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris_split <- initial_split(iris, strata = Species)\ntrain_data <- training(iris_split)\ntest_data <- testing(iris_split)\n\ntree_model <- decision_tree() %>% \n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n\ntree_fit <- workflow() %>%\n  add_model(tree_model) %>%\n  add_formula(Species ~ .) %>%\n  fit(data = train_data)\n\nsp_predict <- predict(tree_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8974359\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target_names[iris.target], name='species') # Mapear números para nomes de espécies\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\ntree = DecisionTreeClassifier(random_state=12345)\ntree.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=12345)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=12345)</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = tree.predict(X_test)\naccuracy_score(y_test, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9777777777777777\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## Árvores de Decisão para regressão no R e Python\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split <- initial_split(mtcars, strata = mpg)\ntrain_data <- training(mtcars_split)\ntest_data <- testing(mtcars_split)\n\ntree_model <- decision_tree() %>% \n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\ntree_fit <- workflow() %>%\n  add_model(tree_model) %>%\n  add_formula(mpg ~ .) %>%\n  fit(data = train_data)\n\nmpg_predict <- predict(tree_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 17.00174\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\ntree = DecisionTreeRegressor(random_state=12345)\ntree.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(random_state=12345)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(random_state=12345)</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = tree.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.5445212937679262\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## Random Forests\n\n<br>\n\n\n**Random Forests** é um algoritmo de aprendizado de máquina que combina várias **árvores de decisão** independentes para realizar tarefas de classificação ou regressão. \n\n. . .\n\nEle pertence à categoria de **métodos ensemble**, que buscam melhorar a precisão e robustez das predições combinando diferentes modelos para se obter um único resultado.\n\n\n## Random Forests\n\nUtiliza bagging (bootstrap aggregation) para criar múltiplas árvores e combinar suas previsões.\n\n\n\n![](/images/rf/bagg.png){.nostretch fig-align=\"center\" width=\"1400px\"}\n\n\n\n## Construção de um Random Forests\n\n\n1) Amostras são geradas com substituição (bootstrap)\n\n. . .\n\n2) Para cada amostra, os dados são divididos utilizando os atributos que maximizem a **pureza dos nós**\n    - **Critérios de divisão:** Gini Index, Entropia, Ganho de Informação, etc.\n\n. . .\n\n\n3) A cada divisão da árvore, uma amostra aleatória de variáveis é usada\n   - Construção recursiva até atingir **critérios de parada** (profundidade máxima, pureza mínima)\n\n. . .\n\n4) Previsão final: Cada árvore possui um peso igual na votação final\n    - Classificação: maioria dos votos\n    - Regressão: média das previsões\n\n\n## Construção de um Random Forests    \n\n![](/images/rf/random.jpeg){.nostretch fig-align=\"center\" width=\"1400px\"}\n\n\n\n\n## Construção de um Random Forests\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n![](/images/rf/rf_class.jpg){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"40%\"}\n\n<br>\n\n<br>\n\n<details>\n  <summary align=\"center\" style='font-size:50px;'>**Qual a classe predita?**</summary>\n  <p align=\"center\" style=\"color: black; font-size:40px;\"> **Pela regra da maioria: classe 1** </p>\n</details>\n\n:::\n\n::::\n\n\n## Hiperparâmetros principais\n\n<br>\n\n- **n_estimators / ntree:** número de árvores na floresta\n\n- **max_features / mtry:** número de variáveis usadas por divisão\n\n- **max_depth:** profundidade máxima das árvores\n\n- **min_samples_split:** mínimo de amostras para uma divisão\n\n\n\n## Vantagens e Desvantagens\n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<span style='font-size:160px;'>&#128077;</span>\n:::\n\n::: {.column width=\"90%\"}\n- Trata efetivamente problemas de **overfitting**\n- Lida bem com **ruídos**, **dados ausentes** e **valores discrepantes**\n- Lida com classificações multiclasses\n- Pouco ajuste de parâmetros\n- Capaz de lidar com **atributos de diferentes tipos** (numéricos, categóricos)\n- Reduz a **variância** e melhora a **precisão** em comparação com uma única árvore\n:::\n\n::::\n\n## Vantagens e Desvantagens\n\n<br>\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<span style='font-size:160px;'>&#128078;</span>\n:::\n\n::: {.column width=\"90%\"}\n\n<p> \n</p>\n\n- Menos interpretável que uma árvore única\n\n- Mais custoso computacionalmente\n\n:::\n\n::::\n\n\n\n\n## Random Forests para classificação no R e Python\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris_split <- initial_split(iris, strata = Species)\ntrain_data <- training(iris_split)\ntest_data <- testing(iris_split)\n\nrf_model <- rand_forest(mtry = 2, trees = 500) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  set_mode(\"classification\")\n\nrf_fit <- workflow() %>%\n  add_model(rf_model) %>%\n  add_formula(Species ~ .) %>%\n  fit(data = train_data)\n\nsp_predict <- predict(rf_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8974359\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target_names[iris.target], name='species') # Mapear números para nomes de espécies\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\nrf_model = RandomForestClassifier(n_estimators=500, max_features=2)\nrf_model.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_features=2, n_estimators=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_features=2, n_estimators=500)</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = rf_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9555555555555556\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## Random Forests para regressão no R e Python\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split <- initial_split(mtcars, strata = mpg)\ntrain_data <- training(mtcars_split)\ntest_data <- testing(mtcars_split)\n\nrf_reg <- rand_forest(mtry = 3, trees = 500) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  set_mode(\"regression\")\n\nrf_reg_fit <- workflow() %>%\n  add_model(rf_reg) %>%\n  add_formula(mpg ~ .) %>%\n  fit(data = train_data)\n\nmpg_predict <- predict(rf_reg_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.18105\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\nreg = RandomForestRegressor(n_estimators=100, max_features=6)\nreg.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=6)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=6)</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.2569464989051697\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## Importância das variáveis\n\n<br>\n\n- Random Forest permite extrair importância das variáveis\n\n- Mostra o quanto cada variável contribui para a redução da impureza ou erro\n\n\n## Importância das variáveis\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nlibrary(vip)\nextract_fit_parsnip(rf_fit) %>% vip(num_features = 4)\n```\n\n::: {.cell-output-display}\n![](preditiva_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n### Python\n\n\n::: {.cell output-location='column'}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Criar DataFrame com variáveis e importâncias\nfeat_imp = pd.DataFrame({\n    \"Variável\": iris.feature_names,\n    \"Importância\": rf_model.feature_importances_\n})\n\n# Ordenar pela importância\nfeat_imp = feat_imp.sort_values(by=\"Importância\", ascending=False)\n\n# Plotar gráfico ordenado\nsns.barplot(x=\"Importância\", y=\"Variável\", data=feat_imp, palette=\"viridis\")\nplt.title(\"Importância das variáveis\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](preditiva_files/figure-revealjs/unnamed-chunk-17-1.png){width=614}\n:::\n:::\n\n\n:::\n\n\n## Support Vector Machine (SVM)\n\n<br>\n\n- Algoritmo supervisionado usado para **classificação** e **regressão**.\n\n. . .\n\n- Busca **uma fronteira ótima** (hiperplano) que separa as classes com a maior margem possível.\n\n. . .\n\n\n- Depende de poucos pontos críticos: os **vetores de suporte**.\n\n\n. . .\n\n\n- Pode trabalhar com dados **não linearmente separáveis** através da técnica do **kernel trick**.\n\n\n\n## Por que aprender SVM?\n\n<br>\n\n- É um dos algoritmos mais eficazes para problemas de classificação com fronteiras complexas.\n\n. . .\n\n- Funciona bem em espaços de alta dimensão.\n\n. . .\n\n- Utiliza o conceito de **margem máxima**, promovendo boa generalização.\n\n. . .\n\n- Amplamente utilizado em áreas como bioinformática, finanças, reconhecimento de padrões.\n\n\n## Princípio básico\n\n<br>\n\nO objetivo do SVM é encontrar um **hiperplano de separação ótimo** que **maximize a margem** entre as classes.\n\n. . .\n\n\nPara problemas de classificação binária, um **hiperplano de separação** é uma superfície que divide o espaço de características em duas regiões, uma para cada classe. Neste caso, esse hiperplano é uma **reta**!\n\n. . .\n\nEm problemas **multiclasse**, pode ser um plano ou uma superfície mais complexa.\n\n\n\n## Hiperplano de Separação\n\n\n![](/images/rf/hiper.jpg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n<p align=\"center\">  \nTemos aqui 4 hiperplanos (A, B, C e D). Qual o melhor?\n</p>\n\n\n## Conceitos de vetores de suporte e Margem Máxima\n\n<br>\n\nOs **vetores de suporte** são os pontos de dados mais próximos ao hiperplano de separação. Eles são fundamentais para o SVM, pois definem a posição do hiperplano e a **margem**.\n\n. . .\n\nA **margem** é a distância entre o hiperplano de separação e os vetores de suporte mais próximos. O objetivo do SVM é **maximizar a margem**, pois isso aumenta a capacidade de **generalização** do modelo.\n\n\n## Conceitos de vetores de suporte e Margem Máxima\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n<br> \n\n![](/images/rf/svm.jpeg){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n<p align=\"center\">  \nMargem = $d_1 + d_2$\n</p>\n\n:::\n\n::: {.column width=\"40%\" .fragment}\n\n<br>\n\n<p align=\"center\">  \n<span style='font-size:70px;'>&#128073;</span> Para encontrar o **hiperplano ótimo**, caímos em um problema de otimização com restrição, que pode ser resolvido utilizando a técnica dos **Multiplicadores de Lagrange**!\n</p>\n\n:::\n\n::::\n\n\n\n## Kernel Trick: separando o inseparável\n\nMas e se tivermos problemas **não linearmente separáveis**?\n\n![](/images/rf/nLin.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n\n## Kernel Trick: separando o inseparável\n\nO SVM é um algoritmo que inicialmente trabalha com dados **linearmente separáveis**. No entanto, muitas vezes encontramos conjuntos de dados que não podem ser separados por um **hiperplano linear**. É aí que entra o **Kernel Trick**.\n\n\n. . .\n\n\nO **Kernel Trick** (Truque do Kernel) nos permite **mapear os dados** para um espaço de características de **dimensão superior**, onde se tornam linearmente separáveis. \n\n\n. . .\n\n\nIsso é feito através de uma **função** chamada **kernel**, que calcula o produto interno entre dois vetores nesse espaço de características.\n\n\n## Kernel Trick: separando o inseparável\n\n\n![](/images/rf/kernelTrick.png){.nostretch fig-align=\"center\" width=\"1400px\"}\n\n\n## Kernel Trick: separando o inseparável\n\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n<br>\n\n<span style='font-size:70px;'>&#128073;</span>  Exemplos comuns:\n\n- Linear\n- Polinomial\n- Radial Basis Function (RBF ou Gaussiano)\n- Sigmoide\n\n:::\n\n::: {.column width=\"60%\" .fragment}\n\n![](/images/rf/kernel.jpg){.nostretch fig-align=\"center\" width=\"800px\"}\n\n:::\n\n::::\n\n\n## Escolha do kernel\n\n<br>\n\nA escolha do kernel adequado é importante para obter um bom desempenho do SVM. É necessário analisar as características dos dados e testar diferentes kernels para encontrar o que **melhor se adapta** ao problema em questão\n\n. . .\n\n\nO uso do **Kernel Trick** pode melhorar o desempenho do SVM ao permitir que ele modele relações não lineares nos dados. No entanto, é preciso ter cuidado para evitar o **overfitting**, garantindo que o modelo generalize bem para dados não vistos.\n\n\n## SVM para problemas multiclasse\n\n\nO SVM como definido funciona para **duas classes**. O que fazemos então se tivermos **mais de duas** classes?\n\n. . .\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n<p></p>\n\n### One-vs-One (OVO)\n\n- Nessa abordagem, é criado um **classificador SVM** para cada **par de classes** possível. \n\n- Cada classificador é usado para classificar uma instância de entrada e a **classe com mais votos** é selecionada como a **classe final**.\n\n\n:::\n\n::: {.column width=\"40%\" .fragment}\n\n![](/images/rf/OVO.png){.nostretch fig-align=\"center\" width=\"800px\"}\n\n:::\n\n::::\n\n\n\n## SVM para problemas multiclasse\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n<p></p>\n\n### One-vs-All (OVA)\n\n- Essa abordagem consiste em treinar **um classificador SVM para cada classe**, onde cada classificador é treinado para distinguir uma classe específica das demais classes combinadas.\n\n- Durante a fase de teste, cada classificador é usado para classificar uma instância de entrada e a classe com a **maior probabilidade** é selecionada como a classe final.\n\n\n:::\n\n::: {.column width=\"40%\" .fragment}\n\n![](/images/rf/OVA.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n:::\n\n::::\n\n\n\n## SVM para classificação no R e Python\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris <- iris %>% filter(Species != \"setosa\") %>% mutate(Species = factor(Species))\niris_split <- initial_split(iris, strata = Species)\ntrain_data <- training(iris_split)\ntest_data <- testing(iris_split)\n\nsvm_model <- svm_rbf() %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"classification\")\n\nsvm_fit <- workflow() %>%\n  add_model(svm_model) %>%\n  add_formula(Species ~ .) %>%\n  fit(data = train_data)\n\nsp_predict <- predict(svm_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8461538\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data[iris.target != 0, :2]\ny = iris.target[iris.target != 0]\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\nsvm_model = SVC(kernel=\"rbf\")\nsvm_model.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = svm_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.6666666666666666\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## SVM para Regressão\n\n<br>\n \n- Também pode ser usado para problemas de regressão, chamado de **SVR (Support Vector Regression)**.\n\n. . .\n\n- Busca encontrar uma função que tenha no máximo $\\epsilon$ de desvio dos pontos de treino, mantendo a maior margem possível.\n\n. . .\n\n\n- É sensível à escolha do kernel e dos hiperparâmetros (ex: $C$, $\\epsilon$ ).\n\n\n\n## SVR para regressão no R e Python\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split <- initial_split(mtcars, strata = mpg)\ntrain_data <- training(mtcars_split)\ntest_data <- testing(mtcars_split)\n\nsvr_reg <- svm_rbf() %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"regression\")\n\nsvr_reg_fit <- workflow() %>%\n  add_model(svr_reg) %>%\n  add_formula(mpg ~ .) %>%\n  fit(data = train_data)\n\nmpg_predict <- predict(svr_reg_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.124458\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\nsvr_reg = SVR(kernel='rbf', C=100, epsilon=0.1)\nsvr_reg.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(C=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(C=100)</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = svr_reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.6697473584156818\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## Principais Hiperparâmetros do SVM\n\n- 🔧 **C (Custo da penalização)**\n  - Controla o equilíbrio entre **margem ampla** e **erros de classificação**.\n  - Valores baixos → margem maior, mais tolerância ao erro.  \n  - Valores altos → menos erros, margem mais estreita (pode causar overfitting).\n  \n. . .\n\n- 🌐 **Kernel**\n  - Função que transforma os dados para um espaço de maior dimensão:\n    - `\"linear\"`: separação linear.\n    - `\"rbf\"` (radial basis function): não linear, padrão.\n    - `\"poly\"`: polinomial, útil para interações complexas.\n\n\n## Principais Hiperparâmetros do SVM\n\n\n\n- ⚙️ **Gamma** (usado com RBF e poly)\n  - Define a influência de um ponto de treino:\n    - Valores altos → influência local → mais complexidade.\n    - Valores baixos → influência global → modelo mais suave.\n    \n. . .\n\n- 📏 **Epsilon (para regressão - SVR)**\n  - Define a zona de tolerância onde **nenhuma penalização é aplicada**.\n  - Controla a sensibilidade do modelo a pequenas variações nos dados.\n\n\n## Pré-processamento para SVM\n\n<br>\n\n- O SVM é sensível à escala das variáveis:\n  - Padronização (ex: `StandardScaler` no Python) é altamente recomendada.    \n  - Em tidymodels, use `step_normalize()` no `recipe`.\n  - Remover outliers pode ajudar na estabilidade da margem.\n\n\n## Vantagens e Desvantagens\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<span style='font-size:160px;'>&#128077;</span>\n:::\n\n::: {.column width=\"90%\"}\n\n<p></p>\n\n- Alta capacidade de generalização\n\n- Funciona bem em dados com muitas variáveis\n\n- Eficiente em fronteiras não lineares com kernels\n:::\n\n::::\n\n. . .\n\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<span style='font-size:160px;'>&#128078;</span>\n:::\n\n::: {.column width=\"90%\"}\n\n<p></p>\n\n- Pode ser lento com bases muito grandes\n\n- Escolha do kernel e parâmetros pode exigir `tuning` intenso\n\n- Pouco interpretável (modelo de “caixa-preta”)\n\n:::\n\n::::\n\n\n## Quando usar SVM?\n\n\n<br> \n\n\n- Quando há fronteiras complexas de decisão\n\n. . .\n\n- Quando se quer evitar overfitting em problemas com poucas amostras\n\n. . .\n\n- Quando outras abordagens lineares falharem\n\n. . .\n\n- Em tarefas onde o tempo de predição não é um gargalo crítico\n\n\n## Redes Neurais Artificiais\n\n<p></p>\n\n**Redes neurais artificiais** são modelos matemáticos e computacionais inspirados no funcionamento do **cérebro humano**.\n\n![](/images/rna/neuronio2.png){.fragment .nostretch fig-align=\"center\" width=\"1000px\"}\n\n## Inspiração biológica\n\nOs **neurônios** se comunicam através de **sinapses**. Sinapse é a região onde dois neurônios entram em contato e através da qual os **impulsos nervosos** são transmitidos entre eles\n\n\n![](/images/rna/sinapse.jpeg){.fragment .nostretch fig-align=\"center\" width=\"500px\"}\n\n\n## Neurônio Artificial\n\n<p></p>\n\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\n\n![](/images/rna/neuronioMP.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n:::\n\n::: {.column width=\"30%\" .fragment}\n\n<br>\n\n$$ \\nu_k = \\sum_{i=1}^p w_{ki}x_i +b_k$$\n\n$$ y_k = \\varphi(\\nu_k)$$\n\n:::\n\n::::\n\n\n## Funções de ativação\n\n<br>\n\nA **função de ativação**, denotada por $\\varphi(\\nu)$, define sua saída ou ativação em termos do sinal $\\nu$. \n\n\n. . .\n\n\nEssa função é responsável por introduzir **não-linearidades** nas operações realizadas pela rede neural, permitindo que ela aprenda e modele **relações complexas** nos dados.\n\n\n## Função Threshold\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n$$\\varphi(\\nu) = \\begin{cases}1 & \\text{se } \\nu \\geq 0, \\\\\n0 & \\text{se }\\nu < 0\\end{cases}$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n<p></p>\n\n![](/images/rna/threshold.jpeg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n:::\n\n::::\n\n<p align=\"center\">  \n<span style='font-size:80px;'>&#128073;</span> Útil em problemas onde se deseja atribuir uma saída binária, como 0 ou 1, com base em um limite.\n</p>\n\n\n\n\n\n## Função Sigmoidal\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n$$\\varphi(\\nu) = \\dfrac{1}{1 + \\exp(-a\\nu)}$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n<p></p>\n\n![](/images/rna/sigmoide.jpg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n:::\n\n::::\n\n<p align=\"center\">  \n<span style='font-size:80px;'>&#128073;</span> Útil em problemas onde a saída representa a probabilidade de pertencer a uma das duas classes.\n\n</p>\n\n\n\n\n## Função Sinal\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n$$\\varphi(\\nu) = \\begin{cases}1 & \\text{se } \\nu > 0, \\\\\n0 & \\text{se }\\nu = 0, \\\\\n-1 & \\text{se } \\nu < 0 \\end{cases}$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n<p></p>\n\n![](/images/rna/sinal.jpg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n:::\n\n::::\n\n<p align=\"center\">  \n<span style='font-size:80px;'>&#128073;</span> Útil em problemas onde se deseja atribuir uma saída discreta de -1 ou 1 com base na polaridade do valor de entrada.\n</p>\n\n\n\n\n\n## Função Tangente Hiperbólica\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n$$\\varphi(\\nu) = \\dfrac{1-\\exp(-\\beta \\nu)}{1+\\exp(-\\beta \\nu)}$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n<p></p>\n\n![](/images/rna/tanh.jpg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n:::\n\n::::\n\n<p align=\"center\">  \n<span style='font-size:80px;'>&#128073;</span> É comumente usada em redes neurais para classificação binária ou regressão.\n</p>\n\n\n\n\n## Função ReLU\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n$$\\varphi(\\nu) = \\max(0, \\nu)$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n<p></p>\n\n![](/images/rna/relu.jpg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n:::\n\n::::\n\n<p align=\"center\">  \n<span style='font-size:80px;'>&#128073;</span> Amplamente usada em camadas ocultas de redes neurais devido à sua simplicidade computacional \n</p>\n\n\n\n## Função Softmax\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n\n\n$$\\varphi(\\nu) = \\dfrac{\\exp({\\nu})}{\\sum_{k=1}^K \\exp(\\nu_k) }$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n<p></p>\n\n![](/images/rna/softmax.jpg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n:::\n\n::::\n\n<p align=\"center\">  \n<span style='font-size:80px;'>&#128073;</span> Usada para classificação multiclasse. As saídas são normalizadas, representando probabilidades. \n</p>\n\n\n\n## Arquiteturas de redes\n\n<br>\n\nExistem **várias arquiteturas** de redes neurais que foram desenvolvidas ao longo dos anos para atender a diferentes **necessidades** e **desafios**.\n\n. . .\n\nA arquitetura de uma RNA, define a sua **especialidade** e qual **tipo de problema** poderá ser utilizada para resolvê-lo.\n\n. . .\n\nO que define a arquitetura de uma RNA basicamente são as **camadas** (camada única ou múltiplas camadas), **número de neurônios** em cada camada e o tipo de **conexão** entre os neurônios (FeedForward ou feedback)\n\n\n\n## Perceptron\n\nO **perceptron** é a arquitetura mais básica de rede neural, composta por **um único neurônio** com conexões diretas (**feedforward**) de entrada e uma função de ativação. \n\n\n<p></p>\n\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\n\n![](/images/rna/neuronioMP.png){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"30%\" .fragment}\n\n<p></p>\n\n- **bias:** ajusta a influência das entradas\n- aprimora a flexibilidade do modelo.\n\n:::\n\n::::\n\n\n\n## Perceptron\n\n\nEssa arquitetura é usada principalmente para problemas de **classificação linearmente separáveis**.\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](/images/rna/li.jpg){.nostretch fig-align=\"center\" width=\"800px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n![](/images/rf/nLin.png){.nostretch fig-align=\"center\" width=\"550px\"}\n\n\n:::\n\n::::\n\n\n## Multi-Layer Perceptron (MLP)\n\n<br>\n\nO **MLP** é uma arquitetura de rede neural **feedforward** com várias **camadas ocultas** entre a **camada de entrada** e a **camada de saída**. \n\n. . .\n\n- **Camada de Entrada**: Recebe os dados de entrada.\n\n. . .\n\n- **Camadas Ocultas**: Realizam o processamento das entradas.\n\n. . .\n\n- **Camada de Saída**: Fornece a saída do modelo.\n\n\n. . .\n\nCada camada possui **vários** neurônios **interconectados**. \n\n\n\n\n## Multi-Layer Perceptron (MLP)\n\n<p></p>\n\n![](/images/rna/mlp.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n## Multi-Layer Perceptron (MLP)\n\n<p></p>\n\n![](/images/rna/rna.png){.nostretch fig-align=\"center\" width=\"1000px\"}\n\n\n\nO MLP é capaz de aprender e modelar relações complexas nos dados e é amplamente usado em problemas de classificação e regressão.\n\n\n\n## Treinamento das Redes Neurais\n\n<br>\n\nO **treinamento** de redes neurais é o processo de ajustar os **pesos** e **bias** (parâmetros) da rede neural para que ela seja capaz de aprender a partir dos dados de treinamento e gerar previsões precisas para novos dados.\n\n. . .\n\n\nO treinamento envolve a **minimização** de uma **função de custo** ou perda, que mede a diferença entre as **saídas previstas** pela rede neural e as **saídas desejadas**.\n\n\n## Treinamento das Redes Neurais\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n<br>\n\n![](https://media3.giphy.com/media/7HAm2aWDviqeQ/giphy-downsized-medium.gif){.nostretch fig-align=\"center\" width=\"1000px\"}\n:::\n\n::: {.column width=\"50%\"}\n\n![](/images/rna/super.png){.nostretch fig-align=\"center\" width=\"500px\"}\n\n\n:::\n\n::::\n\n. . .\n\n<p align=\"center\">  \n<span style='font-size:80px;'>&#128073;</span> Treinamento supervisionado\n</p>\n\n\n## Treinamento do perceptron\n\n\nPara cada exemplo de treinamento, o perceptron realiza uma **propagação direta** dos dados de entrada através da função de ativação. \n\n. . .\n\n\nO **resultado** da propagação direta é comparado com a **saída desejada**. \n\n. . .\n\nSe a **saída do perceptron** corresponder à **saída desejada**, nenhuma atualização nos pesos e bias é necessária e o próximo exemplo de treinamento é processado.\n\n. . .\n\nSe a saída do perceptron for **diferente** da saída desejada, os pesos e o bias são **atualizados** para ajustar o perceptron. \n\n\n\n## Treinamento do perceptron\n\n\nA atualização dos pesos é dada por:\n\n$$\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\eta[\\hat{y}(n)-y(n)]\\mathbf{x}(n)$$\n\n\n. . .\n\nE a atualização do bias:\n\n$$b(n+1) = b(n) + \\eta y(n)$$\n\nem que:\n\n- $\\hat{y}(n)$ é a saída alvo\n- $y(n)$ é a saída da rede\n- $\\eta$ é a taxa de aprendizado\n\n\n## Treinamento do perceptron\n\n<br>\n\nA **taxa de aprendizagem** ($\\eta$) é um hiperparâmetro crítico no treinamento do perceptron:\n\n\n. . .\n\n- Se for **muito grande**, pode levar a oscilações e a não convergência do algoritmo. \n\n. . .\n\n- Se for **muito pequena**, o treinamento pode ser lento. \n\n\n. . .\n\nGeralmente seu valor varia de 0.1 a 1.0\n\n\n## Treinamento de redes MLP\n\n<br>\n\nO algoritmo de aprendizado da MLP é chamado **backpropagation** e é composto, basicamente, de duas etapas\n\n. . .\n\n- **Propagação:** Recebimento dos estímulos que é aplicado aos neurônios da rede, onde seu efeito se **propaga** camada por camada até produzir uma saída como resposta da rede. Neste passo **não há alteração** nos pesos sinápticos.\n\n\n\n\n## Treinamento de redes MLP\n\n<br>\n\n\nA **saída prevista** pela rede MLP é comparada com a **saída desejada** usando uma **função de custo**. A função de custo mede o quão bem a rede MLP está performando em relação ao objetivo desejado. \n\n\n\n. . .\n\n\n\n- **Retropropagação:** Após a saída, os pesos sinápticos são **ajustados** de acordo com a regra de correção de erro. Este sinal é propagado então para toda rede da saída para o entrada (caminho inverso), ou seja, o erro é **retropropagado**.\n\n\n\n\n## Treinamento de redes MLP\n\n\n![](/images/rna/backpropagation.gif){.nostretch fig-align=\"center\" width=\"1200px\"}\n\n\n\n## Ajuste de Hiperparâmetros\n\n\n<br>\n\n\n- **Número de Camadas Ocultas:** Determina a complexidade da rede.\n    - **Mais camadas:** Maior capacidade de modelar relações complexas. Contudo, pode resultar em redes mais lentas para treinar e maior risco de overfitting se não controlado.\n    - **Menos camadas:** Menor capacidade de abstração, podendo falhar em aprender padrões complexos dos dados.\n\n\n\n\n\n\n## Ajuste de Hiperparâmetros\n\n\n<br>\n\n\n- **Número de Neurônios:** Controla a capacidade de aprendizagem da rede.\n    - **Mais neurônios:** Maior capacidade de aprendizagem e representação dos dados, mas pode levar ao overfitting (modelo \"memorizando\" os dados de treinamento em vez de generalizar bem).\n    - **Menos neurônios:** Pode resultar em um modelo muito simples, incapaz de capturar as relações complexas dos dados.\n\n\n\n## Ajuste de Hiperparâmetros\n\n<br>\n\n\n- **Taxa de Aprendizado:** Define o quanto os pesos são atualizados a cada iteração.\n    - **Taxa muito alta:** O modelo pode \"saltar\" a solução ótima, resultando em um treinamento instável.\n    - **Taxa muito baixa:** O treinamento será mais lento, podendo até ficar preso em um ótimo local (máximo ou mínimo).\n    \n\n## Evitando Overfitting: Dropout\n\n\n- **Dropout** é uma técnica de regularização onde, durante o treinamento, neurônios são aleatoriamente \"desconectados\" (ou desligados) de cada vez. Isso impede que o modelo dependa demais de neurônios específicos.\n\n\n<span style='font-size:80px;'>&#128073;</span> Reduz a co-adaptação dos neurônios, forçando o modelo a aprender representações mais robustas. Aumenta a generalização do modelo e melhora o desempenho em dados não vistos.\n\n\n. . . \n\n- Durante o treinamento, uma fração dos neurônios (tipicamente 20%-50%) é removida aleatoriamente a cada iteração, dificultando a adaptação excessiva a um único conjunto de características.\n\n\n\n## Evitando Overfitting: Regularização L2\n\n\n- **A regularização L2** penaliza os pesos grandes ao adicionar uma soma dos quadrados dos pesos à função de custo do modelo. Isso evita que os neurônios se especializem demais em dados específicos.\n\n<span style='font-size:80px;'>&#128073;</span> Ao forçar os pesos a permanecerem pequenos, o modelo tende a ter uma capacidade de generalização maior, reduzindo o risco de overfitting. Penaliza fortemente pesos grandes, incentivando uma solução mais \"suave\" com menos variação nos parâmetros.\n\n. . . \n\n\n- A regularização L2 é muito eficaz para problemas com muitos atributos e ajuda a melhorar a estabilidade do modelo.\n\n\n\n\n\n## Vantagens e Desvantagens\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<br>\n\n<span style='font-size:160px;'>&#128077;</span>\n:::\n\n::: {.column width=\"90%\"}\n\n- **Capacidade de Modelar Relações Complexas:** As RNA são poderosas para capturar padrões não lineares e relações complexas nos dados.\n\n- **Adaptabilidade:** Podem ser aplicadas a uma ampla gama de problemas, tanto para classificação quanto para regressão, além de tarefas como visão computacional e processamento de linguagem natural.\n\n- **Melhoria com Dados Abundantes:** Seu desempenho tende a melhorar conforme a quantidade de dados aumenta, sendo uma escolha excelente em problemas com grandes volumes de dados.\n\n- **Desempenho em Dados Não Estruturados:** Muito eficazes em dados como imagens, áudio e texto, onde outras técnicas de modelagem podem não ser tão eficientes.\n:::\n\n::::\n\n## Vantagens e Desvantagens\n\n\n:::: {.columns}\n\n::: {.column width=\"10%\"}\n\n<br>\n\n<br>\n\n<span style='font-size:160px;'>&#128078;</span>\n:::\n\n::: {.column width=\"90%\"}\n\n\n- **Necessidade de Grande Quantidade de Dados:** As RNA tendem a ter um desempenho inferior quando a quantidade de dados é pequena. Elas precisam de muitos exemplos para generalizar corretamente.\n\n- **Custo Computacional Alto:** O treinamento de redes neurais pode ser extremamente intensivo em termos de tempo e recursos computacionais, especialmente para redes grandes e profundas.\n\n- **Falta de Interpretabilidade:** As RNA são frequentemente descritas como \"caixas pretas\", já que pode ser difícil entender como a rede toma decisões. Isso pode ser um problema em aplicações onde a explicação das decisões é crucial (por exemplo, na área da saúde ou finanças).\n\n- **Overfitting:** As RNA são propensas a overfitting, especialmente quando o número de camadas e neurônios é muito alto, ou quando não há dados suficientes para treinar adequadamente o modelo.\n\n:::\n\n::::\n\n\n\n## Pré-processamento para Redes Neurais Artificiais\n\n- **Limpeza de Dados:** Remover ou imputar valores ausentes e corrigir erros nos dados.\n\n- **Transformação de Variáveis Categóricas:** Usar One-Hot Encoding ou Label Encoding para converter variáveis categóricas em numéricas.\n\n- **Normalização/Padronização:** Escalonar as variáveis para que tenham a mesma escala.\n\n- **Normalização:** Ajuste para o intervalo [0, 1].\n\n- **Padronização:** Ajuste para média 0 e desvio padrão 1.\n\n- **Divisão dos Dados:** Separar os dados em conjuntos de treinamento e teste (ex: 80/20).\n\n- **Balanceamento de Classes (se necessário):** Usar técnicas como SMOTE ou subamostragem/sobreamostragem.\n\n\n\n## RNA para classificação no R e Python\n\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(nnet)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris <- iris %>% filter(Species != \"setosa\") %>% mutate(Species = factor(Species))\niris_split <- initial_split(iris, strata = Species)\ntrain_data <- training(iris_split)\ntest_data <- testing(iris_split)\n\nnn_model <- mlp() %>%\n  set_engine(\"nnet\") %>%\n  set_mode(\"classification\")\n\nnn_fit <- workflow() %>%\n  add_model(nn_model) %>%\n  add_formula(Species ~ .) %>%\n  fit(data = train_data)\n\nsp_predict <- predict(nn_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8846154\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data[iris.target != 0, :2]\ny = iris.target[iris.target != 0]\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\nnn_model = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=12345)\nnn_model.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=12345)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=12345)</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = nn_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.43333333333333335\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## RNA para regressão no R e Python\n\n::: {.panel-tabset}\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split <- initial_split(mtcars, strata = mpg)\ntrain_data <- training(mtcars_split)\ntest_data <- testing(mtcars_split)\n\nnn_reg <- mlp() %>%\n  set_engine(\"nnet\") %>%\n  set_mode(\"regression\")\n\nnn_reg_fit <- workflow() %>%\n  add_model(nn_reg) %>%\n  add_formula(mpg ~ .) %>%\n  fit(data = train_data)\n\nmpg_predict <- predict(nn_reg_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15.50235\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\nnn_reg = MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=12345)\nnn_reg.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=12345)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=12345)</pre></div></div></div></div></div>\n```\n\n:::\n\n```{.python .cell-code}\ny_pred = nn_reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.7468737160368192\n```\n\n\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "preditiva_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}