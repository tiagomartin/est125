[
  {
    "objectID": "projeto.html",
    "href": "projeto.html",
    "title": "Projeto Avaliativo",
    "section": "",
    "text": "Clique aqui para acessar o projeto!\n\n\n\nNeste projeto avaliativo, voc√™s atuar√£o como analistas de dados da empresa fict√≠cia ConecteMais Corp., uma das maiores provedoras de servi√ßos de telecomunica√ß√µes da Calif√≥rnia. A empresa enfrenta um problema cr√≠tico de aumento na taxa de churn (rotatividade de clientes), que afeta diretamente sua lucratividade e sustentabilidade no mercado competitivo.\nA miss√£o da equipe de voc√™s ser√° ajudar a empresa a entender os motivos que levam os clientes a cancelarem seus servi√ßos e a desenvolver solu√ß√µes baseadas em dados que contribuam para prever e reduzir o churn.\nO trabalho ser√° dividido em cinco fases, a serem realizadas de forma sequencial e com entregas parciais conforme o cronograma da disciplina.\n\n\n\n\n\n\nObjetivo: Entender o problema de neg√≥cios, o contexto organizacional, os pap√©is das √°reas envolvidas e o conjunto de dados fornecido.\nTarefas:\n\nInterpretar o problema da empresa com base nos dados e depoimentos.\nFormular hip√≥teses sobre poss√≠veis causas do churn.\nDefinir a vari√°vel-alvo (Churn Label).\nIdentificar o objetivo principal do projeto de minera√ß√£o de dados.\n\nEntreg√°vel: Documento com diagn√≥stico inicial e objetivos do projeto.\n\n\n\n\n\nObjetivo: Avaliar a qualidade dos dados fornecidos e propor t√©cnicas adequadas de pr√©-processamento.\nTarefas:\n\nIdentificar problemas como valores ausentes, colunas redundantes, codifica√ß√µes inconsistentes, outliers, desbalanceamento da vari√°vel-alvo, etc.\nPropor um plano de tratamento dos dados.\n\nEntreg√°vel: Relat√≥rio t√©cnico com diagn√≥stico dos dados e plano de pr√©-processamento.\n\n\n\n\n\nObjetivo: Utilizar t√©cnicas de classifica√ß√£o supervisionada para prever o churn.\nTarefas:\n\nEscolher algoritmos de classifica√ß√£o (por exemplo: √°rvore de decis√£o, random forest, SVM, etc.).\nAplicar valida√ß√£o cruzada.\nAvaliar os modelos com m√©tricas apropriadas (acur√°cia, AUC, F1-score, etc.).\nInterpretar os resultados e identificar vari√°veis mais importantes.\n\nEntreg√°vel: C√≥digo + relat√≥rio com os resultados da modelagem e an√°lise de desempenho.\n\n\n\n\n\nObjetivo: Aplicar t√©cnicas n√£o supervisionadas para entender diferentes perfis de clientes.\nTarefas:\n\nUtilizar algoritmos de agrupamento (ex.: K-means, DBSCAN, hier√°rquico).\nIdentificar padr√µes de comportamento entre grupos de clientes.\nRelacionar os segmentos encontrados com o risco de churn.\n\nEntreg√°vel: Relat√≥rio com an√°lise dos clusters, caracter√≠sticas de cada grupo e implica√ß√µes para o neg√≥cio.\n\n\n\n\n\nObjetivo: Desenvolver um sistema b√°sico de recomenda√ß√£o de a√ß√µes para reter clientes.\nTarefas:\n\nCriar regras ou modelos que recomendem ofertas, servi√ßos ou estrat√©gias de reten√ß√£o personalizadas com base no perfil e risco de churn do cliente.\n\nEntreg√°vel: Prot√≥tipo de sistema (simples) + documento com sugest√µes de a√ß√µes de reten√ß√£o baseadas nos dados.\n\n\n\n\n\nCada fase dever√° ser entregue via Moodle conforme o cronograma abaixo, respeitando os prazos de submiss√£o parcial. As entregas ser√£o avaliadas individualmente e contribuir√£o para a nota final do projeto.\n\nFase 1 + Fase 2 - data de entrega: 30/06/2025\nFase 3 - data de entrega: 21/07/2025\nFase 4 - data de entrega: 11/08/2025\nFase 5 - data de entrega: 02/09/2025\n\nObs.: Este cronograma poder√° sofrer mudan√ßas e adapta√ß√µes ao longo do semestre.\n\n\n\nNeste projeto, voc√™s devem se organizar em dois grupos (um com tr√™s e outro com quatro integrantes) e assumir o papel de um time de cientistas de dados atuando no mundo real, lidando com um problema concreto e multidisciplinar. Al√©m de aplicar t√©cnicas de minera√ß√£o de dados, espera-se que voc√™s:\n\nComuniquem suas descobertas de forma clara e orientada ao neg√≥cio.\nDesenvolvam solu√ß√µes que possam ser √∫teis e acion√°veis pela empresa.\nTrabalhem de forma colaborativa, dividindo responsabilidades.\n\n\n\n\nEste projeto √© uma oportunidade de integrar teoria e pr√°tica, aplicando o que foi aprendido em sala de aula em um desafio realista, com alto valor estrat√©gico. Usem a base de dados com esp√≠rito cr√≠tico, explorem os dados com criatividade e sempre mantenham em mente o impacto das suas an√°lises para o neg√≥cio da ConecteMais Corp.\nBoa sorte e bons insights! üìàüí°üìä",
    "crumbs": [
      "Projeto Avaliativo"
    ]
  },
  {
    "objectID": "projeto.html#vis√£o-geral-do-projeto",
    "href": "projeto.html#vis√£o-geral-do-projeto",
    "title": "Projeto Avaliativo",
    "section": "",
    "text": "Neste projeto avaliativo, voc√™s atuar√£o como analistas de dados da empresa fict√≠cia ConecteMais Corp., uma das maiores provedoras de servi√ßos de telecomunica√ß√µes da Calif√≥rnia. A empresa enfrenta um problema cr√≠tico de aumento na taxa de churn (rotatividade de clientes), que afeta diretamente sua lucratividade e sustentabilidade no mercado competitivo.\nA miss√£o da equipe de voc√™s ser√° ajudar a empresa a entender os motivos que levam os clientes a cancelarem seus servi√ßos e a desenvolver solu√ß√µes baseadas em dados que contribuam para prever e reduzir o churn.\nO trabalho ser√° dividido em cinco fases, a serem realizadas de forma sequencial e com entregas parciais conforme o cronograma da disciplina.",
    "crumbs": [
      "Projeto Avaliativo"
    ]
  },
  {
    "objectID": "projeto.html#estrutura-do-projeto-fases",
    "href": "projeto.html#estrutura-do-projeto-fases",
    "title": "Projeto Avaliativo",
    "section": "",
    "text": "Objetivo: Entender o problema de neg√≥cios, o contexto organizacional, os pap√©is das √°reas envolvidas e o conjunto de dados fornecido.\nTarefas:\n\nInterpretar o problema da empresa com base nos dados e depoimentos.\nFormular hip√≥teses sobre poss√≠veis causas do churn.\nDefinir a vari√°vel-alvo (Churn Label).\nIdentificar o objetivo principal do projeto de minera√ß√£o de dados.\n\nEntreg√°vel: Documento com diagn√≥stico inicial e objetivos do projeto.\n\n\n\n\n\nObjetivo: Avaliar a qualidade dos dados fornecidos e propor t√©cnicas adequadas de pr√©-processamento.\nTarefas:\n\nIdentificar problemas como valores ausentes, colunas redundantes, codifica√ß√µes inconsistentes, outliers, desbalanceamento da vari√°vel-alvo, etc.\nPropor um plano de tratamento dos dados.\n\nEntreg√°vel: Relat√≥rio t√©cnico com diagn√≥stico dos dados e plano de pr√©-processamento.\n\n\n\n\n\nObjetivo: Utilizar t√©cnicas de classifica√ß√£o supervisionada para prever o churn.\nTarefas:\n\nEscolher algoritmos de classifica√ß√£o (por exemplo: √°rvore de decis√£o, random forest, SVM, etc.).\nAplicar valida√ß√£o cruzada.\nAvaliar os modelos com m√©tricas apropriadas (acur√°cia, AUC, F1-score, etc.).\nInterpretar os resultados e identificar vari√°veis mais importantes.\n\nEntreg√°vel: C√≥digo + relat√≥rio com os resultados da modelagem e an√°lise de desempenho.\n\n\n\n\n\nObjetivo: Aplicar t√©cnicas n√£o supervisionadas para entender diferentes perfis de clientes.\nTarefas:\n\nUtilizar algoritmos de agrupamento (ex.: K-means, DBSCAN, hier√°rquico).\nIdentificar padr√µes de comportamento entre grupos de clientes.\nRelacionar os segmentos encontrados com o risco de churn.\n\nEntreg√°vel: Relat√≥rio com an√°lise dos clusters, caracter√≠sticas de cada grupo e implica√ß√µes para o neg√≥cio.\n\n\n\n\n\nObjetivo: Desenvolver um sistema b√°sico de recomenda√ß√£o de a√ß√µes para reter clientes.\nTarefas:\n\nCriar regras ou modelos que recomendem ofertas, servi√ßos ou estrat√©gias de reten√ß√£o personalizadas com base no perfil e risco de churn do cliente.\n\nEntreg√°vel: Prot√≥tipo de sistema (simples) + documento com sugest√µes de a√ß√µes de reten√ß√£o baseadas nos dados.",
    "crumbs": [
      "Projeto Avaliativo"
    ]
  },
  {
    "objectID": "projeto.html#entregas-e-cronograma",
    "href": "projeto.html#entregas-e-cronograma",
    "title": "Projeto Avaliativo",
    "section": "",
    "text": "Cada fase dever√° ser entregue via Moodle conforme o cronograma abaixo, respeitando os prazos de submiss√£o parcial. As entregas ser√£o avaliadas individualmente e contribuir√£o para a nota final do projeto.\n\nFase 1 + Fase 2 - data de entrega: 30/06/2025\nFase 3 - data de entrega: 21/07/2025\nFase 4 - data de entrega: 11/08/2025\nFase 5 - data de entrega: 02/09/2025\n\nObs.: Este cronograma poder√° sofrer mudan√ßas e adapta√ß√µes ao longo do semestre.",
    "crumbs": [
      "Projeto Avaliativo"
    ]
  },
  {
    "objectID": "projeto.html#papel-dos-alunos",
    "href": "projeto.html#papel-dos-alunos",
    "title": "Projeto Avaliativo",
    "section": "",
    "text": "Neste projeto, voc√™s devem se organizar em dois grupos (um com tr√™s e outro com quatro integrantes) e assumir o papel de um time de cientistas de dados atuando no mundo real, lidando com um problema concreto e multidisciplinar. Al√©m de aplicar t√©cnicas de minera√ß√£o de dados, espera-se que voc√™s:\n\nComuniquem suas descobertas de forma clara e orientada ao neg√≥cio.\nDesenvolvam solu√ß√µes que possam ser √∫teis e acion√°veis pela empresa.\nTrabalhem de forma colaborativa, dividindo responsabilidades.",
    "crumbs": [
      "Projeto Avaliativo"
    ]
  },
  {
    "objectID": "projeto.html#considera√ß√µes-finais",
    "href": "projeto.html#considera√ß√µes-finais",
    "title": "Projeto Avaliativo",
    "section": "",
    "text": "Este projeto √© uma oportunidade de integrar teoria e pr√°tica, aplicando o que foi aprendido em sala de aula em um desafio realista, com alto valor estrat√©gico. Usem a base de dados com esp√≠rito cr√≠tico, explorem os dados com criatividade e sempre mantenham em mente o impacto das suas an√°lises para o neg√≥cio da ConecteMais Corp.\nBoa sorte e bons insights! üìàüí°üìä",
    "crumbs": [
      "Projeto Avaliativo"
    ]
  },
  {
    "objectID": "programacao/semana-7.html",
    "href": "programacao/semana-7.html",
    "title": "Semana 07",
    "section": "",
    "text": "Vazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel"
  },
  {
    "objectID": "programacao/semana-7.html#slides",
    "href": "programacao/semana-7.html#slides",
    "title": "Semana 07",
    "section": "Slides",
    "text": "Slides\n An√°lise preditiva"
  },
  {
    "objectID": "programacao/semana-5.html",
    "href": "programacao/semana-5.html",
    "title": "Semana 05",
    "section": "",
    "text": "Pr√©-processamento usando a base de dados Space Titanic"
  },
  {
    "objectID": "programacao/semana-5.html#script-pr√©-processamento-usando-a-base-de-dados-space-titanic-no-r",
    "href": "programacao/semana-5.html#script-pr√©-processamento-usando-a-base-de-dados-space-titanic-no-r",
    "title": "Semana 05",
    "section": "",
    "text": "Pr√©-processamento usando a base de dados Space Titanic"
  },
  {
    "objectID": "programacao/semana-5.html#material-complementar",
    "href": "programacao/semana-5.html#material-complementar",
    "title": "Semana 05",
    "section": "Material complementar",
    "text": "Material complementar\n Desvendando o pacote tidymodels"
  },
  {
    "objectID": "programacao/semana-5.html#slides",
    "href": "programacao/semana-5.html#slides",
    "title": "Semana 05",
    "section": "Slides",
    "text": "Slides\n An√°lise preditiva"
  },
  {
    "objectID": "programacao/semana-5.html#projeto",
    "href": "programacao/semana-5.html#projeto",
    "title": "Semana 05",
    "section": "Projeto",
    "text": "Projeto\n Projeto: An√°lise de Churn na Empresa ConecteMais Corp."
  },
  {
    "objectID": "programacao/semana-3.html",
    "href": "programacao/semana-3.html",
    "title": "Semana 03",
    "section": "",
    "text": "Pr√©-processamento de dados"
  },
  {
    "objectID": "programacao/semana-3.html#slides",
    "href": "programacao/semana-3.html#slides",
    "title": "Semana 03",
    "section": "",
    "text": "Pr√©-processamento de dados"
  },
  {
    "objectID": "programacao/semana-3.html#script-an√°lise-de-dados-explorando-a-base-de-dados-data-science-salaries-2023-do-kaggle",
    "href": "programacao/semana-3.html#script-an√°lise-de-dados-explorando-a-base-de-dados-data-science-salaries-2023-do-kaggle",
    "title": "Semana 03",
    "section": "Script an√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "text": "Script an√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle\n EDA: Sal√°rios em Ci√™ncia de Dados"
  },
  {
    "objectID": "programacao/semana-1.html",
    "href": "programacao/semana-1.html",
    "title": "Semana 01",
    "section": "",
    "text": "Sejam bem-vindos √† disciplina EST125 - Introdu√ß√£o √† Minera√ß√£o de Dados.\nLeiam com aten√ß√£o o plano de ensino da disciplina. Nele est√£o as regras do jogo!"
  },
  {
    "objectID": "material_complementar/vazamento.html",
    "href": "material_complementar/vazamento.html",
    "title": "Vazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel",
    "section": "",
    "text": "Hoje vamos mergulhar em um problema sutil, mas extremamente perigoso no mundo da modelagem preditiva: o vazamento de dados (ou data leakage). Se n√£o diagnosticado e tratado corretamente, ele pode levar √† cria√ß√£o de modelos que parecem incrivelmente precisos em seus dados de treino, mas falham miseravelmente ao serem aplicados a dados reais e n√£o vistos."
  },
  {
    "objectID": "material_complementar/vazamento.html#o-que-√©-vazamento-de-dados-data-leakage",
    "href": "material_complementar/vazamento.html#o-que-√©-vazamento-de-dados-data-leakage",
    "title": "Vazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel",
    "section": "ü§î O que √© vazamento de dados (data leakage)?",
    "text": "ü§î O que √© vazamento de dados (data leakage)?\nImagine que voc√™ est√° tentando prever se um paciente tem uma determinada doen√ßa com base em seus exames. Se voc√™ incluir no seu conjunto de dados de treino uma informa√ß√£o que s√≥ estaria dispon√≠vel ap√≥s o diagn√≥stico (por exemplo, um c√≥digo de procedimento m√©dico realizado ap√≥s a confirma√ß√£o da doen√ßa), seu modelo ‚Äúaprender√°‚Äù essa correla√ß√£o esp√∫ria. Ele n√£o estar√° realmente identificando os fatores de risco da doen√ßa, mas sim ‚Äúcolando‚Äù a resposta com base em informa√ß√µes futuras.\nData leakage ocorre quando informa√ß√µes que n√£o estariam dispon√≠veis no momento da predi√ß√£o s√£o utilizadas no treinamento do modelo. Isso faz com que o modelo aprenda padr√µes irreais, que n√£o se sustentam em dados do mundo real.\nEm outras palavras, o modelo ‚Äúcola‚Äù na resposta porque tem acesso a informa√ß√µes que n√£o deveria ter. Isso gera uma falsa sensa√ß√£o de desempenho alto durante o treinamento e valida√ß√£o."
  },
  {
    "objectID": "material_complementar/vazamento.html#consequ√™ncias-do-vazamento-de-dados",
    "href": "material_complementar/vazamento.html#consequ√™ncias-do-vazamento-de-dados",
    "title": "Vazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel",
    "section": "‚ö†Ô∏è Consequ√™ncias do vazamento de dados",
    "text": "‚ö†Ô∏è Consequ√™ncias do vazamento de dados\n\nüìâ Overfitting extremo: o modelo aprende rela√ß√µes esp√∫rias que n√£o se generalizam.\nü§• Avalia√ß√µes enganosas: m√©tricas como acur√°cia ou AUC s√£o infladas artificialmente.\nüö´ Falhas na produ√ß√£o: quando aplicado em novos dados, o modelo apresenta desempenho muito inferior.\nüí∏ Decis√µes erradas: em contextos reais (sa√∫de, finan√ßas, marketing), isso pode levar a preju√≠zos graves."
  },
  {
    "objectID": "material_complementar/vazamento.html#o-que-pode-causar-data-leakage",
    "href": "material_complementar/vazamento.html#o-que-pode-causar-data-leakage",
    "title": "Vazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel",
    "section": "üßê O que pode causar Data Leakage?",
    "text": "üßê O que pode causar Data Leakage?\nDiversas pr√°ticas (muitas vezes bem-intencionadas) podem causar vazamento de dados:\n\nPr√©-processamento antes da separa√ß√£o treino/teste\n\nEx: imputar valores ausentes usando a m√©dia de toda a base, inclusive do conjunto de teste.\n\nVari√°veis que refletem eventos futuros\n\nEx: tentar prever inadimpl√™ncia usando a vari√°vel ‚Äúpagamento em atraso‚Äù, que s√≥ √© registrada ap√≥s o evento.\n\nVari√°veis altamente correlacionadas com o alvo\n\nEx: incluir o valor da fatura final ao prever se o cliente vai pagar ‚Äî esse valor depende justamente do pagamento.\n\nCria√ß√£o de vari√°veis a partir do target (target leakage)\n\nEx: criar vari√°veis com base no r√≥tulo da vari√°vel-resposta, direta ou indiretamente.\n\nInforma√ß√µes compartilhadas entre treino e teste\n\nEx: repetir registros ou agrupar dados por cliente sem cuidado pode gerar vazamento entre observa√ß√µes de treino e teste."
  },
  {
    "objectID": "material_complementar/vazamento.html#como-evitar-o-data-leakage",
    "href": "material_complementar/vazamento.html#como-evitar-o-data-leakage",
    "title": "Vazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel",
    "section": "üß† Como evitar o Data Leakage?",
    "text": "üß† Como evitar o Data Leakage?\nA boa not√≠cia √© que o vazamento de dados pode ser evitado com boas pr√°ticas:\n\nEntendimento profundo dos dados e do problema: Antes de qualquer coisa, √© crucial entender a origem, o significado e o fluxo dos seus dados. Quais vari√°veis est√£o dispon√≠veis em que momento? Qual √© a linha do tempo dos eventos? Essa compreens√£o ajudar√° a identificar potenciais fontes de vazamento.\nSepara√ß√£o rigorosa dos conjuntos de dados: A divis√£o entre dados de treino, valida√ß√£o e teste deve ser feita antes de qualquer etapa de pr√©-processamento ou engenharia de features. O conjunto de teste s√≥ deve ser usado para avaliar o desempenho final do modelo, ap√≥s todo o desenvolvimento. O conjunto de valida√ß√£o serve para ajustar hiperpar√¢metros e comparar diferentes modelos, sem ‚Äúespiar‚Äù os dados de teste.\nPr√©-processamento e engenharia de features dentro dos folds de valida√ß√£o cruzada: Se voc√™ estiver usando valida√ß√£o cruzada, certifique-se de que qualquer etapa de pr√©-processamento (escalonamento, imputa√ß√£o de valores faltantes, codifica√ß√£o de vari√°veis categ√≥ricas) e engenharia de features seja realizada apenas nos dados de treino de cada fold. As estat√≠sticas e transforma√ß√µes aprendidas em um fold de treino devem ser aplicadas separadamente ao fold de valida√ß√£o correspondente. Isso evita que informa√ß√µes do fold de valida√ß√£o ‚Äúvazem‚Äù para o treino.\nCuidado com vari√°veis derivadas do alvo: Tenha extrema cautela ao criar novas vari√°veis que possam ser derivadas diretamente da vari√°vel alvo ou de informa√ß√µes que s√≥ estariam dispon√≠veis ap√≥s a ocorr√™ncia do evento que voc√™ est√° tentando prever. O exemplo do c√≥digo de procedimento m√©dico √© um caso cl√°ssico.\nAn√°lise temporal cuidadosa: Em problemas com dados temporais, a separa√ß√£o dos conjuntos de treino e teste deve respeitar a ordem cronol√≥gica. Usar dados futuros para treinar um modelo que pretende prever o passado (ou o presente) √© uma receita certa para o vazamento. T√©cnicas como time series split s√£o essenciais nesses casos.\nPipeline de modelagem: A cria√ß√£o de um pipeline bem definido, que encapsula todas as etapas de pr√©-processamento, engenharia de features e treinamento do modelo, ajuda a garantir que o processo seja aplicado de forma consistente e a evitar erros que possam levar ao vazamento.\nAuditoria e revis√£o do c√≥digo: Pe√ßa a colegas para revisarem seu c√≥digo em busca de potenciais fontes de vazamento. Uma nova perspectiva pode identificar problemas que voc√™ pode ter perdido."
  },
  {
    "objectID": "material_complementar/vazamento.html#conclus√£o",
    "href": "material_complementar/vazamento.html#conclus√£o",
    "title": "Vazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel",
    "section": "‚ú® Conclus√£o",
    "text": "‚ú® Conclus√£o\nO vazamento de dados √© sorrateiro. Ele n√£o lan√ßa erros, nem avisa que est√° ali. Mas pode comprometer todo o seu projeto de ci√™ncia de dados.\nPor isso, desconfie de modelos perfeitos demais, questione suas vari√°veis e sempre pergunte:\n\n‚ÄúEssa informa√ß√£o estaria dispon√≠vel na vida real no momento da previs√£o?‚Äù\n\nEvitar o data leakage √© mais do que uma pr√°tica t√©cnica ‚Äî √© um exerc√≠cio de pensamento cr√≠tico e rigor metodol√≥gico."
  },
  {
    "objectID": "material_complementar/desbalanceamento.html",
    "href": "material_complementar/desbalanceamento.html",
    "title": "Desbalanceamento de Classes: Um Problema Real em Ci√™ncia de Dados",
    "section": "",
    "text": "Hoje vamos conversar sobre um desafio comum e muitas vezes subestimado em projetos de ci√™ncia de dados e aprendizagem de m√°quina: o desbalanceamento de classes. Esse problema ocorre quando as classes de interesse em um conjunto de dados n√£o s√£o representadas igualmente. Por exemplo, em um problema de detec√ß√£o de fraude, a maioria das transa√ß√µes ser√° leg√≠tima (classe majorit√°ria), enquanto pouqu√≠ssimas ser√£o fraudulentas (classe minorit√°ria)."
  },
  {
    "objectID": "material_complementar/desbalanceamento.html#quando-o-desbalanceamento-se-torna-um-problema",
    "href": "material_complementar/desbalanceamento.html#quando-o-desbalanceamento-se-torna-um-problema",
    "title": "Desbalanceamento de Classes: Um Problema Real em Ci√™ncia de Dados",
    "section": "Quando o desbalanceamento se torna um problema?",
    "text": "Quando o desbalanceamento se torna um problema?\nN√£o existe uma regra universal para definir a partir de qual propor√ß√£o um conjunto de dados √© ‚Äúdesbalanceado‚Äù. A verdade √© que a defini√ß√£o de desbalanceamento √© contextual e depende do problema que voc√™ est√° resolvendo.\nNo entanto, para efeitos pr√°ticos, se a sua classe de interesse (geralmente a minorit√°ria) representa menos de 40% do seu conjunto de dados, j√° √© um ind√≠cio de que a base de dados √© desbalanceada e podemos pensar em estrat√©gias para lidar com isso. Quanto menor essa propor√ß√£o, mais cr√≠tico se torna o problema. Cen√°rios com 1:100 (1% da classe minorit√°ria), como detec√ß√£o de fraudes ou doen√ßas raras, s√£o considerados severamente desbalanceados e exigem tratamento espec√≠fico. Propor√ß√µes ainda mais extremas, como 1:1000 ou menos, s√£o comuns em dom√≠nios como a detec√ß√£o de ataques cibern√©ticos.\n\n\nA tabela a seguir apresenta os nomes e as faixas geralmente aceitas para diferentes graus de desbalanceamento de classes:\n\n\n\n\n\nPorcentagem de dados pertencentes √† classe minorit√°ria\n\n\nGrau de desequil√≠brio\n\n\n\n\n\n\n20%-40% do conjunto de dados\n\n\nLeve\n\n\n\n\n1%-20% do conjunto de dados\n\n\nModerado\n\n\n\n\n&lt;1% do conjunto de dados\n\n\nExtremo"
  },
  {
    "objectID": "material_complementar/desbalanceamento.html#consequ√™ncias-do-desbalanceamento-de-classes",
    "href": "material_complementar/desbalanceamento.html#consequ√™ncias-do-desbalanceamento-de-classes",
    "title": "Desbalanceamento de Classes: Um Problema Real em Ci√™ncia de Dados",
    "section": "Consequ√™ncias do desbalanceamento de classes",
    "text": "Consequ√™ncias do desbalanceamento de classes\nO desbalanceamento de classes pode ter um impacto significativo na performance dos seus modelos, levando a conclus√µes enganosas. Aqui est√£o as principais consequ√™ncias:\n\nVi√©s para a Classe Majorit√°ria: Modelos de aprendizagem de m√°quina tendem a ser otimizados para a acur√°cia geral. Quando h√° um desbalanceamento, eles podem aprender a prever a classe majorit√°ria com alta frequ√™ncia para minimizar o erro total. Isso resulta em uma acur√°cia inflacionada, que n√£o reflete a capacidade real do modelo de identificar a classe minorit√°ria, geralmente a de maior interesse (ex: fraudes, doen√ßas raras).\nBaixo Desempenho na Classe Minorit√°ria: A consequ√™ncia direta do vi√©s √© que o modelo ter√° dificuldade em identificar corretamente as inst√¢ncias da classe minorit√°ria. Isso se traduz em m√©tricas como precis√£o, recall e F1-score muito baixas para essa classe, indicando que o modelo √© ineficaz para o objetivo principal.\nModelos In√∫teis na Pr√°tica: Um modelo que n√£o consegue identificar a classe minorit√°ria, mesmo com uma alta acur√°cia geral, √© frequentemente in√∫til em cen√°rios reais. Em muitos problemas, a classe minorit√°ria √© a que carrega o maior valor ou risco (fraudes, doen√ßas, defeitos). Mesmo um desbalanceamento ‚Äúmoderado‚Äù pode significar a perda de oportunidades cruciais ou a falha na detec√ß√£o de eventos importantes."
  },
  {
    "objectID": "material_complementar/desbalanceamento.html#como-lidar-com-o-desbalanceamento",
    "href": "material_complementar/desbalanceamento.html#como-lidar-com-o-desbalanceamento",
    "title": "Desbalanceamento de Classes: Um Problema Real em Ci√™ncia de Dados",
    "section": "Como lidar com o desbalanceamento?",
    "text": "Como lidar com o desbalanceamento?\nLidar com o desbalanceamento de classes √© um passo crucial para construir modelos preditivos eficazes, especialmente quando a classe minorit√°ria √© a de maior interesse. Felizmente, existem diversas estrat√©gias que podemos empregar para mitigar esse problema. Vamos explorar as principais, acompanhadas de exemplos pr√°ticos em R.\n\n1. Reamostragem (Resampling)\nA reamostragem √© uma das abordagens mais diretas e populares, focando em alterar a propor√ß√£o das classes no seu conjunto de dados de treinamento.\n\n\n\n\n\nExistem duas vertentes principais, que tamb√©m podem trabalhar simultaneamente:\n\nOversampling (Sobreamostragem): A ideia do oversampling √© aumentar o n√∫mero de inst√¢ncias da classe minorit√°ria, tornando-a mais ‚Äúvis√≠vel‚Äù para o algoritmo de aprendizado. Uma t√©cnica amplamente utilizada √© o SMOTE (Synthetic Minority Over-sampling Technique). O SMOTE n√£o duplica as amostras existentes; ele cria novos exemplos sint√©ticos da classe minorit√°ria, baseando-se nas similaridades entre os exemplos minorit√°rios existentes (seus vizinhos mais pr√≥ximos). Isso ajuda a expandir a representa√ß√£o da classe minorit√°ria sem introduzir overfitting por repeti√ß√£o exata de dados.\n\n\n\n\n\n\n\n# Exemplo de SMOTE em R\n# Primeiramente, instale o pacote 'smotefamily' se ainda n√£o tiver:\n# install.packages(\"smotefamily\")\nlibrary(smotefamily)\nlibrary(tidyverse) \nlibrary(ROSE)\n\ndata(hacide)\n\n# Gerar o gr√°fico de barras\nggplot(dados_treino, aes(x = target, fill = target)) +\n  geom_bar() +\n  # Adicionar r√≥tulos de contagem em cima das barras\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +\n  labs(\n    title = \"Distribui√ß√£o das Classes em Dados Sint√©ticos Desbalanceados\",\n    x = \"Classe\",\n    y = \"Contagem de Amostras\",\n    fill = \"Classe\"\n  ) +\n  scale_fill_manual(values = c(\"1\" = \"#FD800D\", \"0\" = \"#2077B5\")) + # Cores personalizadas\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) # Centraliza o t√≠tulo\n\n\n\n\n\n\n\n# Preparando os dados para smotefamily::SMOTE()\n# A fun√ß√£o SMOTE() do smotefamily espera:\n# - X: um dataframe de vari√°veis preditoras (apenas num√©ricas)\n# - target: um vetor da vari√°vel target\n# - K: n√∫mero de vizinhos (padr√£o √© 5)\n# - dup_size: um fator de duplica√ß√£o para a classe minorit√°ria.\n#             Se a minorit√°ria √© 100 e queremos que ela tenha 300 (3x), dup_size seria 2 (100 + 2*100 = 300)\n\n# Separando as features (X) da vari√°vel target\nX_features &lt;- dados_treino %&gt;% dplyr::select(-target)\ny_target &lt;- dados_treino$target\n\n# Aplicando SMOTE usando smotefamily\n# Queremos que a classe '1' (minorit√°ria) tenha aproximadamente 33 vezes o n√∫mero original de exemplos.\n# O n√∫mero original √© 43. Queremos aproximadamente 1450.\n# O fator de duplica√ß√£o (dup_size) √© o n√∫mero de c√≥pias sint√©ticas *al√©m* das originais.\n# Ent√£o, para ir de 43 para 1450, precisamos de aproximadamente 1400 novas amostras, o que √© 33x o original.\n# Se target='1' tem 43 amostras, e voc√™ quer 'dup_size=33', voc√™ ter√° 43 (originais) + 33*43 (sint√©ticas) = 1462 amostras '1'.\ndados_smote &lt;- SMOTE(X = X_features, target = y_target, K = 5, dup_size = 33)\n\n# O resultado vem como uma lista, precisamos extrair o dataframe balanceado\ndados_smote_df &lt;- dados_smote$data\n\n\n# Gerar o gr√°fico de barras\nggplot(dados_smote_df, aes(x = class, fill = class)) +\n  geom_bar() +\n  # Adicionar r√≥tulos de contagem em cima das barras\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +\n  labs(\n    title = \"Distribui√ß√£o das classes ap√≥s SMOTE\",\n    x = \"Classe\",\n    y = \"Contagem de Amostras\",\n    fill = \"Classe\"\n  ) +\n  scale_fill_manual(values = c(\"1\" = \"#FD800D\", \"0\" = \"#2077B5\")) + # Cores personalizadas\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) # Centraliza o t√≠tulo\n\n\n\n\n\n\n\n\n\nUndersampling (Subamostragem): Em contraste com o oversampling, o undersampling visa reduzir o n√∫mero de inst√¢ncias da classe majorit√°ria para equilibrar as propor√ß√µes. A forma mais simples √© a subamostragem aleat√≥ria, onde inst√¢ncias da classe majorit√°ria s√£o removidas aleatoriamente at√© que o balan√ßo desejado seja atingido. Embora possa ser eficaz, a principal desvantagem √© a potencial perda de informa√ß√µes importantes contidas nas amostras descartadas da classe majorit√°ria. Por isso, deve ser usado com cautela, especialmente em datasets menores.\n\n\n# Exemplo de Undersampling aleat√≥rio em R\n# Instale o pacote 'ROSE' se ainda n√£o tiver:\n# install.packages(\"ROSE\")\nlibrary(ROSE)\n\n# Usando os mesmos 'dados' desbalanceados criados anteriormente\n# Aplicando Undersampling: o m√©todo \"under\" do ovun.balance tentar√° equilibrar as classes\n# reduzindo a majorit√°ria para o mesmo n√∫mero de exemplos da minorit√°ria.\ndados_under &lt;- ovun.sample(target ~ ., data = dados_treino, method = \"under\")$data\n\n# Gerar o gr√°fico de barras\nggplot(dados_under, aes(x = target, fill = target)) +\n  geom_bar() +\n  # Adicionar r√≥tulos de contagem em cima das barras\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +\n  labs(\n    title = \"Distribui√ß√£o das classes ap√≥s undersampling\",\n    x = \"Classe\",\n    y = \"Contagem de Amostras\",\n    fill = \"Classe\"\n  ) +\n  scale_fill_manual(values = c(\"1\" = \"#FD800D\", \"0\" = \"#2077B5\")) + # Cores personalizadas\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) # Centraliza o t√≠tulo\n\n\n\n\n\n\n\n\n\nOversampling e undersampling combinados: A combina√ß√£o de oversampling e undersampling √© frequentemente a estrat√©gia mais eficaz, especialmente em casos de desbalanceamento severo. A ideia √© aumentar a classe minorit√°ria (com SMOTE, por exemplo) e, ao mesmo tempo, reduzir a classe majorit√°ria, mas sem perder informa√ß√µes cruciais. Isso permite um controle mais fino sobre a propor√ß√£o final das classes e pode resultar em modelos mais robustos.\n\n\n# Exemplo de Oversampling e Undersampling simult√¢neos com ROSE\n# Instale o pacote 'ROSE' se ainda n√£o tiver:\n# install.packages(\"ROSE\")\nlibrary(ROSE)\n\n# Usando os mesmos 'dados' desbalanceados criados anteriormente\n# Aplicando reamostragem h√≠brida: tentando balancear as classes\n# 'method = \"both\"' usa uma combina√ß√£o de oversampling e undersampling.\ndados_hibrido &lt;- ovun.sample(target ~ ., data = dados_treino, method = \"both\")$data\n\n# Gerar o gr√°fico de barras\nggplot(dados_hibrido, aes(x = target, fill = target)) +\n  geom_bar() +\n  # Adicionar r√≥tulos de contagem em cima das barras\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +\n  labs(\n    title = \"Distribui√ß√£o das classes ap√≥s combina√ß√£o de over e undersampling\",\n    x = \"Classe\",\n    y = \"Contagem de Amostras\",\n    fill = \"Classe\"\n  ) +\n  scale_fill_manual(values = c(\"1\" = \"#FD800D\", \"0\" = \"#2077B5\")) + # Cores personalizadas\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) # Centraliza o t√≠tulo\n\n\n\n\n\n\n\n\n\n\n2. Mudan√ßa no Algoritmo: Ajustes Intr√≠nsecos para o Desbalanceamento\nAlguns algoritmos de aprendizado de m√°quina oferecem mecanismos internos ou par√¢metros que podem ser ajustados para lidar com o desbalanceamento, sem a necessidade de reamostrar o conjunto de dados diretamente. Esses algoritmos permitem atribuir ‚Äúcustos‚Äù diferentes a erros de classifica√ß√£o. Por exemplo, em um problema de detec√ß√£o de doen√ßas raras, um falso negativo (n√£o detectar a doen√ßa quando ela existe) pode ter um custo muito maior (ex: vida humana) do que um falso positivo (diagnosticar a doen√ßa quando ela n√£o existe). Ao definir uma matriz de custos, o algoritmo ser√° penalizado mais severamente por cometer erros na classe minorit√°ria, incentivando-o a classific√°-la corretamente.\n\nAlgoritmos de Cost-Sensitive Learning:\n\n√Årvores de Decis√£o\nM√°quinas de Vetor de Suporte\nBoosting Algorithms\nRedes Neurais Artificiais\nRegress√£o Log√≠stica\n\n\n\nExemplo Pr√°tico: Random Forest com pesos de classe\nVamos primeiro treinar um modelo de Random Forest padr√£o para ver como ele se comporta sem nenhuma considera√ß√£o de custo.\n\nlibrary(randomForest)\nlibrary(caret)\nlibrary(pROC)\n# Treinar o modelo Random Forest\n# ntree: n√∫mero de √°rvores na floresta\n# importance: calcula a import√¢ncia das vari√°veis (opcional, mas √∫til)\nmodelo_rf_base &lt;- randomForest(target ~ ., data = dados_treino, ntree = 500, classwt = NULL, importance = TRUE)\n\n# Fazer previs√µes no conjunto de teste\n# type=\"prob\" para obter as probabilidades das classes\npredicoes_rf_base &lt;- predict(modelo_rf_base, newdata = dados_teste, type = \"class\")\n\n# Avaliar o modelo\nmc_rf_base &lt;- confusionMatrix(predicoes_rf_base, dados_teste$target, positive = \"1\")\nmc_rf_base\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 482  17\n         1   1   0\n                                          \n               Accuracy : 0.964           \n                 95% CI : (0.9437, 0.9785)\n    No Information Rate : 0.966           \n    P-Value [Acc &gt; NIR] : 0.656554        \n                                          \n                  Kappa : -0.0038         \n                                          \n Mcnemar's Test P-Value : 0.000407        \n                                          \n            Sensitivity : 0.0000          \n            Specificity : 0.9979          \n         Pos Pred Value : 0.0000          \n         Neg Pred Value : 0.9659          \n             Prevalence : 0.0340          \n         Detection Rate : 0.0000          \n   Detection Prevalence : 0.0020          \n      Balanced Accuracy : 0.4990          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nNote que temos uma acur√°cia geral alta, mas um Recall (Sensitivity) baixo para a classe ‚Äúp‚Äù (minorit√°ria), indicando que o modelo tem dificuldade em detect√°-la. Agora, vamos treinar outro modelo Random Forest, mas desta vez, aplicando pesos de classe para dar mais import√¢ncia √† classe minorit√°ria.\nNo nosso banco de dados de treinamento,\n\nfreq_classes &lt;- table(dados_treino$target)\nfreq_classes\n\n\n   0    1 \n1457   43 \n\n\na propor√ß√£o √© 1457 indiv√≠duos na ‚Äú0‚Äù e 43 na classe ‚Äú1‚Äù, a classe ‚Äú0‚Äù tem aproximadamente 34 vezes mais amostras. Para dar mais peso √† classe ‚Äú1‚Äù, poder√≠amos usar:\nPesos 0 = 1\nPesos 1 = (N√∫mero de amostras '0') / (N√∫mero de amostras '1')\nOu, mais genericamente, a frequ√™ncia inversa:\nPesos 0 = 1 / Frequ√™ncia de 0\nPesos 1 = 1 / Frequ√™ncia de 1\n\n# Definir os pesos de classe\n# Dar um peso maior √† classe '1' (minorit√°ria)\n\npesos_classes &lt;- 1 / freq_classes\npesos_classes &lt;- pesos_classes / min(pesos_classes) # Normaliza para que o menor peso seja 1\n\n# Treinar o modelo Random Forest com pesos de classe\nmodelo_rf_custo &lt;- randomForest(target ~ .,\n                                data = dados_treino,\n                                ntree = 500,\n                                classwt = pesos_classes, # AQUI aplicamos os pesos de classe!\n                                importance = TRUE)\n\n# Fazer previs√µes no conjunto de teste\npredicoes_rf_custo &lt;- predict(modelo_rf_custo, newdata = dados_teste, type = \"class\")\n\n# Avaliar o modelo\nmc_rf_custo &lt;- confusionMatrix(predicoes_rf_custo, dados_teste$target, positive = \"1\")\nmc_rf_custo\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 483  17\n         1   0   0\n                                          \n               Accuracy : 0.966           \n                 95% CI : (0.9461, 0.9801)\n    No Information Rate : 0.966           \n    P-Value [Acc &gt; NIR] : 0.5640325       \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : 0.0001042       \n                                          \n            Sensitivity : 0.000           \n            Specificity : 1.000           \n         Pos Pred Value :   NaN           \n         Neg Pred Value : 0.966           \n             Prevalence : 0.034           \n         Detection Rate : 0.000           \n   Detection Prevalence : 0.000           \n      Balanced Accuracy : 0.500           \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n\n\n3. Mudan√ßa nas m√©tricas de avalia√ß√£o\nComo discutido anteriormente, a acur√°cia √© uma m√©trica extremamente enganosa em conjuntos de dados desbalanceados. √â absolutamente essencial mudar o foco para m√©tricas que forne√ßam uma imagem mais fiel do desempenho do seu modelo, especialmente na classe minorit√°ria.\n\nPrecis√£o (Precision): Responde √† pergunta: ‚ÄúDas vezes que meu modelo previu a classe positiva, quantas estavam realmente corretas?‚Äù.\nRecall (Sensibilidade/Revoca√ß√£o): Responde √† pergunta: ‚ÄúDas vezes que a classe positiva realmente ocorreu, quantas meu modelo conseguiu detectar?‚Äù. Para a classe minorit√°ria, o recall √© frequentemente a m√©trica mais cr√≠tica, pois indica a capacidade do modelo de ‚Äúencontrar‚Äù as inst√¢ncias raras.\nF1-Score: √â a m√©dia harm√¥nica da Precis√£o e do Recall. √â uma m√©trica √∫til quando voc√™ precisa de um equil√≠brio entre n√£o ter muitos falsos positivos (precis√£o) e n√£o perder muitos positivos reais (recall).\nCurvas ROC (Receiver Operating Characteristic) e PR (Precision-Recall):\n\nA AUC-ROC avalia a capacidade do modelo de distinguir entre as classes em diferentes limiares de decis√£o.\nA AUC-PR √© frequentemente prefer√≠vel √† AUC-ROC para conjuntos de dados altamente desbalanceados. Ela se concentra especificamente na rela√ß√£o entre Precis√£o e Recall, que s√£o m√©tricas mais informativas sobre o desempenho da classe minorit√°ria quando h√° um grande desequil√≠brio. Uma alta AUC-PR indica um bom desempenho na identifica√ß√£o da classe positiva sem um excesso de falsos positivos."
  },
  {
    "objectID": "material_complementar/desbalanceamento.html#conclus√£o",
    "href": "material_complementar/desbalanceamento.html#conclus√£o",
    "title": "Desbalanceamento de Classes: Um Problema Real em Ci√™ncia de Dados",
    "section": "Conclus√£o",
    "text": "Conclus√£o\nLidar com o desbalanceamento de classes √© mais do que apenas uma tarefa t√©cnica; √© uma quest√£o de garantir que seus modelos sejam √∫teis e confi√°veis no mundo real, especialmente para os eventos raros, mas cr√≠ticos. A chave √© identificar o problema cedo (lembre-se: se a classe minorit√°ria est√° abaixo de 40%, j√° √© um alerta!), compreender suas consequ√™ncias e aplicar as t√©cnicas mais adequadas de reamostragem (incluindo a abordagem h√≠brida), ajuste de algoritmo ou avalia√ß√£o de m√©tricas.\nN√£o existe uma solu√ß√£o √∫nica que sirva para todos os casos. A escolha da melhor abordagem depende do seu conjunto de dados, do algoritmo que voc√™ est√° usando e, fundamentalmente, do objetivo de neg√≥cio. A experimenta√ß√£o e a valida√ß√£o cuidadosa s√£o sempre os melhores caminhos!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introdu√ß√£o √† Minera√ß√£o de Dados",
    "section": "",
    "text": "P√°gina dedicada √† disciplina EST125 - Introdu√ß√£o √† Minera√ß√£o de Dados\n\n\nA minera√ß√£o de dados √© o processo de explorar grandes conjuntos de dados para extrair informa√ß√µes valiosas e √∫teis. Essa t√©cnica √© utilizada em diversas √°reas, como neg√≥cios, sa√∫de, ci√™ncia e governo, para identificar padr√µes, prever comportamentos e tomar decis√µes informadas.\n\n\nEmenta\nDefini√ß√£o. Rela√ß√£o com a descoberta de conhecimento, estat√≠stica e intelig√™ncia computacional. Obten√ß√£o, normaliza√ß√£o e limpeza de dados. Sele√ß√£o, transforma√ß√£o e minera√ß√£o. Classifica√ß√£o e regress√£o. Modelos e algoritmos para minera√ß√£o de dados. Extra√ß√£o de informa√ß√µes e s√≠ntese. Classifica√ß√£o supervisionada, n√£o-supervisionada e algoritmos h√≠bridos. Arvores de decis√£o e sua cria√ß√£o. Algoritmos e t√©cnicas de regress√£o. Intera√ß√£o entre t√©cnicas de minera√ß√£o de dados e outras t√©cnicas computacionais.\n\n\nConte√∫do Program√°tico\n\nConceitos iniciais: Defini√ß√£o de Minera√ß√£o de Dados. Rela√ß√£o do processo de minera√ß√£o de dados com descoberta de conhecimento, estat√≠stica, intelig√™ncia computacional. Fases de um processo de descoberta de conhecimento: obten√ß√£o e normaliza√ß√£o de dados, limpeza de dados, sele√ß√£o e transforma√ß√£o, minera√ß√£o, avalia√ß√£o do conhecimento.\nAn√°lise preditiva: Tarefas de minera√ß√£o de dados: classifica√ß√£o e regress√£o.\nAn√°lise de agrupamentos: A tarefa de agrupamento. Avalia√ß√£o de modelos para an√°lise de agrupamento.\nRegras de associa√ß√£o: Processo de minera√ß√£o de regras de Associa√ß√£o. Algoritmos para minera√ß√£o de regras de associa√ß√£o.\nDetec√ß√£o de anomalias: Introdu√ß√£o. Processo de detec√ß√£o de anomalias. M√©todos estat√≠sticos. M√©todos algor√≠tmicos.\n\n\n\nHor√°rio de Aulas\nNeste semestre, as aulas da disciplina ser√£o ministradas no LABEST III.\n\n\n\nDia\nHor√°rio\nLocal\n\n\n\n\nTer√ßa-feira\n17:10 - 18:50\nLABEST III\n\n\nQuinta-feira\n17:10 - 18:50\nLABEST III\n\n\n\n\n\n\n\n\n\n\n\nRefer√™ncias Bibliogr√°ficas\n\nAmaral, F. C. N. 2001. Data Mining: T√©cnicas e Aplica√ß√µes Para o Marketing Direto. Berkeley.\n\n\nCarvalho, L. A. V. 2005. Datamining - a Mineracao de Dados No Marketing, Medicina, Economia, Engenharia e Administracao. Ed. Ci√™ncia Moderna.\n\n\nDe Castro, Leandro, and Daniel Ferrari. 2016. Introdu√ß√£o √† Minera√ß√£o de Dados: Conceitos B√°sicos, Algoritmos e Aplica√ß√µes.\n\n\nGoldschmidt, R., and E. Passos. 2005. Data Mining: Um Guia Pr√°tico. Elsevier Editora.\n\n\nHan, J., M. Kamber, and J. Pei. 2011. Data Mining: Concepts and Techniques. The Morgan Kaufmann Series in Data Management Systems. Morgan Kaufmann.\n\n\nHastie, T., R. Tibshirani, and J. H. Friedman. 2001. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer.\n\n\nJohnson, R. A., and D. W. Wichern. 2007. Applied Multivariate Statistical Analysis. Applied Multivariate Statistical Analysis. Pearson Prentice Hall.\n\n\nKumar, V., P. TAM, and M. Steinbach. 2009. Introdu√ß√£o Ao Data Mining; Minera√ß√£o de Dados.\n\n\nMorettin, P. A., and W. O. Bussab. 2017. ESTAT√çSTICA b√ÅSICA. Editora Saraiva.\n\n\nSilva, Leandro, Sarajane Peres, and Clodis Boscarioli. 2017. Introdu√ß√£o a Minera√ß√£o de Dados Com Aplica√ß√µes Em r."
  },
  {
    "objectID": "aulas.html",
    "href": "aulas.html",
    "title": "Aulas",
    "section": "",
    "text": "Conceitos iniciais\nPr√©-processamento de dados\nAn√°lise preditiva\nReconhecimento de padr√µes\nRegras de associa√ß√£o\nDetec√ß√£o de anomalias",
    "crumbs": [
      "Aulas"
    ]
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#motiva√ß√£o-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#motiva√ß√£o-1",
    "title": "Conceitos Iniciais",
    "section": "Motiva√ß√£o",
    "text": "Motiva√ß√£o\n\n\n\n‚ÄúN√≥s estamos nos afogando em informa√ß√£o, mas sedentos por conhecimento‚Äù.\n\n\nNaisbitt (1982)\n\n\n\n\n\n\n\n\n\nS√£o os dados, o novo petr√≥leo?"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#muitos-dados-pouco-conhecimento",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#muitos-dados-pouco-conhecimento",
    "title": "Conceitos Iniciais",
    "section": "Muitos dados, pouco conhecimento!",
    "text": "Muitos dados, pouco conhecimento!\nA quantidade de dados gerados diariamente vem aumentando exponencialmente, tornando a an√°lise de dados cada vez mais importante\n\n\n\nFonte: statista\n\n\n\nüëâ Um zettabyte √© igual a 1 trilh√£o de gigabytes üò≤"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#muitos-dados-pouco-conhecimento-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#muitos-dados-pouco-conhecimento-1",
    "title": "Conceitos Iniciais",
    "section": "Muitos dados, pouco conhecimento!",
    "text": "Muitos dados, pouco conhecimento!"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#como-usar-o-conhecimento",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#como-usar-o-conhecimento",
    "title": "Conceitos Iniciais",
    "section": "Como usar o conhecimento?",
    "text": "Como usar o conhecimento?\nNeg√≥cios: tomada de decis√µes baseadas em dados"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#como-usar-o-conhecimento-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#como-usar-o-conhecimento-1",
    "title": "Conceitos Iniciais",
    "section": "Como usar o conhecimento?",
    "text": "Como usar o conhecimento?\nSa√∫de: descobrir novos tratamentos e medicamentos"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#como-usar-o-conhecimento-2",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#como-usar-o-conhecimento-2",
    "title": "Conceitos Iniciais",
    "section": "Como usar o conhecimento?",
    "text": "Como usar o conhecimento?\nSeguran√ßa: prever e prevenir crimes e amea√ßas"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#como-usar-o-conhecimento-3",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#como-usar-o-conhecimento-3",
    "title": "Conceitos Iniciais",
    "section": "Como usar o conhecimento?",
    "text": "Como usar o conhecimento?\nMarketing: segmenta√ß√£o de clientes e personaliza√ß√£o de produtos/servi√ßos"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#o-que-√©-minera√ß√£o-de-dados",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#o-que-√©-minera√ß√£o-de-dados",
    "title": "Conceitos Iniciais",
    "section": "O que √© minera√ß√£o de dados?",
    "text": "O que √© minera√ß√£o de dados?\n\n\nA minera√ß√£o de dados √© o processo de descobrir conhecimentos ocultos e padr√µes em grandes volumes de dados.\nTem como objetivo transformar dados brutos em informa√ß√µes √∫teis e conhecimentos para a tomada de decis√£o."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#o-que-√©-minera√ß√£o-de-dados-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#o-que-√©-minera√ß√£o-de-dados-1",
    "title": "Conceitos Iniciais",
    "section": "O que √© minera√ß√£o de dados?",
    "text": "O que √© minera√ß√£o de dados?\n\n\n\n\n\n\n\n\n\nEla usa t√©cnicas de aprendizado de m√°quina, estat√≠stica e intelig√™ncia artificial para analisar dados e extrair insights.\n\n\nEssas t√©cnicas transformam, de maneira inteligente e autom√°tica, os dados dispon√≠veis em informa√ß√µes √∫teis, que representem o conhecimento para ser usado com sabedoria."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria",
    "title": "Conceitos Iniciais",
    "section": "Dado, informa√ß√£o, conhecimento e sabedoria",
    "text": "Dado, informa√ß√£o, conhecimento e sabedoria\n\n\n\n\n\n\n\n\n\n\n\nInforma√ß√£o, e n√£o dados, valem dinheiro, tempo, conhecimento!"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-1",
    "title": "Conceitos Iniciais",
    "section": "Dado, informa√ß√£o, conhecimento e sabedoria",
    "text": "Dado, informa√ß√£o, conhecimento e sabedoria\n\n\n\n\n\n\n\n\n\n\n \n ‚ÄúO conhecimento come√ßa com a defini√ß√£o dos termos.‚Äù \n\n\nS√≥crates"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-2",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-2",
    "title": "Conceitos Iniciais",
    "section": "Dado, informa√ß√£o, conhecimento e sabedoria",
    "text": "Dado, informa√ß√£o, conhecimento e sabedoria\nDado: s√£o fatos brutos, sem significado ou contexto, que podem ser registrados e armazenados."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-3",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-3",
    "title": "Conceitos Iniciais",
    "section": "Dado, informa√ß√£o, conhecimento e sabedoria",
    "text": "Dado, informa√ß√£o, conhecimento e sabedoria\nInforma√ß√£o: √© o resultado da organiza√ß√£o e interpreta√ß√£o dos dados, dando-lhes significado e contexto."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-4",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-4",
    "title": "Conceitos Iniciais",
    "section": "Dado, informa√ß√£o, conhecimento e sabedoria",
    "text": "Dado, informa√ß√£o, conhecimento e sabedoria\nConhecimento: √© a compreens√£o dos relacionamentos entre as informa√ß√µes e a capacidade de aplicar essas informa√ß√µes em uma situa√ß√£o espec√≠fica."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-5",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#dado-informa√ß√£o-conhecimento-e-sabedoria-5",
    "title": "Conceitos Iniciais",
    "section": "Dado, informa√ß√£o, conhecimento e sabedoria",
    "text": "Dado, informa√ß√£o, conhecimento e sabedoria\nSabedoria: √© a habilidade de usar o conhecimento de forma √©tica e eficaz para tomar decis√µes s√°bias."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-semma-e-kdd",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-semma-e-kdd",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM, SEMMA e KDD",
    "text": "CRISP-DM, SEMMA e KDD\n\n\nGerenciamento de projetos √© um campo estabelecido focado no planejamento, entrega, controle e monitoramento de um projeto.\n\n\n\nExistem v√°rias metodologias para gerenciar projetos de minera√ß√£o de dados.\n\n\n\n\nTr√™s das metodologias mais comuns s√£o CRISP-DM, SEMMA e KDD."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM",
    "text": "CRISP-DM\n\n\nCRISP-DM (CRoss Industry Standard Process for Data Mining) √© uma metodologia de minera√ß√£o de dados amplamente utilizada.\n\n\n\nEnvolve seis etapas: compreens√£o do neg√≥cio, compreens√£o dos dados, prepara√ß√£o dos dados, modelagem, avalia√ß√£o e implementa√ß√£o.\n\n\n\n\n√â uma abordagem iterativa e pode ser adaptada para atender a necessidades espec√≠ficas do projeto."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-1",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM",
    "text": "CRISP-DM"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-compreens√£o-do-neg√≥cio",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-compreens√£o-do-neg√≥cio",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM: Compreens√£o do neg√≥cio",
    "text": "CRISP-DM: Compreens√£o do neg√≥cio\n\n\nNesta fase, o objetivo √© entender os objetivos do projeto, identificar as necessidades do usu√°rio e definir os crit√©rios de sucesso.\n\n\n\n√â importante definir claramente o escopo do projeto e entender os recursos dispon√≠veis."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-compreens√£o-dos-dados",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-compreens√£o-dos-dados",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM: Compreens√£o dos Dados",
    "text": "CRISP-DM: Compreens√£o dos Dados\n\n\nNesta fase, o objetivo √© coletar, limpar, integrar e explorar os dados.\n\n\n\n√â importante identificar a qualidade dos dados, definir as vari√°veis relevantes e compreender a estrutura dos dados."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-prepara√ß√£o-dos-dados",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-prepara√ß√£o-dos-dados",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM: Prepara√ß√£o dos Dados",
    "text": "CRISP-DM: Prepara√ß√£o dos Dados\n\n\nNesta fase, o objetivo √© selecionar as vari√°veis relevantes, transformar os dados e criar conjuntos de dados de treinamento e teste.\n\n\n\n√â importante garantir que os dados estejam limpos, completos e estruturados de maneira adequada para a modelagem."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-modelagem",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-modelagem",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM: Modelagem",
    "text": "CRISP-DM: Modelagem\n\n\nNesta fase, o objetivo √© construir e avaliar modelos que possam prever ou classificar novos casos.\n\n\n\n√â importante selecionar as t√©cnicas de modelagem apropriadas e avaliar a efic√°cia do modelo usando conjuntos de dados de treinamento e teste."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-avalia√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-avalia√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM: Avalia√ß√£o",
    "text": "CRISP-DM: Avalia√ß√£o\n\n\nNesta fase, o objetivo √© avaliar a efic√°cia do modelo em rela√ß√£o aos crit√©rios de sucesso definidos na fase de compreens√£o do neg√≥cio.\n\n\n\n√â importante avaliar o modelo em conjuntos de dados de teste independentes e garantir que ele atenda aos requisitos do usu√°rio."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-implanta√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#crisp-dm-implanta√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "CRISP-DM: Implanta√ß√£o",
    "text": "CRISP-DM: Implanta√ß√£o\n\n\nNesta fase, o objetivo √© implantar o modelo em um ambiente de produ√ß√£o e monitor√°-lo regularmente para garantir que continue a atender aos requisitos do usu√°rio.\n\n\n\n√â importante desenvolver um plano de implanta√ß√£o e treinamento para garantir que o modelo seja adotado pelos usu√°rios."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma",
    "title": "Conceitos Iniciais",
    "section": "SEMMA",
    "text": "SEMMA\n\n\nSEMMA (Sample, Explore, Modify, Model, Assess) √© uma metodologia de minera√ß√£o de dados desenvolvida pela SAS.\n\n\n\nEnvolve cinco etapas: amostragem, explora√ß√£o, modifica√ß√£o, modelagem e avalia√ß√£o.\n\n\n\n\nSEMMA √© uma abordagem focada no modelo e pode ser usada em conjun√ß√£o com outras metodologias de gerenciamento de projetos."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-1",
    "title": "Conceitos Iniciais",
    "section": "SEMMA",
    "text": "SEMMA"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-amostragem",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-amostragem",
    "title": "Conceitos Iniciais",
    "section": "SEMMA: Amostragem",
    "text": "SEMMA: Amostragem\n\n\nNesta fase, o objetivo √© selecionar um subconjunto representativo dos dados originais.\n\n\n\n√â importante garantir que a amostra seja grande o suficiente para fornecer resultados precisos e confi√°veis."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-explora√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-explora√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "SEMMA: Explora√ß√£o",
    "text": "SEMMA: Explora√ß√£o\n\n\nNesta fase, o objetivo √© explorar os dados selecionados na etapa anterior e identificar padr√µes ou tend√™ncias.\n\n\n\n√â importante visualizar os dados para identificar padr√µes facilmente e identificar outliers."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-modifica√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-modifica√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "SEMMA: Modifica√ß√£o",
    "text": "SEMMA: Modifica√ß√£o\n\n\nNesta fase, o objetivo √© transformar os dados de forma apropriada para que possam ser usados na modelagem.\n\n\n\n√â importante realizar a limpeza dos dados, selecionar as vari√°veis relevantes e transformar os dados para que possam ser usados na modelagem."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-modelagem",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-modelagem",
    "title": "Conceitos Iniciais",
    "section": "SEMMA: Modelagem",
    "text": "SEMMA: Modelagem\n\n\nNesta fase, o objetivo √© criar e testar modelos usando os dados preparados na etapa anterior.\n\n\n\n√â importante selecionar as t√©cnicas de modelagem apropriadas e avaliar a efic√°cia do modelo usando conjuntos de dados de treinamento e teste."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-avalia√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#semma-avalia√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "SEMMA: Avalia√ß√£o",
    "text": "SEMMA: Avalia√ß√£o\n\n\nNesta fase, o objetivo √© avaliar a efic√°cia do modelo em rela√ß√£o aos crit√©rios de sucesso definidos na fase de amostragem.\n\n\n\n√â importante avaliar o modelo em conjuntos de dados de teste independentes e garantir que ele atenda aos requisitos do usu√°rio."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd",
    "title": "Conceitos Iniciais",
    "section": "KDD",
    "text": "KDD\n\n\nKDD (Knowledge Discovery in Databases) √© uma metodologia mais ampla que CRISP-DM e SEMMA.\n\n\n\nEnvolve basicamente tr√™s etapas: pr√©-processamento, minera√ß√£o de dados e p√≥s-processamento.\n\nPr√©-processamento inclui limpeza, integra√ß√£o e transforma√ß√£o de dados.\nA minera√ß√£o de dados inclui sele√ß√£o, pr√©-processamento e modelagem de dados.\nO p√≥s-processamento inclui interpreta√ß√£o e avalia√ß√£o dos resultados."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-1",
    "title": "Conceitos Iniciais",
    "section": "KDD",
    "text": "KDD"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-sele√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-sele√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "KDD: Sele√ß√£o",
    "text": "KDD: Sele√ß√£o\n\n\nNesta fase, o objetivo √© selecionar os dados relevantes para o problema de minera√ß√£o.\n\n\n\n√â importante identificar as fontes de dados relevantes e escolher as vari√°veis de interesse."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-pr√©-processamento",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-pr√©-processamento",
    "title": "Conceitos Iniciais",
    "section": "KDD: Pr√©-processamento",
    "text": "KDD: Pr√©-processamento\n\n\nNesta fase, o objetivo √© preparar os dados para an√°lise.\n\n\n\n√â importante lidar com valores ausentes, tratar outliers e realizar normaliza√ß√£o ou padroniza√ß√£o de vari√°veis."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-limpeza",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-limpeza",
    "title": "Conceitos Iniciais",
    "section": "KDD: Limpeza",
    "text": "KDD: Limpeza\n\n\nNesta fase, o objetivo √© identificar e corrigir erros nos dados.\n\n\n\n√â importante lidar com dados duplicados, corrigir erros de digita√ß√£o e identificar e tratar valores discrepantes."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-transforma√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-transforma√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "KDD: Transforma√ß√£o",
    "text": "KDD: Transforma√ß√£o\n\n\nNesta fase, o objetivo √© transformar os dados em uma forma que seja adequada para a an√°lise.\n\n\n\n√â importante realizar transforma√ß√µes como discretiza√ß√£o, agrega√ß√£o, e normaliza√ß√£o para que os dados possam ser usados com sucesso na modelagem."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-minera√ß√£o-de-dados",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-minera√ß√£o-de-dados",
    "title": "Conceitos Iniciais",
    "section": "KDD: Minera√ß√£o de dados",
    "text": "KDD: Minera√ß√£o de dados\n\n\nNesta fase, o objetivo √© aplicar t√©cnicas de minera√ß√£o de dados para identificar padr√µes e relacionamentos nos dados.\n\n\n\n√â importante escolher as t√©cnicas de minera√ß√£o de dados apropriadas e avaliar a efic√°cia dos modelos gerados."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-interpreta√ß√£o-e-avalia√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#kdd-interpreta√ß√£o-e-avalia√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "KDD: Interpreta√ß√£o e avalia√ß√£o",
    "text": "KDD: Interpreta√ß√£o e avalia√ß√£o\n\n\nNesta fase, o objetivo √© interpretar os resultados da minera√ß√£o de dados e avaliar sua relev√¢ncia para o problema de neg√≥cio.\n\n\n\n√â importante avaliar a efic√°cia dos modelos gerados e determinar se as descobertas s√£o significativas e √∫teis."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#qual-metodologia-utilizar",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#qual-metodologia-utilizar",
    "title": "Conceitos Iniciais",
    "section": "Qual metodologia utilizar?",
    "text": "Qual metodologia utilizar?"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#qual-metodologia-utilizar-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#qual-metodologia-utilizar-1",
    "title": "Conceitos Iniciais",
    "section": "Qual metodologia utilizar?",
    "text": "Qual metodologia utilizar?\nCRISP-DM, SEMMA e KDD s√£o todas metodologias amplamente utilizadas para gerenciamento de projetos de minera√ß√£o de dados.\n\nCRISP-DM √© uma abordagem iterativa em cascata, SEMMA √© uma abordagem em cascata focada no modelo e KDD √© uma metodologia mais ampla com tr√™s etapas principais.\n\n\nA escolha da metodologia depender√° das necessidades espec√≠ficas do projeto e dos recursos dispon√≠veis para o gerenciamento do projeto."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#tarefas-e-t√©cnicas-de-minera√ß√£o-de-dados",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#tarefas-e-t√©cnicas-de-minera√ß√£o-de-dados",
    "title": "Conceitos Iniciais",
    "section": "Tarefas e t√©cnicas de minera√ß√£o de dados",
    "text": "Tarefas e t√©cnicas de minera√ß√£o de dados\nUma tarefa de minera√ß√£o de dados determina o tipo de problema que ser√° resolvido pelo processo de minera√ß√£o de dados.\n\nPodem ser descritivas ou preditivas.\n\n\nJ√° a t√©cnica, representa o algoritmo que pode ser empregado para a execu√ß√£o da tarefa."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#tarefas-e-t√©cnicas-de-minera√ß√£o-de-dados-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#tarefas-e-t√©cnicas-de-minera√ß√£o-de-dados-1",
    "title": "Conceitos Iniciais",
    "section": "Tarefas e t√©cnicas de minera√ß√£o de dados",
    "text": "Tarefas e t√©cnicas de minera√ß√£o de dados\nAs tarefas de minera√ß√£o de dados incluem: classifica√ß√£o, clustering, regress√£o, associa√ß√£o e detec√ß√£o de anomalias.\n\nCada uma dessas tarefas tem um objetivo espec√≠fico na an√°lise de dados."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#classifica√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#classifica√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "Classifica√ß√£o",
    "text": "Classifica√ß√£o\n\n\n\n\n\n\n\n\n\n\nA classifica√ß√£o √© uma tarefa de minera√ß√£o de dados que envolve a atribui√ß√£o de um r√≥tulo a um conjunto de dados com base em um conjunto de caracter√≠sticas.\n\n\n\nAs t√©cnicas de classifica√ß√£o incluem: √°rvores de decis√£o, redes neurais, SVM, Naive Bayes, etc."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#clusteriza√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#clusteriza√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "Clusteriza√ß√£o",
    "text": "Clusteriza√ß√£o\n\n\n\n\nClustering √© uma tarefa de minera√ß√£o de dados que envolve a organiza√ß√£o de um conjunto de dados em grupos com base em suas caracter√≠sticas.\n\n\n\n\n\n\n\n\n\nAs t√©cnicas de clustering incluem: k-means, DBSCAN, aglomera√ß√£o hier√°rquica, etc."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#regress√£o-ou-predi√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#regress√£o-ou-predi√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "Regress√£o ou predi√ß√£o",
    "text": "Regress√£o ou predi√ß√£o\n\n\n\n\nA regress√£o ou predi√ß√£o √© uma tarefa de minera√ß√£o de dados que envolve a identifica√ß√£o de uma rela√ß√£o entre uma vari√°vel dependente e uma ou mais vari√°veis independentes.\n\n\n\n\n\n\n\n\nAs t√©cnicas de regress√£o incluem: regress√£o linear, regress√£o log√≠stica, etc."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#associa√ß√£o",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#associa√ß√£o",
    "title": "Conceitos Iniciais",
    "section": "Associa√ß√£o",
    "text": "Associa√ß√£o\n\n\n\n\n\n\n\n\n\n\nA associa√ß√£o √© uma tarefa de minera√ß√£o de dados que envolve a descoberta de rela√ß√µes entre itens em um conjunto de dados.\n\n\n\nAs t√©cnicas de associa√ß√£o incluem: regras de associa√ß√£o, an√°lise de cesta de compras, etc."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#detec√ß√£o-de-anomalias",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#detec√ß√£o-de-anomalias",
    "title": "Conceitos Iniciais",
    "section": "Detec√ß√£o de anomalias",
    "text": "Detec√ß√£o de anomalias\n\n\n\n\nA detec√ß√£o de anomalias √© uma tarefa de minera√ß√£o de dados que envolve a identifica√ß√£o de pontos de dados incomuns em um conjunto de dados.\n\n\n\n\n\n\n\n\n\nAs t√©cnicas de detec√ß√£o de anomalias incluem: an√°lise de desvio, detec√ß√£o de outliers, etc."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-da-defini√ß√£o-dos-objetivos",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-da-defini√ß√£o-dos-objetivos",
    "title": "Conceitos Iniciais",
    "section": "Import√¢ncia da defini√ß√£o dos objetivos",
    "text": "Import√¢ncia da defini√ß√£o dos objetivos\nEnquanto isso, no pa√≠s das maravilhas‚Ä¶"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#defini√ß√£o-dos-objetivos",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#defini√ß√£o-dos-objetivos",
    "title": "Conceitos Iniciais",
    "section": "Defini√ß√£o dos objetivos",
    "text": "Defini√ß√£o dos objetivos\nDefinir claramente os objetivos da an√°lise de dados √© fundamental para o sucesso do projeto.\n\nOs objetivos devem ser espec√≠ficos, mensur√°veis, alcan√ß√°veis, relevantes e ter um prazo estabelecido (SMART).\n\n\n\nPor exemplo, suponha que uma empresa queira aumentar as vendas em uma determinada regi√£o.\n\nUsando a metodologia SMART, a empresa pode definir o objetivo da seguinte maneira:"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#defini√ß√£o-dos-objetivos-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#defini√ß√£o-dos-objetivos-1",
    "title": "Conceitos Iniciais",
    "section": "Defini√ß√£o dos objetivos",
    "text": "Defini√ß√£o dos objetivos\n\nEspec√≠fico: O objetivo deve ser espec√≠fico e detalhado.\n\nPor exemplo, ‚ÄúAumentar as vendas de determinada categoria de produtos na regi√£o em 10% no pr√≥ximo trimestre‚Äù.\n\n\n\n\nMensur√°vel: O objetivo deve ser mensur√°vel para que possa ser avaliado.\n\nPor exemplo, a empresa pode usar dados de vendas para medir o progresso em rela√ß√£o √† meta.\n\n\n\n\n\nAlcan√ß√°vel: O objetivo deve ser alcan√ß√°vel para evitar frustra√ß√£o e des√¢nimo da equipe.\n\nA empresa pode avaliar a capacidade da equipe, recursos dispon√≠veis e condi√ß√µes de mercado para determinar se a meta √© alcan√ß√°vel."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#defini√ß√£o-dos-objetivos-2",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#defini√ß√£o-dos-objetivos-2",
    "title": "Conceitos Iniciais",
    "section": "Defini√ß√£o dos objetivos",
    "text": "Defini√ß√£o dos objetivos\n\nRelevante: O objetivo deve ser relevante para os objetivos de neg√≥cios da empresa.\n\nA meta de aumentar as vendas em uma determinada regi√£o deve estar alinhada com os objetivos de neg√≥cios da empresa.\n\n\n\n\nTemporal: O objetivo deve ser definido em um prazo determinado.\n\nPor exemplo, a meta deve ser alcan√ßada em tr√™s meses."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#defini√ß√£o-dos-objetivos-3",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#defini√ß√£o-dos-objetivos-3",
    "title": "Conceitos Iniciais",
    "section": "Defini√ß√£o dos objetivos",
    "text": "Defini√ß√£o dos objetivos\nCom a defini√ß√£o clara do objetivo usando a metodologia SMART, a empresa pode tra√ßar um plano de a√ß√£o, coletar dados relevantes, realizar an√°lises para identificar as causas da queda nas vendas, e implementar medidas para atingir a meta.\n\nA empresa tamb√©m pode monitorar o progresso e ajustar as medidas se necess√°rio para garantir que a meta seja alcan√ßada no prazo estabelecido."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-de-entender-o-problema",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-de-entender-o-problema",
    "title": "Conceitos Iniciais",
    "section": "Import√¢ncia de entender o problema",
    "text": "Import√¢ncia de entender o problema\n\n\nEntenda o problema e depois pense em como resolv√™-lo"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-de-entender-o-problema-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-de-entender-o-problema-1",
    "title": "Conceitos Iniciais",
    "section": "Import√¢ncia de entender o problema",
    "text": "Import√¢ncia de entender o problema\n\nQual o problema nessa foto?"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-de-entender-o-problema-2",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-de-entender-o-problema-2",
    "title": "Conceitos Iniciais",
    "section": "Import√¢ncia de entender o problema",
    "text": "Import√¢ncia de entender o problema\n\nSendo o cavalo, vamos troc√°-lo por um avi√£o!"
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-de-entender-o-problema-3",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#import√¢ncia-de-entender-o-problema-3",
    "title": "Conceitos Iniciais",
    "section": "Import√¢ncia de entender o problema",
    "text": "Import√¢ncia de entender o problema\n\nCompre um mais potente..."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#t√©cnica-dos-cinco-porqu√™s",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#t√©cnica-dos-cinco-porqu√™s",
    "title": "Conceitos Iniciais",
    "section": "T√©cnica dos cinco porqu√™s",
    "text": "T√©cnica dos cinco porqu√™s\n\n\n\n\n\n\n\n\n\n\nA t√©cnica dos cinco porqu√™s √© uma t√©cnica de an√°lise de causalidade que pode ser usada para identificar a causa raiz de um problema, atrav√©s de perguntas sucessivas."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#t√©cnica-dos-cinco-porqu√™s-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#t√©cnica-dos-cinco-porqu√™s-1",
    "title": "Conceitos Iniciais",
    "section": "T√©cnica dos cinco porqu√™s",
    "text": "T√©cnica dos cinco porqu√™s\nO processo da t√©cnica dos cinco porqu√™s pode ser resumido nas seguintes etapas:\n\n\nIdentificar o problema: Comece identificando o problema que precisa ser resolvido.\n\n\n\n\nFazer a primeira pergunta: Pergunte ‚ÄúPor que o problema ocorreu?‚Äù e identifique a causa mais prov√°vel.\n\n\n\n\nFazer a segunda pergunta: Pergunte ‚ÄúPor que a causa identificada na pergunta anterior ocorreu?‚Äù e identifique a causa mais prov√°vel."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#t√©cnica-dos-cinco-porqu√™s-2",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#t√©cnica-dos-cinco-porqu√™s-2",
    "title": "Conceitos Iniciais",
    "section": "T√©cnica dos cinco porqu√™s",
    "text": "T√©cnica dos cinco porqu√™s\n\nContinuar com as perguntas subsequentes: Continue fazendo perguntas sucessivas at√© que a causa raiz seja identificada.\n\n\n\nResolver o problema: Uma vez que a causa raiz √© identificada, √© poss√≠vel implementar solu√ß√µes para resolver o problema.\n\n\n\nüëâ √â importante notar que, embora a t√©cnica dos cinco porqu√™s seja uma t√©cnica √∫til, ela n√£o √© uma solu√ß√£o universal para todos os problemas.\n\n√Äs vezes, a causa raiz de um problema pode ser mais complexa e exigir t√©cnicas de an√°lise mais avan√ßadas.\nAl√©m disso, √© importante n√£o se limitar a apenas cinco perguntas se necess√°rio para chegar √† causa raiz do problema."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#exemplo-do-uso-da-t√©cnica-dos-cinco-porqu√™s",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#exemplo-do-uso-da-t√©cnica-dos-cinco-porqu√™s",
    "title": "Conceitos Iniciais",
    "section": "Exemplo do uso da t√©cnica dos cinco porqu√™s",
    "text": "Exemplo do uso da t√©cnica dos cinco porqu√™s\n\nü§î Problema: O faturamento da empresa diminuiu no √∫ltimo trimestre\n\n\n\nPor que o faturamento da empresa diminuiu no √∫ltimo trimestre?\n\n\nResposta: Porque as vendas de um dos produtos mais vendidos ca√≠ram.\n\n\n\n\nPor que as vendas do produto mais vendido ca√≠ram?\n\n\nResposta: Porque a concorr√™ncia come√ßou a oferecer pre√ßos mais baixos.\n\n\n\n\nPor que a concorr√™ncia come√ßou a oferecer pre√ßos mais baixos?\n\n\nResposta: Porque um novo concorrente entrou no mercado e come√ßou a oferecer pre√ßos mais baixos para ganhar participa√ß√£o de mercado."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#exemplo-do-uso-da-t√©cnica-dos-cinco-porqu√™s-1",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#exemplo-do-uso-da-t√©cnica-dos-cinco-porqu√™s-1",
    "title": "Conceitos Iniciais",
    "section": "Exemplo do uso da t√©cnica dos cinco porqu√™s",
    "text": "Exemplo do uso da t√©cnica dos cinco porqu√™s\n\nPor que o novo concorrente conseguiu oferecer pre√ßos mais baixos?\n\n\nResposta: Porque ele tem custos de produ√ß√£o mais baixos do que a nossa empresa.\n\n\n\nPor que nossos custos de produ√ß√£o s√£o mais altos do que os do novo concorrente?\n\n\nResposta: Porque n√£o atualizamos nossos equipamentos de produ√ß√£o h√° anos, o que torna nosso processo menos eficiente e mais caro."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#exemplo-do-uso-da-t√©cnica-dos-cinco-porqu√™s-2",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#exemplo-do-uso-da-t√©cnica-dos-cinco-porqu√™s-2",
    "title": "Conceitos Iniciais",
    "section": "Exemplo do uso da t√©cnica dos cinco porqu√™s",
    "text": "Exemplo do uso da t√©cnica dos cinco porqu√™s\nüëâ Com essa an√°lise, a causa raiz do problema do faturamento da empresa ter diminu√≠do no √∫ltimo trimestre foi identificada: a falta de atualiza√ß√£o dos equipamentos de produ√ß√£o.\n\n\nDe posse dessa informa√ß√£o, a empresa pode agora tomar medidas para atualizar seus equipamentos de produ√ß√£o e aumentar sua efici√™ncia para voltar a ter um faturamento maior."
  },
  {
    "objectID": "aulas/Conceitos iniciais/conceitos_iniciais.html#references",
    "href": "aulas/Conceitos iniciais/conceitos_iniciais.html#references",
    "title": "Conceitos Iniciais",
    "section": "References",
    "text": "References\n\n\nNaisbitt, J. 1982. Megatrends: Ten New Directions Transforming Our Lives. Megatrends: Ten New Directions Transforming Our Lives, Nr. 158. Warner Books."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-que-√©-intelig√™ncia-artificial-ia",
    "href": "aulas/Analise_preditiva/preditiva.html#o-que-√©-intelig√™ncia-artificial-ia",
    "title": "An√°lise Preditiva",
    "section": "O que √© Intelig√™ncia Artificial (IA)?",
    "text": "O que √© Intelig√™ncia Artificial (IA)?\n\n\n\n\n\n\n\nA capacidade de um sistema computacional simular habilidades cognitivas humanas\n\n\n\nVis√£o Computacional\nProcessamento de Linguagem Natural\nRob√≥tica\nMachine Learning"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-que-√©-intelig√™ncia-artificial-ia-1",
    "href": "aulas/Analise_preditiva/preditiva.html#o-que-√©-intelig√™ncia-artificial-ia-1",
    "title": "An√°lise Preditiva",
    "section": "O que √© Intelig√™ncia Artificial (IA)?",
    "text": "O que √© Intelig√™ncia Artificial (IA)?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-que-√©-machine-learning",
    "href": "aulas/Analise_preditiva/preditiva.html#o-que-√©-machine-learning",
    "title": "An√°lise Preditiva",
    "section": "O que √© Machine Learning?",
    "text": "O que √© Machine Learning?\n\nUm subcampo da IA que permite que sistemas aprendam a partir de dados, sem serem explicitamente programados"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-que-√©-machine-learning-1",
    "href": "aulas/Analise_preditiva/preditiva.html#o-que-√©-machine-learning-1",
    "title": "An√°lise Preditiva",
    "section": "O que √© Machine Learning?",
    "text": "O que √© Machine Learning?\n\n\nüìä\n\n\nDo ponto de vista da Estat√≠stica, Machine Learning (ML) √© uma extens√£o e uma aplica√ß√£o computacional de m√©todos estat√≠sticos com um forte foco em predi√ß√£o e descoberta de padr√µes em dados, muitas vezes em larga escala e com menor √™nfase na infer√™ncia causal tradicional.\n\n\n\n\n\nüíª\n\n\nDo ponto de vista da Ci√™ncia da Computa√ß√£o, Machine Learning (ML) √© um campo que se concentra no desenvolvimento de algoritmos e sistemas computacionais que podem aprender a partir de dados para realizar tarefas sem serem explicitamente programados para cada uma delas."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-que-√©-machine-learning-2",
    "href": "aulas/Analise_preditiva/preditiva.html#o-que-√©-machine-learning-2",
    "title": "An√°lise Preditiva",
    "section": "O que √© Machine Learning?",
    "text": "O que √© Machine Learning?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#as-duas-culturas",
    "href": "aulas/Analise_preditiva/preditiva.html#as-duas-culturas",
    "title": "An√°lise Preditiva",
    "section": "As duas culturas‚Ä¶",
    "text": "As duas culturas‚Ä¶\n\n\n\nData Modeling Culture: Domina a comunidade estat√≠stica. Nela se assume que o modelo utilizado √© correto. Testar suposi√ß√µes √© fundamental. Foco em infer√™ncia e na interpreta√ß√£o dos par√¢metros.\n\n\n\n\nAlgorithmic Modeling Culture: Domina a comunidade de machine learning. Nela n√£o se assume que o modelo utilizado √© correto; o modelo √© utilizado apenas para criar bons algoritmos preditivos. Podemos interpretar os resultados, mas esse, em geral, n√£o √© o foco."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplos-pr√°ticos-de-aplica√ß√µes-de-ml-no-dia-a-dia.",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplos-pr√°ticos-de-aplica√ß√µes-de-ml-no-dia-a-dia.",
    "title": "An√°lise Preditiva",
    "section": "Exemplos pr√°ticos de aplica√ß√µes de ML no dia a dia.",
    "text": "Exemplos pr√°ticos de aplica√ß√µes de ML no dia a dia.\n\n\nSistemas de recomenda√ß√£o (Netflix, Amazon).\nFiltros de spam (Gmail).\nCarros aut√¥nomos (Tesla).\nDiagn√≥stico m√©dico (detec√ß√£o de tumores).\nReconhecimento facial (smartphones)."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#como-as-m√°quinas-aprendem",
    "href": "aulas/Analise_preditiva/preditiva.html#como-as-m√°quinas-aprendem",
    "title": "An√°lise Preditiva",
    "section": "Como as M√°quinas Aprendem?",
    "text": "Como as M√°quinas Aprendem?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#componentes-do-aprendizado-de-m√°quina",
    "href": "aulas/Analise_preditiva/preditiva.html#componentes-do-aprendizado-de-m√°quina",
    "title": "An√°lise Preditiva",
    "section": "Componentes do aprendizado de m√°quina",
    "text": "Componentes do aprendizado de m√°quina\n\n\n\n\n\nhttps://vas3k.com/blog/machine_learning/\n\n\n\n\n\n\nDados\nCaracter√≠sticas/Features\nAlgoritmos"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-mapa-da-aprendizagem-de-m√°quina",
    "href": "aulas/Analise_preditiva/preditiva.html#o-mapa-da-aprendizagem-de-m√°quina",
    "title": "An√°lise Preditiva",
    "section": "O mapa da aprendizagem de m√°quina",
    "text": "O mapa da aprendizagem de m√°quina\n\n\n\n\n\n\n\nüëâ Nunca h√° uma √∫nica maneira de resolver um problema no mundo do aprendizado de m√°quina.\nüëâ Sempre existem v√°rios algoritmos que se encaixam, e voc√™ deve escolher qual deles se encaixa melhor.\nüëâ Tudo pode ser resolvido com uma rede neural? Sim, mas quem pagar√° por todo esse custo?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-mapa-da-aprendizagem-de-m√°quina-1",
    "href": "aulas/Analise_preditiva/preditiva.html#o-mapa-da-aprendizagem-de-m√°quina-1",
    "title": "An√°lise Preditiva",
    "section": "O mapa da aprendizagem de m√°quina",
    "text": "O mapa da aprendizagem de m√°quina"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#aprendizado-de-m√°quina-cl√°ssico",
    "href": "aulas/Analise_preditiva/preditiva.html#aprendizado-de-m√°quina-cl√°ssico",
    "title": "An√°lise Preditiva",
    "section": "Aprendizado de M√°quina Cl√°ssico",
    "text": "Aprendizado de M√°quina Cl√°ssico"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#aprendizado-de-m√°quina-cl√°ssico-1",
    "href": "aulas/Analise_preditiva/preditiva.html#aprendizado-de-m√°quina-cl√°ssico-1",
    "title": "An√°lise Preditiva",
    "section": "Aprendizado de M√°quina Cl√°ssico",
    "text": "Aprendizado de M√°quina Cl√°ssico\nüëâ O aprendizado de m√°quina cl√°ssico √© frequentemente dividido em duas categorias ‚Äì Aprendizado Supervisionado e N√£o Supervisionado.\n\n\n\nAprendizado Supervisionado: usa um algoritmo que precisa de exemplos rotulados para desempenhar suas tarefas.\n\n\n\n\n\nAprendizado N√£o-supervisionado: os dados n√£o s√£o rotulados, n√£o h√° professor e a m√°quina est√° tentando encontrar padr√µes por conta pr√≥pria."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#aprendizado-supervisionado",
    "href": "aulas/Analise_preditiva/preditiva.html#aprendizado-supervisionado",
    "title": "An√°lise Preditiva",
    "section": "Aprendizado Supervisionado",
    "text": "Aprendizado Supervisionado\n\nüí° Claramente, a m√°quina aprender√° mais r√°pido com um professor. Por isso, √© mais comum encontrarmos esse caso nas tarefas da vida real.\n\n\n\nExistem dois tipos de tarefas:\n\n\nclassifica√ß√£o: predi√ß√£o de categoria de um objeto e\n\nregress√£o: predi√ß√£o de um ponto espec√≠fico em um eixo num√©rico."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#tarefa-de-classifica√ß√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#tarefa-de-classifica√ß√£o",
    "title": "An√°lise Preditiva",
    "section": "Tarefa de classifica√ß√£o",
    "text": "Tarefa de classifica√ß√£o\n\nOs algoritmos de classifica√ß√£o dividem os objetos com base em um dos atributos conhecidos de antem√£o.\n\n\n\n\n\n\nUsados nos dias de hoje para:\n\nFiltragem de spam;\nDetec√ß√£o de idioma;\nPesquisa por documentos semelhantes;\nAn√°lise de sentimentos;\nReconhecimento de caracteres;\nDetec√ß√£o de fraude."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#tarefa-de-classifica√ß√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#tarefa-de-classifica√ß√£o-1",
    "title": "An√°lise Preditiva",
    "section": "Tarefa de classifica√ß√£o",
    "text": "Tarefa de classifica√ß√£o\n\n\nAlgoritmos populares: Naive Bayes, Decision Tree, Logistic Regression, K-Nearest Neighbours, Support Vector Machine."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#tarefa-de-regress√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#tarefa-de-regress√£o",
    "title": "An√°lise Preditiva",
    "section": "Tarefa de regress√£o",
    "text": "Tarefa de regress√£o\n\nSe a vari√°vel resposta √© quantitativa, temos um problema de an√°lise de regress√£o\n\n\n\n\n\n\nUsados nos dias de hoje para:\n\nPrevis√µes do pre√ßo das a√ß√µes;\nAn√°lise de demanda e volume de vendas;\nDiagn√≥stico m√©dico."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#tarefa-de-regress√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#tarefa-de-regress√£o-1",
    "title": "An√°lise Preditiva",
    "section": "Tarefa de regress√£o",
    "text": "Tarefa de regress√£o\n\n\nAlgoritmos populares: Decision Tree, Regress√£o Linear e Regress√£o Polinomial, K-Nearest Neighbours, Support Vector Machine."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos",
    "text": "Avalia√ß√£o de modelos\nüí° Independente do modelo escolhido, √© importante saber se um modelo de machine learning est√° realmente funcionando. √â a√≠ que entra a avalia√ß√£o de modelos!\n\n\n\n\n\n\n\n\n\nA avalia√ß√£o de modelos de machine learning √© como um detetive investigando um caso."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-1",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-1",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos",
    "text": "Avalia√ß√£o de modelos"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-2",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-2",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos",
    "text": "Avalia√ß√£o de modelos\n\nü§î O nosso modelo √© um her√≥i ou um impostor?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#matriz-de-confus√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#matriz-de-confus√£o",
    "title": "An√°lise Preditiva",
    "section": "Matriz de confus√£o",
    "text": "Matriz de confus√£o\nPermite a visualiza√ß√£o do desempenho de um algoritmo de classifica√ß√£o"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#matriz-de-confus√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#matriz-de-confus√£o-1",
    "title": "An√°lise Preditiva",
    "section": "Matriz de confus√£o",
    "text": "Matriz de confus√£o\n\n\n\nVP (Verdadeiro Positivo): objeto da classe positiva classificado como positivo\n\n\n\n\nVN (Verdadeiro Negativo): objeto da classe negativa classificado como negativo\n\n\n\n\n\nFP (Falso Positivo): objeto da classe negativa classificado como positivo. Tamb√©m conhecido como alarme falso ou Erro tipo 1\n\n\n\n\n\n\nFN (Falso Negativo): objeto da classe positiva classificado como negativo. √â tamb√©m conhecido como Erro Tipo 2"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#matriz-de-confus√£o-2",
    "href": "aulas/Analise_preditiva/preditiva.html#matriz-de-confus√£o-2",
    "title": "An√°lise Preditiva",
    "section": "Matriz de confus√£o",
    "text": "Matriz de confus√£o\n\nExemplo: Sejam as seguintes matrizes de confus√£o, obtidas de dois classificadores quaisquer."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#acur√°cia",
    "href": "aulas/Analise_preditiva/preditiva.html#acur√°cia",
    "title": "An√°lise Preditiva",
    "section": "Acur√°cia",
    "text": "Acur√°cia\n\n\n\n\n\n\n\n\n\nMede a propor√ß√£o de previs√µes corretas do modelo em rela√ß√£o ao total de previs√µes feitas.\n\n\n\n√â como sua nota em uma prova!"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#acur√°cia-1",
    "href": "aulas/Analise_preditiva/preditiva.html#acur√°cia-1",
    "title": "An√°lise Preditiva",
    "section": "Acur√°cia",
    "text": "Acur√°cia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Taxa de Erro Aparente do classificador √© dada por\n\\[TEA = 1 - ACC\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#acur√°cia-2",
    "href": "aulas/Analise_preditiva/preditiva.html#acur√°cia-2",
    "title": "An√°lise Preditiva",
    "section": "Acur√°cia",
    "text": "Acur√°cia"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#acur√°cia-3",
    "href": "aulas/Analise_preditiva/preditiva.html#acur√°cia-3",
    "title": "An√°lise Preditiva",
    "section": "Acur√°cia",
    "text": "Acur√°cia\n\n\n\nMas ser√° que a acur√°cia √© suficiente para avaliar nossos modelos de forma precisa?\n\n\n\n√Äs vezes, uma √∫nica m√©trica n√£o √© capaz de nos contar toda a hist√≥ria."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#precis√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#precis√£o",
    "title": "An√°lise Preditiva",
    "section": "Precis√£o",
    "text": "Precis√£o\n\n\n\n\n\n\n\n\n\n\nEla nos diz quantas das previs√µes positivas foram realmente corretas."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#precis√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#precis√£o-1",
    "title": "An√°lise Preditiva",
    "section": "Precis√£o",
    "text": "Precis√£o\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPorcentagem de verdadeiros positivos dentre todos os objetos classificados como positivos"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#precis√£o-2",
    "href": "aulas/Analise_preditiva/preditiva.html#precis√£o-2",
    "title": "An√°lise Preditiva",
    "section": "Precis√£o",
    "text": "Precis√£o"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#sensibilidade",
    "href": "aulas/Analise_preditiva/preditiva.html#sensibilidade",
    "title": "An√°lise Preditiva",
    "section": "Sensibilidade",
    "text": "Sensibilidade\n\n\n\n\n\n\n\n\n\n\nMede a propor√ß√£o de casos positivos reais que foram encontrados pelo modelo"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#sensibilidade-1",
    "href": "aulas/Analise_preditiva/preditiva.html#sensibilidade-1",
    "title": "An√°lise Preditiva",
    "section": "Sensibilidade",
    "text": "Sensibilidade\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTamb√©m conhecida por Recall ou Taxa de Verdadeiros Positivos (TVP)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#sensibilidade-2",
    "href": "aulas/Analise_preditiva/preditiva.html#sensibilidade-2",
    "title": "An√°lise Preditiva",
    "section": "Sensibilidade",
    "text": "Sensibilidade"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#especificidade",
    "href": "aulas/Analise_preditiva/preditiva.html#especificidade",
    "title": "An√°lise Preditiva",
    "section": "Especificidade",
    "text": "Especificidade\n\n\n\n\n\n\n\n\n\n\nAjuda a identificar a capacidade do modelo em reconhecer corretamente as amostras negativas"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#especificidade-1",
    "href": "aulas/Analise_preditiva/preditiva.html#especificidade-1",
    "title": "An√°lise Preditiva",
    "section": "Especificidade",
    "text": "Especificidade\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTamb√©m conhecida por Taxa de Verdadeiros Negativos (TVN)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#especificidade-2",
    "href": "aulas/Analise_preditiva/preditiva.html#especificidade-2",
    "title": "An√°lise Preditiva",
    "section": "Especificidade",
    "text": "Especificidade"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#taxa-de-falso-positivo",
    "href": "aulas/Analise_preditiva/preditiva.html#taxa-de-falso-positivo",
    "title": "An√°lise Preditiva",
    "section": "Taxa de Falso Positivo",
    "text": "Taxa de Falso Positivo\n\n\n\n\n\n\n\n\n\n\n\nEla mede a propor√ß√£o de amostras negativas classificadas como positivas pelo modelo"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#taxa-de-falso-positivo-1",
    "href": "aulas/Analise_preditiva/preditiva.html#taxa-de-falso-positivo-1",
    "title": "An√°lise Preditiva",
    "section": "Taxa de Falso Positivo",
    "text": "Taxa de Falso Positivo"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#taxa-de-falso-positivo-2",
    "href": "aulas/Analise_preditiva/preditiva.html#taxa-de-falso-positivo-2",
    "title": "An√°lise Preditiva",
    "section": "Taxa de Falso Positivo",
    "text": "Taxa de Falso Positivo"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#f_1-score",
    "href": "aulas/Analise_preditiva/preditiva.html#f_1-score",
    "title": "An√°lise Preditiva",
    "section": "\n\\(F_1\\)-Score",
    "text": "\\(F_1\\)-Score\n\n\n\n\n\n\n\n\n\n\n\n\nEle leva em considera√ß√£o tanto precis√£o quanto a sensibilidade, dando uma medida balanceada do desempenho do modelo."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#f_1-score-1",
    "href": "aulas/Analise_preditiva/preditiva.html#f_1-score-1",
    "title": "An√°lise Preditiva",
    "section": "\n\\(F_1\\)-Score",
    "text": "\\(F_1\\)-Score\n\n\n\\[F_1 \\text{-score} = 2 \\times \\dfrac{\\text{Precis√£o} \\times \\text{Sensibilidade}}{\\text{Precis√£o} + \\text{Sensibilidade}}\\]\n\n\nO \\(F_1\\)-Score √© como um equilibrista em uma corda bamba."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#curva-roc",
    "href": "aulas/Analise_preditiva/preditiva.html#curva-roc",
    "title": "An√°lise Preditiva",
    "section": "Curva ROC",
    "text": "Curva ROC\n\nA Curva ROC √© como um mapa que nos guia pela sensibilidade e pelos falsos positivos do modelo em diferentes configura√ß√µes.\n\n\n\nEla nos mostra o qu√£o bem nosso modelo pode distinguir entre as classes."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#curva-roc-1",
    "href": "aulas/Analise_preditiva/preditiva.html#curva-roc-1",
    "title": "An√°lise Preditiva",
    "section": "Curva ROC",
    "text": "Curva ROC\n\nRepresenta o n√∫mero de vezes que o classificador acertou a predi√ß√£o contra o n√∫mero de vezes que o classificador errou a predi√ß√£o\n\n\n\n\nA √°rea sob a curva ROC, conhecida como AUC-ROC, √© uma m√©trica comumente utilizada para avaliar o desempenho global do modelo.\n\n\n\n\nQuanto maior a AUC-ROC, melhor √© o desempenho do modelo em discriminar corretamente as classes."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos de predi√ß√£o",
    "text": "Avalia√ß√£o de modelos de predi√ß√£o\n\n\nSeja \\(d_j\\), \\(j = 1,\\cdots, n\\), a resposta desejada para o objeto \\(j\\) e \\(y_j\\) a resposta estimada (predita) do algoritmo, obtida a partir de uma entrada \\(\\mathbf{x_j}\\) apresentada ao algoritmo.\n\n\n\nSeja ent√£o, \\(e_j = d_j - y_j\\) a diferen√ßa entre o valor observado e o valor predito para o objeto \\(j\\).\n\n\n\n\nPodemos definir as seguintes m√©tricas para avalia√ß√£o de modelos preditivos."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-1",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos de predi√ß√£o",
    "text": "Avalia√ß√£o de modelos de predi√ß√£o\nErro Quadr√°tico M√©dio (MSE - Mean Squared Error):\n\\[MSE =\\dfrac{1}{n} \\displaystyle{\\sum_{j=1}^n e_j^2}\\]\n\nPonto forte: Penaliza fortemente erros maiores devido ao termo quadr√°tico. Isso significa que o MSE √© sens√≠vel a outliers (valores discrepantes).\nPonto fraco: Como eleva os erros ao quadrado, a unidade da m√©trica resultante n√£o √© a mesma da vari√°vel original, o que dificulta a interpreta√ß√£o direta da magnitude do erro."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-2",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-2",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos de predi√ß√£o",
    "text": "Avalia√ß√£o de modelos de predi√ß√£o\nRaiz do Erro Quadr√°tico M√©dio (RMSE - Root Mean Squared Error):\n\\[RMSE =\\sqrt{\\dfrac{1}{n} \\displaystyle{\\sum_{j=1}^n e_j^2}}\\]\n\nPonto forte: Mant√©m a propriedade de penalizar erros maiores, mas retorna o erro na mesma unidade da vari√°vel original, facilitando a interpreta√ß√£o. √â uma m√©trica amplamente utilizada.\nPonto fraco: Ainda √© sens√≠vel a outliers, pois se baseia no MSE."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-3",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-3",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos de predi√ß√£o",
    "text": "Avalia√ß√£o de modelos de predi√ß√£o\nErro M√©dio Absoluto (MAE - Mean Absolute Error):\n\\[MAE = \\dfrac{1}{n} \\displaystyle{\\sum_{j=1}^n |e_j|}\\]\n\nPonto forte: √â mais robusto a outliers em compara√ß√£o com MSE e RMSE, pois n√£o eleva os erros ao quadrado. Fornece uma medida direta da magnitude m√©dia dos erros na unidade original da vari√°vel.\nPonto fraco: N√£o penaliza erros maiores de forma t√£o intensa quanto o MSE e RMSE. Pode n√£o ser ideal se erros grandes tiverem um impacto significativamente maior no seu problema."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-4",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-4",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos de predi√ß√£o",
    "text": "Avalia√ß√£o de modelos de predi√ß√£o\nErro Percentual Absoluto M√©dio (MAPE - Mean Absolute Percentage Error):\n\\[MAPE = \\dfrac{1}{n} \\displaystyle{\\sum_{j=1}^n |e_j/d_j|}\\times 100\\]\n\nPonto forte: √â f√°cil de interpretar, pois expressa o erro em termos percentuais. Isso pode ser √∫til para comparar o desempenho de modelos em diferentes escalas.\nPonto fraco: N√£o √© definido quando os valores reais s√£o zero. Al√©m disso, pode ser assim√©trico, penalizando mais os erros de previs√£o abaixo do valor real do que acima. Pode ser inst√°vel se houver valores reais muito pequenos."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-5",
    "href": "aulas/Analise_preditiva/preditiva.html#avalia√ß√£o-de-modelos-de-predi√ß√£o-5",
    "title": "An√°lise Preditiva",
    "section": "Avalia√ß√£o de modelos de predi√ß√£o",
    "text": "Avalia√ß√£o de modelos de predi√ß√£o\n\nü§î Qual a melhor m√©trica?\n\nEm resumo:\n\nSe voc√™ se preocupa muito com grandes erros: MSE e RMSE s√£o boas op√ß√µes.\n\n\n\nSe voc√™ quer uma m√©trica robusta a outliers: MAE √© uma boa escolha.\n\n\n\n\nSe a interpretabilidade em termos percentuais √© importante (com cuidado com valores zero/pequenos): MAPE pode ser √∫til.\n\n\n\n\nüí° O ideal √© analisar todas as m√©tricas em conjunto, considerando o contexto do seu problema e utilizando valida√ß√£o cruzada."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#qual-o-melhor-modelo",
    "href": "aulas/Analise_preditiva/preditiva.html#qual-o-melhor-modelo",
    "title": "An√°lise Preditiva",
    "section": "Qual o melhor modelo?",
    "text": "Qual o melhor modelo?\n\n\n\nüëâ Suponha que tenhamos dados simulados utilizando o seguinte modelo:"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#qual-o-melhor-modelo-1",
    "href": "aulas/Analise_preditiva/preditiva.html#qual-o-melhor-modelo-1",
    "title": "An√°lise Preditiva",
    "section": "Qual o melhor modelo?",
    "text": "Qual o melhor modelo?\n\n\n\nüëâ Podemos estimar diversos modelos \\(y\\) que predizem o verdadeiro valor de \\(d\\)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#qual-o-melhor-modelo-2",
    "href": "aulas/Analise_preditiva/preditiva.html#qual-o-melhor-modelo-2",
    "title": "An√°lise Preditiva",
    "section": "Qual o melhor modelo?",
    "text": "Qual o melhor modelo?\n\n\n\nüëâ Nosso interesse est√° em treinar o modelo e avaliar a sua capacidade de generaliza√ß√£o"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-holdout",
    "href": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-holdout",
    "title": "An√°lise Preditiva",
    "section": "Valida√ß√£o holdout",
    "text": "Valida√ß√£o holdout\n\n\n\nüëâ Dados originais: treinamento e teste\nüëâ Dados de treinamento: treinamento e valida√ß√£o"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-holdout-1",
    "href": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-holdout-1",
    "title": "An√°lise Preditiva",
    "section": "Valida√ß√£o holdout",
    "text": "Valida√ß√£o holdout\n\n\n\nExemplo: Vamos avaliar a rela√ß√£o entre Frequ√™ncia Card√≠aca e Idade de 270 pacientes"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-holdout-2",
    "href": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-holdout-2",
    "title": "An√°lise Preditiva",
    "section": "Valida√ß√£o holdout",
    "text": "Valida√ß√£o holdout\n\nü§î Qual o melhor modelo nesse caso?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-holdout-3",
    "href": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-holdout-3",
    "title": "An√°lise Preditiva",
    "section": "Valida√ß√£o holdout",
    "text": "Valida√ß√£o holdout\n\nüëâ 70% da base para treino e 30% para valida√ß√£o"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-cruzada-k-fold",
    "href": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-cruzada-k-fold",
    "title": "An√°lise Preditiva",
    "section": "Valida√ß√£o cruzada k-fold",
    "text": "Valida√ß√£o cruzada k-fold\nüëâ Dados de treinamento: \\(k\\) partes iguais. Treina com \\(k-1\\) partes, e valida com uma"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-cruzada-k-fold-1",
    "href": "aulas/Analise_preditiva/preditiva.html#valida√ß√£o-cruzada-k-fold-1",
    "title": "An√°lise Preditiva",
    "section": "Valida√ß√£o cruzada k-fold",
    "text": "Valida√ß√£o cruzada k-fold"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#trade-off-bias-vari√¢ncia",
    "href": "aulas/Analise_preditiva/preditiva.html#trade-off-bias-vari√¢ncia",
    "title": "An√°lise Preditiva",
    "section": "Trade-off bias-vari√¢ncia",
    "text": "Trade-off bias-vari√¢ncia\nImagine que voc√™ est√° tentando acertar um alvo com dardos. Seus arremessos podem ser agrupados de tr√™s maneiras diferentes:\n\n\nGrupo de alto vi√©s (bias): Seus arremessos s√£o consistentemente agrupados longe do alvo, mas pr√≥ximos uns dos outros. Isso indica um alto vi√©s, pois voc√™ est√° fazendo arremessos incorretos, mas de forma consistente.\n\n\n\n\nGrupo de alta vari√¢ncia: Seus arremessos est√£o espalhados por toda a √°rea, longe do alvo e uns dos outros. Isso indica alta vari√¢ncia, pois seus arremessos s√£o inconsistentes e imprevis√≠veis.\n\n\n\n\n\nGrupo equilibrado: Seus arremessos est√£o agrupados pr√≥ximo ao alvo e tamb√©m est√£o pr√≥ximos uns dos outros. Isso √© o equil√≠brio entre vi√©s e vari√¢ncia, onde voc√™ est√° acertando o alvo de forma consistente e precisa."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#trade-off-bias-vari√¢ncia-1",
    "href": "aulas/Analise_preditiva/preditiva.html#trade-off-bias-vari√¢ncia-1",
    "title": "An√°lise Preditiva",
    "section": "Trade-off bias-vari√¢ncia",
    "text": "Trade-off bias-vari√¢ncia"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#trade-off-bias-vari√¢ncia-2",
    "href": "aulas/Analise_preditiva/preditiva.html#trade-off-bias-vari√¢ncia-2",
    "title": "An√°lise Preditiva",
    "section": "Trade-off bias-vari√¢ncia",
    "text": "Trade-off bias-vari√¢ncia\n\nQueremos construir modelos que n√£o apenas performem bem nos dados de treinamento, mas que tamb√©m fa√ßam previs√µes precisas em dados novos e n√£o vistos.\n\nO erro total de um modelo preditivo em dados n√£o vistos pode ser decomposto em tr√™s componentes principais, ou seja,\n\\[\\text{Erro Total} = \\text{Bias}^2 + \\text{Vari√¢ncia} + \\text{Erro Irredut√≠vel}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#trade-off-bias-vari√¢ncia-3",
    "href": "aulas/Analise_preditiva/preditiva.html#trade-off-bias-vari√¢ncia-3",
    "title": "An√°lise Preditiva",
    "section": "Trade-off bias-vari√¢ncia",
    "text": "Trade-off bias-vari√¢ncia\n\n\n\nBias (Vi√©s): Erro devido a suposi√ß√µes simplificadoras no modelo. Um modelo com alto bias tende a subajustar (underfit) os dados de treinamento, perdendo rela√ß√µes importantes entre as vari√°veis.\n\n\n\n\nVari√¢ncia: Sensibilidade do modelo a pequenas varia√ß√µes nos dados de treinamento. Um modelo com alta vari√¢ncia tende a sobreajustar (overfit) os dados de treinamento, aprendendo at√© mesmo o ru√≠do presente neles.\n\n\n\n\n\nErro Irredut√≠vel: Ru√≠do inerente aos dados que n√£o pode ser reduzido por nenhum modelo."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-problema-do-subajuste-underfitting",
    "href": "aulas/Analise_preditiva/preditiva.html#o-problema-do-subajuste-underfitting",
    "title": "An√°lise Preditiva",
    "section": "O Problema do Subajuste (Underfitting)",
    "text": "O Problema do Subajuste (Underfitting)\n\nModelos com alto bias fazem suposi√ß√µes fortes sobre a forma da fun√ß√£o que mapeia as entradas para as sa√≠das. S√£o tipicamente modelos mais simples, com poucos par√¢metros.\n\n\n\nConsequ√™ncias:\n\nDesempenho ruim tanto nos dados de treinamento quanto nos dados de teste.\nIncapacidade de capturar a complexidade real dos dados."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-problema-do-sobreajuste-overfitting",
    "href": "aulas/Analise_preditiva/preditiva.html#o-problema-do-sobreajuste-overfitting",
    "title": "An√°lise Preditiva",
    "section": "O Problema do Sobreajuste (Overfitting)",
    "text": "O Problema do Sobreajuste (Overfitting)\n\nModelos com alta vari√¢ncia aprendem os dados de treinamento muito bem, incluindo o ru√≠do presente neles. S√£o tipicamente modelos mais complexos, com muitos par√¢metros.\n\n\nConsequ√™ncias:\n\nExcelente desempenho nos dados de treinamento.\nDesempenho significativamente pior em dados de teste n√£o vistos. O modelo ‚Äúdecorou‚Äù os dados de treinamento em vez de aprender padr√µes gerais."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#underfitting-e-overfitting",
    "href": "aulas/Analise_preditiva/preditiva.html#underfitting-e-overfitting",
    "title": "An√°lise Preditiva",
    "section": "Underfitting e Overfitting",
    "text": "Underfitting e Overfitting"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia",
    "href": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia",
    "title": "An√°lise Preditiva",
    "section": "Bias vs.¬†Vari√¢ncia",
    "text": "Bias vs.¬†Vari√¢ncia\n\n\n\n\nVoltemos ao nosso modelo simulado:"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia-1",
    "href": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia-1",
    "title": "An√°lise Preditiva",
    "section": "Bias vs.¬†Vari√¢ncia",
    "text": "Bias vs.¬†Vari√¢ncia\n\nüëâ Vamos ajustar um modelo polinomial de grau 2"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia-2",
    "href": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia-2",
    "title": "An√°lise Preditiva",
    "section": "Bias vs.¬†Vari√¢ncia",
    "text": "Bias vs.¬†Vari√¢ncia"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia-3",
    "href": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia-3",
    "title": "An√°lise Preditiva",
    "section": "Bias vs.¬†Vari√¢ncia",
    "text": "Bias vs.¬†Vari√¢ncia\n\nüëâ Vamos ajustar um modelo polinomial de grau 10"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia-4",
    "href": "aulas/Analise_preditiva/preditiva.html#bias-vs.-vari√¢ncia-4",
    "title": "An√°lise Preditiva",
    "section": "Bias vs.¬†Vari√¢ncia",
    "text": "Bias vs.¬†Vari√¢ncia"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-tradeoff---a-balan√ßa-delicada",
    "href": "aulas/Analise_preditiva/preditiva.html#o-tradeoff---a-balan√ßa-delicada",
    "title": "An√°lise Preditiva",
    "section": "O Tradeoff - A Balan√ßa Delicada",
    "text": "O Tradeoff - A Balan√ßa Delicada\nüëâ Bias e vari√¢ncia est√£o inversamente relacionados. Tentar reduzir um geralmente aumenta o outro.\n\n\nModelos simples tendem a ter alto bias e baixa vari√¢ncia.\nModelos complexos tendem a ter baixo bias e alta vari√¢ncia.\n\n\n\nüí° O objetivo √© encontrar um modelo com um bom equil√≠brio entre bias e vari√¢ncia, que minimize o erro de generaliza√ß√£o."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-tradeoff---a-balan√ßa-delicada-1",
    "href": "aulas/Analise_preditiva/preditiva.html#o-tradeoff---a-balan√ßa-delicada-1",
    "title": "An√°lise Preditiva",
    "section": "O Tradeoff - A Balan√ßa Delicada",
    "text": "O Tradeoff - A Balan√ßa Delicada"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#como-lidar-com-o-tradeoff",
    "href": "aulas/Analise_preditiva/preditiva.html#como-lidar-com-o-tradeoff",
    "title": "An√°lise Preditiva",
    "section": "Como Lidar com o Tradeoff?",
    "text": "Como Lidar com o Tradeoff?\n\n\n\nSele√ß√£o de Modelos: Experimentar diferentes tipos de modelos (lineares, n√£o lineares, √°rvores, redes neurais, etc.).\n\n\n\n\nAjuste de Hiperpar√¢metros: Controlar a complexidade do modelo ajustando seus hiperpar√¢metros (ex: profundidade m√°xima de uma √°rvore, n√∫mero de neur√¥nios em uma camada).\n\n\n\n\n\nValida√ß√£o Cruzada: Usar t√©cnicas de valida√ß√£o cruzada para estimar o desempenho do modelo em dados n√£o vistos de forma mais robusta e ajudar a identificar overfitting.\n\n\n\n\n\nEngenharia de Features: Criar features mais informativas pode reduzir o bias."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#como-lidar-com-o-tradeoff-1",
    "href": "aulas/Analise_preditiva/preditiva.html#como-lidar-com-o-tradeoff-1",
    "title": "An√°lise Preditiva",
    "section": "Como Lidar com o Tradeoff?",
    "text": "Como Lidar com o Tradeoff?\n\n\n\nRegulariza√ß√£o: T√©cnicas como L1 e L2 adicionam uma penalidade √† complexidade do modelo, ajudando a reduzir a vari√¢ncia (overfitting).\n\n\n\n\nMais Dados: Em alguns casos, aumentar a quantidade de dados de treinamento pode ajudar a reduzir a vari√¢ncia.\n\n\n\n\n\nEnsemble Methods: Combinar m√∫ltiplos modelos (ex: Random Forest, Gradient Boosting) pode ajudar a reduzir tanto o bias quanto a vari√¢ncia."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#algoritmos-de-predi√ß√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#algoritmos-de-predi√ß√£o-1",
    "title": "An√°lise Preditiva",
    "section": "Algoritmos de predi√ß√£o",
    "text": "Algoritmos de predi√ß√£o\n\n\n\nAlgoritmos mais comuns usados para problemas de predi√ß√£o:\n\nK-Nearest Neighbors (KNN)\nNaive Bayes\n√Årvores de decis√£o\nRandom Forests\nM√°quinas de Vetores de Suporte (SVM)\nRedes Neurais Artificiais (ANN)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#k-nearest-neighbors-knn",
    "href": "aulas/Analise_preditiva/preditiva.html#k-nearest-neighbors-knn",
    "title": "An√°lise Preditiva",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\n\nO KNN √© um algoritmo de aprendizado supervisionado de classifica√ß√£o e regress√£o, que usa a proximidade dos objetos para classificar novas inst√¢ncias.\n\n√â um dos algoritmos mais simples e intuitivos de aprendizado de m√°quina. Utiliza a ideia do vizinho mais pr√≥ximo, o que significa que ele determina a classe de uma inst√¢ncia com base nas classes de seus vizinhos mais pr√≥ximos.\n\n\nEm outras palavras, se a maioria dos vizinhos mais pr√≥ximos de uma inst√¢ncia pertence a uma classe espec√≠fica, ent√£o a inst√¢ncia tamb√©m √© classificada como pertencente a essa classe."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#algoritmo-knn",
    "href": "aulas/Analise_preditiva/preditiva.html#algoritmo-knn",
    "title": "An√°lise Preditiva",
    "section": "Algoritmo KNN",
    "text": "Algoritmo KNN"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#algoritmo-knn-1",
    "href": "aulas/Analise_preditiva/preditiva.html#algoritmo-knn-1",
    "title": "An√°lise Preditiva",
    "section": "Algoritmo KNN",
    "text": "Algoritmo KNN\n\nKNN √© usado para problemas de classifica√ß√£o e regress√£o.\n\n\nEm problemas de classifica√ß√£o, a classe mais comum entre os K vizinhos mais pr√≥ximos √© escolhida como a classe da nova inst√¢ncia.\n\n\n\n\nEm problemas de regress√£o, a m√©dia ou mediana dos valores alvo dos K vizinhos mais pr√≥ximos √© escolhida como o valor alvo da nova inst√¢ncia."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#a-escolha-do-valor-de-k",
    "href": "aulas/Analise_preditiva/preditiva.html#a-escolha-do-valor-de-k",
    "title": "An√°lise Preditiva",
    "section": "A escolha do valor de K",
    "text": "A escolha do valor de K\n\nO valor de K √© um par√¢metro importante em KNN. Ele determina o n√∫mero de vizinhos mais pr√≥ximos que s√£o usados para classificar uma nova inst√¢ncia.\n\nUm valor de K pequeno pode levar a um modelo muito sens√≠vel ao ru√≠do nos dados (overfitting), enquanto um valor grande de K pode levar a uma perda de detalhes importantes nos dados (underfitting).\n\n\nA escolha do valor K ideal √© frequentemente realizada por meio de t√©cnicas de valida√ß√£o cruzada.\n\n\nK √≠mpar evita empates."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#m√©tricas-de-dist√¢ncia",
    "href": "aulas/Analise_preditiva/preditiva.html#m√©tricas-de-dist√¢ncia",
    "title": "An√°lise Preditiva",
    "section": "M√©tricas de Dist√¢ncia",
    "text": "M√©tricas de Dist√¢ncia\n\n\nDist√¢ncia Euclidiana: A mais comum.\n\n\\[d_{ij} = \\displaystyle{\\sqrt{(\\mathbf{x}_i - \\mathbf{x}_j)^t(\\mathbf{x}_i - \\mathbf{x}_j)}} = \\sqrt{\\displaystyle{\\sum_{k=1}^p(x_{ik} - x_{jk})^2}}\\]\n\n\nDist√¢ncia de Minkowski: Soma das diferen√ßas absolutas entre as coordenadas.\n\n\\[d_{ij} = \\left( \\displaystyle{\\sum_{k=1}^P} |X_{ik} - X_{jk}|^{\\lambda}\\right)^{\\frac{1}{\\lambda}}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#m√©tricas-de-dist√¢ncia-1",
    "href": "aulas/Analise_preditiva/preditiva.html#m√©tricas-de-dist√¢ncia-1",
    "title": "An√°lise Preditiva",
    "section": "M√©tricas de Dist√¢ncia",
    "text": "M√©tricas de Dist√¢ncia\n\n\n\nPara a dist√¢ncia de Minkowski:\n\nSe \\(\\lambda = 1\\), temos a chamada m√©trica de Manhattan. √â tamb√©m conhecida como city block.\n\nSe \\(\\lambda = 2\\), temos a dist√¢ncia euclidiana.\nA m√©trica de Minkowski √© menos afetada pela presen√ßa de valores discrepantes na amostra do que a dist√¢ncia Euclidiana.\n\n\n\n\n\nSe os dados forem textuais: usar cosseno"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\nüëç\n\n\nSimples de entender e implementar.\nN√£o faz muitas suposi√ß√µes sobre os dados (n√£o param√©trico).\n√ötil para dados complexos e n√£o lineares.\nPode ser usado tanto para classifica√ß√£o quanto para regress√£o."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-1",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-1",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\n\nüëé\n\n\nComputacionalmente caro para grandes conjuntos de dados (c√°lculo de dist√¢ncia para todos os pontos).\nSens√≠vel √† escala dos dados (vari√°veis com escalas maiores podem dominar o c√°lculo da dist√¢ncia) - import√¢ncia da normaliza√ß√£o/padroniza√ß√£o.\nDesempenho pode degradar em dados com muitas dimens√µes (maldi√ß√£o da dimensionalidade).\nEscolha do valor de k pode ser crucial e n√£o √© trivial."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#pr√©-processamento-de-dados-para-knn",
    "href": "aulas/Analise_preditiva/preditiva.html#pr√©-processamento-de-dados-para-knn",
    "title": "An√°lise Preditiva",
    "section": "Pr√©-processamento de Dados para KNN",
    "text": "Pr√©-processamento de Dados para KNN\n\n\n\nüíª\n\n\n\nEscalonamento de features: Padroniza√ß√£o (m√©dia zero, desvio padr√£o um) ou normaliza√ß√£o (escala entre 0 e 1) para garantir que todas as features contribuam igualmente para o c√°lculo da dist√¢ncia.\n\nTratamento de valores ausentes: Imputa√ß√£o ou remo√ß√£o.\n\nSele√ß√£o de features: Reduzir a dimensionalidade para melhorar o desempenho."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-1",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-1",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\nJo√£o possui as seguintes caracter√≠sticas\n\nmenos de 30 anos\nrenda m√©dia\n√© estudante\npossu√≠ um bom cr√©dito na pra√ßa!"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-2",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-2",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\nJo√£o compraria ou n√£o compraria o computador?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-3",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-3",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\nQuem √© o Jo√£o?\n\n¬† ¬†\n\n\n\\(x_0 = (\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Bom})\\)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-4",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-4",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\nUsando o classificador KNN com k = 5"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-5",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-5",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\nTemos ent√£o que os 5 vizinhos mais pr√≥ximos a Jo√£o s√£o\n\n\n\n‚ùå\n\n\n\n\n\\(x_2 = \\{\\leq 30, \\text{ Alta}, \\text{ Sim}, \\text{ Bom}\\}\\)\n\n\n\\(x_8 = \\{\\leq 30, \\text{M√©dia}, \\text{ N√£o}, \\text{ Bom}\\}\\)\n\n\n\n\n\n\n‚úÖ\n\n\n\n\\(x_9 = \\{\\leq 30, \\text{ Baixa}, \\text{ Sim}, \\text{ Bom}\\}\\)\n\n\n\\(x_{10} = \\{&gt; 40, \\text{ M√©dia}, \\text{ Sim}, \\text{ Excelente}\\}\\)\n\n\n\\(x_{11} = \\{\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Excelente}\\}\\)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-6",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-6",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\nJo√£o comprar√° o computador?\n\nDe acordo com o KNN: SIM!"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#knn-para-classifica√ß√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#knn-para-classifica√ß√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "KNN para classifica√ß√£o no R e Python",
    "text": "KNN para classifica√ß√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris_split &lt;- initial_split(iris, strata = Species)\ntrain_data &lt;- training(iris_split)\ntest_data &lt;- testing(iris_split)\n\nknn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;% \n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\nknn_fit &lt;- workflow() %&gt;%\n  add_model(knn_spec) %&gt;%\n  add_formula(Species ~ .) %&gt;%\n  fit(data = train_data)\n\nsp_predict &lt;- predict(knn_fit, test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n\n[1] 0.974359\n\n\n\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, stratify=iris.target, random_state=12345\n)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n\n\n\nKNeighborsClassifier()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nKNeighborsClassifierKNeighborsClassifier()\n\n\n\ny_pred = knn.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.9473684210526315"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#knn-para-regress√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#knn-para-regress√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "KNN para regress√£o no R e Python",
    "text": "KNN para regress√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split &lt;- initial_split(mtcars, strata = mpg)\ntrain_data &lt;- training(mtcars_split)\ntest_data &lt;- testing(mtcars_split)\n\nknn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"regression\")\n\nknn_fit &lt;- workflow() %&gt;%\n  add_model(knn_spec) %&gt;%\n  add_formula(mpg ~ .) %&gt;%\n  fit(data = train_data)\n\nmpg_predict &lt;- predict(knn_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n\n[1] 6.069807\n\n\n\n\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train, y_train)\n\n\n\n\nKNeighborsRegressor()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nKNeighborsRegressorKNeighborsRegressor()\n\n\n\ny_pred = knn_reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n\n1.1408296168142682"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#algoritmo-naive-bayes",
    "href": "aulas/Analise_preditiva/preditiva.html#algoritmo-naive-bayes",
    "title": "An√°lise Preditiva",
    "section": "Algoritmo Naive Bayes",
    "text": "Algoritmo Naive Bayes\n\nBaseia-se no Teorema de Bayes, que afirma que a probabilidade de um evento ocorrer dado que outro evento j√° ocorreu √© proporcional √† probabilidade deste √∫ltimo evento ocorrer dado o primeiro.\n\nA ‚Äúingenuidade‚Äù (Naive): assume que os preditores s√£o independentes entre si, dado o valor da classe.\n\n\nAplicado principalmente em problemas de classifica√ß√£o, especialmente com dados categ√≥ricos ou textuais."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#revis√£o-do-teorema-de-bayes",
    "href": "aulas/Analise_preditiva/preditiva.html#revis√£o-do-teorema-de-bayes",
    "title": "An√°lise Preditiva",
    "section": "Revis√£o do Teorema de Bayes",
    "text": "Revis√£o do Teorema de Bayes\nSejam A e B dois eventos\n\\[P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)},\\,\\,\\,\\,\\,\\,\\, \\text{para } P(B)&gt;0\\]\n\nO Teorema de Bayes Aplicado ao Naive Bayes:\n\\[P(\\text{Classe}|\\text{Caracter√≠sticas}) = \\dfrac{P(\\text{Caracter√≠sticas}|\\text{Classe})P(\\text{Classe})}{P(\\text{Caracter√≠sticas})}\\]\n\n\n\nüëâO objetivo √© encontrar a classe com a maior probabilidade posterior."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#o-teorema-de-bayes-aplicado-ao-naive-bayes",
    "href": "aulas/Analise_preditiva/preditiva.html#o-teorema-de-bayes-aplicado-ao-naive-bayes",
    "title": "An√°lise Preditiva",
    "section": "O Teorema de Bayes Aplicado ao Naive Bayes",
    "text": "O Teorema de Bayes Aplicado ao Naive Bayes\nDevido a essa suposi√ß√£o de independ√™ncia condicional, a probabilidade das caracter√≠sticas conjuntas dado a classe pode ser simplificada para o produto das probabilidades de cada caracter√≠stica individual dado a classe:\n\\[P(X_1, X_2,\\cdots, X_n|\\text{Classe}) = P(X_1|\\text{Classe}) \\times P(X_2|\\text{Classe}) \\times \\cdots \\times P(X_n|\\text{Classe})\\]\n\nSubstituindo na f√≥rmula de Bayes, nos leva a\n\\[P(\\text{Classe}|\\text{Caracter√≠sticas}) \\propto P(\\text{Classe}) \\times \\prod_{i=1}^n P(X_i|\\text{Classe})\\]\nObserva√ß√£o importante: O denominador \\(P(\\text{Caracter√≠sticas})\\) √© constante para todas as classes, ent√£o podemos ignor√°-lo para fins de compara√ß√£o"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#tipos-de-naive-bayes",
    "href": "aulas/Analise_preditiva/preditiva.html#tipos-de-naive-bayes",
    "title": "An√°lise Preditiva",
    "section": "Tipos de Naive Bayes",
    "text": "Tipos de Naive Bayes\n\n\n\nGaussian Naive Bayes: Para caracter√≠sticas cont√≠nuas. Assume que as caracter√≠sticas s√£o distribu√≠das de acordo com uma distribui√ß√£o Gaussiana (Normal).\n\n\n\n\nMultinomial Naive Bayes: Ideal para contagens de ocorr√™ncias (ex: contagem de palavras em documentos, usado em classifica√ß√£o de texto).\n\n\n\n\n\nBernoulli Naive Bayes: Adequado para dados bin√°rios (presen√ßa/aus√™ncia de uma caracter√≠stica)."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-2",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-2",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\n\nüëç\n\n\n\nSimplicidade: F√°cil de entender e implementar.\n\nEfici√™ncia: Muito r√°pido para treinar e prever, especialmente em grandes conjuntos de dados.\n\nBom desempenho: Surpreendentemente eficaz em muitos problemas reais, mesmo com a suposi√ß√£o de independ√™ncia.\n\nProcessamento de texto: Excelente para classifica√ß√£o de documentos e filtragem de spam.\n\nN√£o requer muitos dados: Pode funcionar bem com conjuntos de dados menores."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-3",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-3",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\n\nüëé\n\n\n\nSuposi√ß√£o de independ√™ncia: A suposi√ß√£o de independ√™ncia entre as caracter√≠sticas raramente √© verdadeira no mundo real, o que pode limitar a precis√£o.\n\nProblema de ‚Äúzero frequ√™ncia‚Äù: Se uma categoria de caracter√≠stica n√£o aparecer no conjunto de treinamento para uma determinada classe, a probabilidade para essa caracter√≠stica ser√° zero, o que leva a uma probabilidade posterior zero. (para resolver: Laplace Smoothing)\n\nEstimativa de Probabilidades: Pode ter um desempenho ruim quando h√° caracter√≠sticas num√©ricas com distribui√ß√µes complexas (a suposi√ß√£o Gaussiana pode n√£o se aplicar)."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#pr√©-processamento-de-dados-para-naive-bayes",
    "href": "aulas/Analise_preditiva/preditiva.html#pr√©-processamento-de-dados-para-naive-bayes",
    "title": "An√°lise Preditiva",
    "section": "Pr√©-processamento de Dados para Naive Bayes",
    "text": "Pr√©-processamento de Dados para Naive Bayes\n\n\n\nüíª\n\n\n\nDados Categ√≥ricos: Transformar para formato num√©rico (One-Hot Encoding ou Label Encoding, dependendo do tipo de NB).\n\nDados Num√©ricos:\n\n\nGaussianNB: Assumir distribui√ß√£o normal. Normaliza√ß√£o/padroniza√ß√£o pode ajudar, mas n√£o √© estritamente necess√°ria como em KNN, j√° que n√£o se baseia em dist√¢ncia.\n\nDiscretiza√ß√£o: Converter dados num√©ricos em categ√≥ricos (bins) para usar Multinomial ou Bernoulli NB.\n\n\n\nTratamento de Valores Ausentes: Imputa√ß√£o ou remo√ß√£o.\n\nTextuais: Tokeniza√ß√£o, remo√ß√£o de stop words, stemiza√ß√£o/lema, TF-IDF, CountVectorizer."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-7",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-7",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-8",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-8",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\nVoltando ao Jo√£ozinho‚Ä¶\n\n¬† ¬†\n\n\n\\(x_0 = (\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Bom})\\)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-9",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-9",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\nProbabilidade de ocorr√™ncia das classes\n\n\n\\[ P(\\text{classe = Sim}) = \\dfrac{9}{14}\\] \\[P(\\text{classe = N√£o}) = \\dfrac{5}{14}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-10",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-10",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\n\\(x_0 = (\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Bom})\\)\n\n\nPara o atributo Idade:\n\\[ P(\\text{Idade} \\leq 30 | \\text{classe = Sim}) = \\dfrac{2}{9} \\\\ P(\\text{Idade} \\leq 30 | \\text{classe = N√£o}) = \\dfrac{3}{5}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-11",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-11",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\n\\(x_0 = (\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Bom})\\)\n\n\nPara o atributo Renda:\n\\[ P(\\text{Renda = M√©dia} | \\text{classe = Sim}) = \\dfrac{4}{9} \\\\ P(\\text{Renda = M√©dia} | \\text{classe = N√£o}) = \\dfrac{2}{5}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-12",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-12",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\n\\(x_0 = (\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Bom})\\)\n\n\nPara o atributo Estudante:\n\\[ P(\\text{Estudante = Sim} | \\text{classe = Sim}) = \\dfrac{6}{9} \\\\ P(\\text{Estudante = Sim} | \\text{classe = N√£o}) = \\dfrac{2}{5}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-13",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-13",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\n\\(x_0 = (\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Bom})\\)\n\n\nPara o atributo Cr√©dito:\n\\[ P(\\text{Cr√©dito = Bom} | \\text{classe = Sim}) = \\dfrac{6}{9} \\\\ P(\\text{Cr√©dito = Bom} | \\text{classe = N√£o}) = \\dfrac{3}{5}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-14",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-14",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\nTemos ent√£o, sob independ√™ncia:\n\\[P(x_0|\\text{Sim}) = P(\\leq 30 \\cap \\text{M√©dia} \\cap \\text{Sim} \\cap \\text{Bom}|\\text{Sim} ) \\\\ = \\dfrac{2}{9} \\times \\dfrac{4}{9}\\times \\dfrac{6}{9}\\times \\dfrac{6}{9} = \\dfrac{288}{729} = 0,0439\\\\\\]\n\ne,\n\\[P(x_0|\\text{N√£o}) = P(\\leq 30 \\cap \\text{M√©dia} \\cap \\text{Sim} \\cap \\text{Bom}|\\text{N√£o} ) \\\\ = \\dfrac{3}{5} \\times \\dfrac{2}{5}\\times \\dfrac{2}{5}\\times \\dfrac{3}{5} = \\dfrac{36}{625} = 0,0576\\\\\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-15",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-15",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\nPelo Teorema da Probabilidade Total:\n\\[\n\\begin{eqnarray*}\nP(x_0) &=& P(x_0|\\text{Sim}) \\times P(\\text{Sim}) + P(x_0|\\text{N√£o}) \\times P(\\text{N√£o}) \\\\ &=& 0,0439 \\times 0,6429 + 0,0576 \\times 0,3571 \\\\ &=& 0,0488\n\\end{eqnarray*}\n\\]\n\nAssim, pelo Teorema de Bayes:\n\\[P(\\text{Sim}|x_0) = \\dfrac{P(x_0|\\text{Sim}) \\times P(\\text{Sim})}{P(x_0)} = \\dfrac{0,0439 \\times 0,6429}{0,0488} = 0,5783\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-16",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-16",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\ne,\n\\[P(\\text{N√£o}|x_0) = \\dfrac{P(x_0|\\text{N√£o}) \\times P(\\text{N√£o})}{P(x_0)} = \\dfrac{0,0576 \\times 0,3571}{0,0488} = 0,4215 \\\\ \\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-17",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-17",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\nJo√£o comprar√° o computador?\n\nDe acordo com o Naive Bayes: SIM!"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#naive-bayes-para-classifica√ß√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#naive-bayes-para-classifica√ß√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "Naive Bayes para classifica√ß√£o no R e Python",
    "text": "Naive Bayes para classifica√ß√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris_split &lt;- initial_split(iris, strata = Species)\ntrain_data &lt;- training(iris_split)\ntest_data &lt;- testing(iris_split)\n\nnb_model &lt;- naive_Bayes() %&gt;% \n  set_engine(\"klaR\") %&gt;%\n  set_mode(\"classification\")\n\nnb_fit &lt;- workflow() %&gt;%\n  add_model(nb_model) %&gt;%\n  add_formula(Species ~ .) %&gt;%\n  fit(data = train_data)\n\nsp_predict &lt;- predict(nb_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n\n[1] 0.9487179\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB # Para dados cont√≠nuos como Iris\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.datasets import load_iris # Para carregar o dataset Iris\n\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target_names[iris.target], name='species') # Mapear n√∫meros para nomes de esp√©cies\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\nmodel_nb = GaussianNB()\nmodel_nb.fit(X_train, y_train)\n\n\n\n\nGaussianNB()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nGaussianNBGaussianNB()\n\n\n\ny_pred = model_nb.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.9555555555555556"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#√°rvores-de-decis√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#√°rvores-de-decis√£o",
    "title": "An√°lise Preditiva",
    "section": "√Årvores de decis√£o",
    "text": "√Årvores de decis√£o\n\nUma √°rvore de decis√£o √© um modelo de aprendizado de m√°quina que utiliza uma estrutura de √°rvore para tomar decis√µes com base em condi√ß√µes nos dados de entrada.\n\nEssa estrutura hier√°rquica consiste em n√≥s que representam testes sobre atributos e arestas que conectam os n√≥s, indicando os resultados desses testes.\n\n\nCada n√≥ interno da √°rvore representa um teste em um atributo espec√≠fico, enquanto as folhas representam as classes ou valores de sa√≠da."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#√°rvores-de-decis√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#√°rvores-de-decis√£o-1",
    "title": "An√°lise Preditiva",
    "section": "√Årvores de decis√£o",
    "text": "√Årvores de decis√£o\n\n\n\n\n\n\n\n\n\nAo percorrer a √°rvore da raiz at√© uma folha, os dados de entrada s√£o avaliados de acordo com os testes em cada n√≥, seguindo o caminho apropriado at√© alcan√ßar uma folha, onde √© tomada a decis√£o final."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#principais-algoritmos",
    "href": "aulas/Analise_preditiva/preditiva.html#principais-algoritmos",
    "title": "An√°lise Preditiva",
    "section": "Principais Algoritmos",
    "text": "Principais Algoritmos\n\n\nID3 (Iterative Dichotomiser 3)\n\n\n√â um dos primeiros algoritmos de √°rvore de decis√£o e utiliza o ganho de informa√ß√£o como crit√©rio para selecionar a melhor divis√£o em cada n√≥. No entanto, o ID3 n√£o lida diretamente com atributos num√©ricos.\n\n\n\nFavorece caracter√≠sticas com muitos valores distintos."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#principais-algoritmos-1",
    "href": "aulas/Analise_preditiva/preditiva.html#principais-algoritmos-1",
    "title": "An√°lise Preditiva",
    "section": "Principais Algoritmos",
    "text": "Principais Algoritmos\n\n\nC4.5\n\n\n√â uma extens√£o do ID3 e possui melhorias, incluindo a capacidade de lidar com atributos num√©ricos e valores ausentes. Al√©m disso, o C4.5 utiliza a raz√£o de ganho como m√©trica de sele√ß√£o de atributos, em vez do ganho de informa√ß√£o utilizado pelo ID3.\n\n\n\nCorrige o vi√©s em rela√ß√£o a caracter√≠sticas com muitos valores."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#principais-algoritmos-2",
    "href": "aulas/Analise_preditiva/preditiva.html#principais-algoritmos-2",
    "title": "An√°lise Preditiva",
    "section": "Principais Algoritmos",
    "text": "Principais Algoritmos\n\n\nCART (Classification and Regression Trees)\n\n\nPode ser usado tanto para classifica√ß√£o quanto para regress√£o.\n\n\n\nPara classifica√ß√£o, usa o √≠ndice de Gini como crit√©rio de divis√£o.\n\n\n\n\nGera √°rvores bin√°rias (cada n√≥ tem exatamente dois filhos).\n\n\n\n\nEle busca minimizar a impureza nos n√≥s da √°rvore."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#principais-algoritmos-3",
    "href": "aulas/Analise_preditiva/preditiva.html#principais-algoritmos-3",
    "title": "An√°lise Preditiva",
    "section": "Principais Algoritmos",
    "text": "Principais Algoritmos\n\n\nCART (Classification and Regression Trees)\n\n\nPara regress√£o, busca dividir os dados de forma a minimizar o erro quadr√°tico m√©dio (MSE) ou o desvio absoluto m√©dio (MAE) nos n√≥s folha."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#processo-de-prepara√ß√£o-de-um-modelo-de-√°rvore-de-decis√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#processo-de-prepara√ß√£o-de-um-modelo-de-√°rvore-de-decis√£o",
    "title": "An√°lise Preditiva",
    "section": "Processo de prepara√ß√£o de um modelo de √Årvore de Decis√£o",
    "text": "Processo de prepara√ß√£o de um modelo de √Årvore de Decis√£o"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n\n\nO objetivo dos crit√©rios de divis√£o √© encontrar a melhor caracter√≠stica para separar os dados em subconjuntos mais ‚Äúpuros‚Äù em rela√ß√£o √† vari√°vel alvo (classe)."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-1",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n\nEntropia\n\nA entropia √© uma medida de impureza ou aleatoriedade dos dados em um algoritmo de √°rvore de decis√£o. Ela √© utilizada para avaliar o qu√£o homog√™neos ou heterog√™neos s√£o os exemplos de uma determinada classe em um conjunto de dados.\n\n√â calculada a partir da distribui√ß√£o das classes no conjunto de dados. Quanto maior a entropia, maior a incerteza sobre a classe de um exemplo. Quanto menor a entropia, mais homog√™neos s√£o os exemplos em rela√ß√£o √† classe."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-2",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-2",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n\nEntropia\n\n\\[ \\text{E(S)} = \\sum_{i=1}^c -p_i\\log_2 p_i\\]\nem que \\(p_i\\) √© a propor√ß√£o de exemplos na classe \\(i\\) da amostra de treinamento S."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-3",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-3",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n\nGanho de Informa√ß√£o (G)\n\nO ganho de informa√ß√£o √© uma m√©trica utilizada para medir a relev√¢ncia de um atributo na divis√£o dos dados. Ele indica a quantidade de informa√ß√£o que um atributo fornece sobre a classe ou vari√°vel de sa√≠da.\n\nMede a redu√ß√£o na entropia ap√≥s a divis√£o do conjunto de dados por uma caracter√≠stica.\n\n\nO ID3 escolhe a caracter√≠stica com o maior ganho de informa√ß√£o."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-4",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-4",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n\nGanho de Informa√ß√£o (G)\n\n\\[ \\text{G}(S, A) = \\text{E}(S) - \\text{E}(S,A)\\]\nem que \\(E(S,A) = \\sum_{v\\in \\text{valores de } A} \\dfrac{|S_v|}{|S|} \\times \\text{E}(S_v)\\) √© a entropia do atributo A."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-5",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-5",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n\nRaz√£o de ganho\n\nTamb√©m conhecida como gain ratio, √© uma m√©trica utilizada para selecionar os melhores atributos de divis√£o, levando em considera√ß√£o o vi√©s por atributos com muitos valores poss√≠veis.\n\nEnquanto o ganho de informa√ß√£o mede a redu√ß√£o da entropia dos dados ap√≥s a divis√£o com base em um atributo, a raz√£o de ganho ajusta esse valor, levando em considera√ß√£o o n√∫mero de valores distintos do atributo.\n\n\nEssa corre√ß√£o √© importante para evitar que atributos com muitos valores poss√≠veis tenham vantagem sobre atributos com menos valores."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-6",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-6",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n\nRaz√£o de ganho\n\n\\[ \\text{Gain ratio}(S, A) = \\dfrac{G(S, A)}{E(S, A)}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-7",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-7",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n√çndice de Gini\n\nO √≠ndice de Gini √© outra medida de impureza utilizada para avaliar a heterogeneidade dos dados em rela√ß√£o √† classe ou vari√°vel de sa√≠da.\n\nMedida da probabilidade de uma amostra ser classificada incorretamente se fosse aleatoriamente rotulada de acordo com a distribui√ß√£o das classes no subconjunto.\n\n\nUm valor menor indica maior pureza.\n\n\nUsado pelo CART para classifica√ß√£o."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-8",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-classifica√ß√£o-8",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Classifica√ß√£o",
    "text": "Crit√©rios de Divis√£o para Classifica√ß√£o\n\n√çndice de Gini\n\n\\[ \\text{Gini}(S) = 1 - \\sum_{i=1}^c p_i^2\\] em que \\(p_i\\) √© a propor√ß√£o de exemplos na classe \\(i\\) da amostra de treinamento S.\n\nO CART busca a divis√£o que resulta na maior redu√ß√£o no √≠ndice de Gini.\n\n\nO √≠ndice de Gini varia de 0 a 1, sendo 0 quando todos os exemplos pertencem √† mesma classe (alta pureza) e 1 quando os exemplos est√£o igualmente distribu√≠dos entre as classes (alta impureza)."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-regress√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-regress√£o",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Regress√£o",
    "text": "Crit√©rios de Divis√£o para Regress√£o\n\n\n\nO objetivo √© dividir os dados de forma a minimizar a variabilidade da vari√°vel alvo dentro de cada n√≥ folha."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-regress√£o-1",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-regress√£o-1",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Regress√£o",
    "text": "Crit√©rios de Divis√£o para Regress√£o\n\nRedu√ß√£o do Erro Quadr√°tico M√©dio (MSE):\n\nO CART para regress√£o geralmente busca a divis√£o que maximiza a redu√ß√£o no MSE.\n\nCalcula-se o MSE no n√≥ pai e o MSE ponderado nos n√≥s filhos ap√≥s a divis√£o. A divis√£o que proporciona a maior redu√ß√£o √© escolhida.\n\n\n\\[MSE = \\dfrac{1}{|S|} \\displaystyle{\\sum_{i\\in S} (y_i - \\bar{y})^2}\\]\nem que \\(y_i\\) √© o valor da vari√°vel alvo para a amostra \\(i\\) e \\(\\bar{y}\\) √© a m√©dia dos valores da vari√°vel alvo em S."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-regress√£o-2",
    "href": "aulas/Analise_preditiva/preditiva.html#crit√©rios-de-divis√£o-para-regress√£o-2",
    "title": "An√°lise Preditiva",
    "section": "Crit√©rios de Divis√£o para Regress√£o",
    "text": "Crit√©rios de Divis√£o para Regress√£o\n\nRedu√ß√£o do Desvio Absoluto M√©dio (MAE):\n\nAlternativamente, alguns algoritmos de √°rvores de regress√£o podem usar o MAE.\n\n\\[MAE = \\dfrac{1}{|S|} \\displaystyle{\\sum_{i\\in S} |y_i - \\text{mediana}(y)|}\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-4",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-4",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\n\nüëç\n\n\nF√°ceis de entender e interpretar (modelos ‚Äúcaixa branca‚Äù).\nPodem lidar com dados num√©ricos e categ√≥ricos.\nN√£o exigem muita prepara√ß√£o dos dados (n√£o sens√≠veis a escala ou transforma√ß√µes mon√≥tonas).\nCapazes de capturar rela√ß√µes n√£o lineares entre as caracter√≠sticas e a vari√°vel alvo.\n√öteis para sele√ß√£o de caracter√≠sticas (as caracter√≠sticas mais pr√≥ximas da raiz s√£o consideradas mais importantes)."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-5",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-5",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\n\nüëé\n\n\nTend√™ncia ao overfitting (ajustar-se demais aos dados de treinamento, resultando em mau desempenho em dados novos).\nPodem ser sens√≠veis a pequenas varia√ß√µes nos dados de treinamento.\nPodem criar √°rvores complexas que n√£o generalizam bem.\nPara algumas tarefas, podem n√£o ser t√£o precisas quanto outros algoritmos.\nO algoritmo guloso de constru√ß√£o da √°rvore nem sempre encontra a melhor √°rvore poss√≠vel."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#controle-de-overfitting-podas",
    "href": "aulas/Analise_preditiva/preditiva.html#controle-de-overfitting-podas",
    "title": "An√°lise Preditiva",
    "section": "Controle de Overfitting (Podas)",
    "text": "Controle de Overfitting (Podas)\n\nA poda √© o processo de reduzir o tamanho de uma √°rvore de decis√£o, removendo se√ß√µes da √°rvore que podem estar superajustando os dados de treinamento.\n\nO objetivo da poda √© simplificar a √°rvore, tornando-a mais generaliz√°vel e menos propensa a memorizar ru√≠dos espec√≠ficos do conjunto de treinamento."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#controle-de-overfitting-podas-1",
    "href": "aulas/Analise_preditiva/preditiva.html#controle-de-overfitting-podas-1",
    "title": "An√°lise Preditiva",
    "section": "Controle de Overfitting (Podas)",
    "text": "Controle de Overfitting (Podas)\n\nTipos de Poda:\n\n\nPr√©-poda: Parar o crescimento da √°rvore antecipadamente com base em crit√©rios (ex: profundidade m√°xima, n√∫mero m√≠nimo de amostras por n√≥).\n\nMais eficiente computacionalmente; pode levar ao underfitting\n\n\n\n\n\n\nP√≥s-poda: Construir a √°rvore completa e depois remover n√≥s que n√£o melhoram o desempenho em um conjunto de valida√ß√£o.\n\n√Årvores com melhor capacidade de generaliza√ß√£o; mais intensiva computacionalmente"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-18",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-18",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-19",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-19",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\nC√°lculo da Entropia\n\n\\[ \\text{E(S)} = \\sum_{i=1}^c -p_i\\log_2 p_i\\]\n\n\\[ \\text{E(S)} = -\\dfrac{9}{14}\\log_2 (\\dfrac{9}{14}) - \\dfrac{5}{14}\\log_2 (\\dfrac{5}{14}) = 0,940\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-20",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-20",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\n\n\nClasse\n\n\n\n\n\n\n\n\n\n\n\nSim\n\n\nN√£o\n\n\n\n\n\n\nIdade\n\n\n=&lt; 30\n\n\n2\n\n\n3\n\n\n5\n\n\n\n\n31‚Ä¶40\n\n\n4\n\n\n0\n\n\n4\n\n\n\n\n&gt;40\n\n\n3\n\n\n2\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGanho de informa√ß√£o para o atributo idade\n\n\\[ \\text{E(S, Idade)} = \\dfrac{|\\leq 30|}{|S|}\\times \\text{E}(\\leq 30)\\\\\n+ \\dfrac{|31...40|}{|S|}\\times \\text{E}(31...40) \\\\ +  \\dfrac{|&gt;40|}{|S|}\\times \\text{E}(&gt;40)\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-21",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-21",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\n\n\nClasse\n\n\n\n\n\n\n\n\n\n\n\nSim\n\n\nN√£o\n\n\n\n\n\n\nIdade\n\n\n=&lt; 30\n\n\n2\n\n\n3\n\n\n5\n\n\n\n\n31‚Ä¶40\n\n\n4\n\n\n0\n\n\n4\n\n\n\n\n&gt;40\n\n\n3\n\n\n2\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGanho de informa√ß√£o para o atributo idade\n\n\\[ \\text{E(S, Idade)} = \\dfrac{5}{14}\\times \\text{E}(\\leq 30) \\\\ + \\dfrac{4}{14}\\times \\text{E}(31...40) \\\\ +  \\dfrac{5}{14}\\times \\text{E}(&gt;40)\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-22",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-22",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\n\n\nClasse\n\n\n\n\n\n\n\n\n\n\n\nSim\n\n\nN√£o\n\n\n\n\n\n\nIdade\n\n\n=&lt; 30\n\n\n2\n\n\n3\n\n\n5\n\n\n\n\n31‚Ä¶40\n\n\n4\n\n\n0\n\n\n4\n\n\n\n\n&gt;40\n\n\n3\n\n\n2\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGanho de informa√ß√£o para o atributo idade\n\n\\[ \\text{E}(\\leq 30) = -\\dfrac{2}{5}\\log_2 (\\dfrac{2}{5}) - \\dfrac{3}{5}\\log_2 (\\dfrac{3}{5}) = 0,971\\] \\[ \\text{E}(31...40) = 0\\,\\,\\,\\,\\,\\,\\,\\, \\text{e} \\,\\,\\,\\,\\,\\,\\,\\, \\text{E}(&gt;40) = 0,971\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-23",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-23",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\n\n\n\nClasse\n\n\n\n\n\n\n\n\n\n\n\nSim\n\n\nN√£o\n\n\n\n\n\n\nIdade\n\n\n=&lt; 30\n\n\n2\n\n\n3\n\n\n5\n\n\n\n\n31‚Ä¶40\n\n\n4\n\n\n0\n\n\n4\n\n\n\n\n&gt;40\n\n\n3\n\n\n2\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGanho de informa√ß√£o para o atributo idade\n\n\\[ \\text{E(S, Idade)} = \\dfrac{5}{|14|}\\times 0,971  + \\dfrac{4}{|14|}\\times 0 \\\\ +  \\dfrac{5}{14}\\times 0,971 = 0,693\\]\n\\[ \\text{G}(S, \\text{Idade}) = \\text{E}(S) - \\text{E}(S,\\text{Idade})\\\\ = 0,940 - 0,693 = 0,247\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-24",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-24",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-25",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-25",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\nEscolhemos o atributo com maior ganho de informa√ß√£o como n√≥ de decis√£o. No nosso caso, o atributo Idade. A partir da√≠, dividimos o conjunto de dados a partir das categorias da vari√°vel idade e repetimos o mesmo processo em todos os ramos.\n\nUm ramo com entropia de 0 √© um n√≥ folha. Um ramo com entropia maior que 0 precisa de mais divis√£o."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-26",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-26",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n√Årvore estimada"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-27",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-27",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\n\n\n\n\n\n\n\nVoltando ao Jo√£ozinho‚Ä¶\n\n¬† ¬†\n\n\n\\(x_0 = (\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Bom})\\)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-28",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-28",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\\(x_0 = (\\leq 30, \\text{ M√©dia}, \\text{ Sim}, \\text{ Bom})\\)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-29",
    "href": "aulas/Analise_preditiva/preditiva.html#exemplo-compra-de-um-computador-29",
    "title": "An√°lise Preditiva",
    "section": "Exemplo: Compra de um computador",
    "text": "Exemplo: Compra de um computador\n\n\nJo√£o comprar√° o computador?\n\nDe acordo com √Årvores de Decis√£o: SIM!"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#√°rvores-de-decis√£o-para-classifica√ß√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#√°rvores-de-decis√£o-para-classifica√ß√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "√Årvores de Decis√£o para classifica√ß√£o no R e Python",
    "text": "√Årvores de Decis√£o para classifica√ß√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris_split &lt;- initial_split(iris, strata = Species)\ntrain_data &lt;- training(iris_split)\ntest_data &lt;- testing(iris_split)\n\ntree_model &lt;- decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ntree_fit &lt;- workflow() %&gt;%\n  add_model(tree_model) %&gt;%\n  add_formula(Species ~ .) %&gt;%\n  fit(data = train_data)\n\nsp_predict &lt;- predict(tree_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n\n[1] 0.8974359\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target_names[iris.target], name='species') # Mapear n√∫meros para nomes de esp√©cies\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\ntree = DecisionTreeClassifier(random_state=12345)\ntree.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(random_state=12345)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nDecisionTreeClassifierDecisionTreeClassifier(random_state=12345)\n\n\n\ny_pred = tree.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.9777777777777777"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#√°rvores-de-decis√£o-para-regress√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#√°rvores-de-decis√£o-para-regress√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "√Årvores de Decis√£o para regress√£o no R e Python",
    "text": "√Årvores de Decis√£o para regress√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split &lt;- initial_split(mtcars, strata = mpg)\ntrain_data &lt;- training(mtcars_split)\ntest_data &lt;- testing(mtcars_split)\n\ntree_model &lt;- decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\ntree_fit &lt;- workflow() %&gt;%\n  add_model(tree_model) %&gt;%\n  add_formula(mpg ~ .) %&gt;%\n  fit(data = train_data)\n\nmpg_predict &lt;- predict(tree_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n\n[1] 17.00174\n\n\n\n\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\ntree = DecisionTreeRegressor(random_state=12345)\ntree.fit(X_train, y_train)\n\n\n\n\nDecisionTreeRegressor(random_state=12345)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nDecisionTreeRegressorDecisionTreeRegressor(random_state=12345)\n\n\n\ny_pred = tree.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n\n0.5445212937679262"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#random-forests",
    "href": "aulas/Analise_preditiva/preditiva.html#random-forests",
    "title": "An√°lise Preditiva",
    "section": "Random Forests",
    "text": "Random Forests\n\nRandom Forests √© um algoritmo de aprendizado de m√°quina que combina v√°rias √°rvores de decis√£o independentes para realizar tarefas de classifica√ß√£o ou regress√£o.\n\nEle pertence √† categoria de m√©todos ensemble, que buscam melhorar a precis√£o e robustez das predi√ß√µes combinando diferentes modelos para se obter um √∫nico resultado."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#random-forests-1",
    "href": "aulas/Analise_preditiva/preditiva.html#random-forests-1",
    "title": "An√°lise Preditiva",
    "section": "Random Forests",
    "text": "Random Forests\nUtiliza bagging (bootstrap aggregation) para criar m√∫ltiplas √°rvores e combinar suas previs√µes."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#constru√ß√£o-de-um-random-forests",
    "href": "aulas/Analise_preditiva/preditiva.html#constru√ß√£o-de-um-random-forests",
    "title": "An√°lise Preditiva",
    "section": "Constru√ß√£o de um Random Forests",
    "text": "Constru√ß√£o de um Random Forests\n\nAmostras s√£o geradas com substitui√ß√£o (bootstrap)\n\n\n\nPara cada amostra, os dados s√£o divididos utilizando os atributos que maximizem a pureza dos n√≥s\n\n\nCrit√©rios de divis√£o: Gini Index, Entropia, Ganho de Informa√ß√£o, etc.\n\n\n\n\n\n\nA cada divis√£o da √°rvore, uma amostra aleat√≥ria de vari√°veis √© usada\n\nConstru√ß√£o recursiva at√© atingir crit√©rios de parada (profundidade m√°xima, pureza m√≠nima)\n\n\n\n\n\n\nPrevis√£o final: Cada √°rvore possui um peso igual na vota√ß√£o final\n\nClassifica√ß√£o: maioria dos votos\nRegress√£o: m√©dia das previs√µes"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#constru√ß√£o-de-um-random-forests-1",
    "href": "aulas/Analise_preditiva/preditiva.html#constru√ß√£o-de-um-random-forests-1",
    "title": "An√°lise Preditiva",
    "section": "Constru√ß√£o de um Random Forests",
    "text": "Constru√ß√£o de um Random Forests"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#constru√ß√£o-de-um-random-forests-2",
    "href": "aulas/Analise_preditiva/preditiva.html#constru√ß√£o-de-um-random-forests-2",
    "title": "An√°lise Preditiva",
    "section": "Constru√ß√£o de um Random Forests",
    "text": "Constru√ß√£o de um Random Forests\n\n\n\n\n\n\n\n\n\nQual a classe predita?\n\nPela regra da maioria: classe 1"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#hiperpar√¢metros-principais",
    "href": "aulas/Analise_preditiva/preditiva.html#hiperpar√¢metros-principais",
    "title": "An√°lise Preditiva",
    "section": "Hiperpar√¢metros principais",
    "text": "Hiperpar√¢metros principais\n\n\nn_estimators / ntree: n√∫mero de √°rvores na floresta\nmax_features / mtry: n√∫mero de vari√°veis usadas por divis√£o\nmax_depth: profundidade m√°xima das √°rvores\nmin_samples_split: m√≠nimo de amostras para uma divis√£o"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-6",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-6",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\n\nüëç\n\n\nTrata efetivamente problemas de overfitting\n\nLida bem com ru√≠dos, dados ausentes e valores discrepantes\n\nLida com classifica√ß√µes multiclasses\nPouco ajuste de par√¢metros\nCapaz de lidar com atributos de diferentes tipos (num√©ricos, categ√≥ricos)\nReduz a vari√¢ncia e melhora a precis√£o em compara√ß√£o com uma √∫nica √°rvore"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-7",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-7",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\nüëé\n\n\n\n\nMenos interpret√°vel que uma √°rvore √∫nica\nMais custoso computacionalmente"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#random-forests-para-classifica√ß√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#random-forests-para-classifica√ß√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "Random Forests para classifica√ß√£o no R e Python",
    "text": "Random Forests para classifica√ß√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris_split &lt;- initial_split(iris, strata = Species)\ntrain_data &lt;- training(iris_split)\ntest_data &lt;- testing(iris_split)\n\nrf_model &lt;- rand_forest(mtry = 2, trees = 500) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\nrf_fit &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_formula(Species ~ .) %&gt;%\n  fit(data = train_data)\n\nsp_predict &lt;- predict(rf_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n\n[1] 0.8974359\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target_names[iris.target], name='species') # Mapear n√∫meros para nomes de esp√©cies\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\nrf_model = RandomForestClassifier(n_estimators=500, max_features=2)\nrf_model.fit(X_train, y_train)\n\n\n\n\nRandomForestClassifier(max_features=2, n_estimators=500)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRandomForestClassifierRandomForestClassifier(max_features=2, n_estimators=500)\n\n\n\ny_pred = rf_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.9777777777777777"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#random-forests-para-regress√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#random-forests-para-regress√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "Random Forests para regress√£o no R e Python",
    "text": "Random Forests para regress√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split &lt;- initial_split(mtcars, strata = mpg)\ntrain_data &lt;- training(mtcars_split)\ntest_data &lt;- testing(mtcars_split)\n\nrf_reg &lt;- rand_forest(mtry = 3, trees = 500) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_reg_fit &lt;- workflow() %&gt;%\n  add_model(rf_reg) %&gt;%\n  add_formula(mpg ~ .) %&gt;%\n  fit(data = train_data)\n\nmpg_predict &lt;- predict(rf_reg_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n\n[1] 6.18105\n\n\n\n\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\nreg = RandomForestRegressor(n_estimators=100, max_features=6)\nreg.fit(X_train, y_train)\n\n\n\n\nRandomForestRegressor(max_features=6)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRandomForestRegressorRandomForestRegressor(max_features=6)\n\n\n\ny_pred = reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n\n0.2567399557335787"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#import√¢ncia-das-vari√°veis",
    "href": "aulas/Analise_preditiva/preditiva.html#import√¢ncia-das-vari√°veis",
    "title": "An√°lise Preditiva",
    "section": "Import√¢ncia das vari√°veis",
    "text": "Import√¢ncia das vari√°veis\n\n\nRandom Forest permite extrair import√¢ncia das vari√°veis\nMostra o quanto cada vari√°vel contribui para a redu√ß√£o da impureza ou erro"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#import√¢ncia-das-vari√°veis-1",
    "href": "aulas/Analise_preditiva/preditiva.html#import√¢ncia-das-vari√°veis-1",
    "title": "An√°lise Preditiva",
    "section": "Import√¢ncia das vari√°veis",
    "text": "Import√¢ncia das vari√°veis\n\n\nR\nPython\n\n\n\n\n\nlibrary(vip)\nextract_fit_parsnip(rf_fit) %&gt;% vip(num_features = 4)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Criar DataFrame com vari√°veis e import√¢ncias\nfeat_imp = pd.DataFrame({\n    \"Vari√°vel\": iris.feature_names,\n    \"Import√¢ncia\": rf_model.feature_importances_\n})\n\n# Ordenar pela import√¢ncia\nfeat_imp = feat_imp.sort_values(by=\"Import√¢ncia\", ascending=False)\n\n# Plotar gr√°fico ordenado\nsns.barplot(x=\"Import√¢ncia\", y=\"Vari√°vel\", data=feat_imp, palette=\"viridis\")\nplt.title(\"Import√¢ncia das vari√°veis\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#support-vector-machine-svm",
    "href": "aulas/Analise_preditiva/preditiva.html#support-vector-machine-svm",
    "title": "An√°lise Preditiva",
    "section": "Support Vector Machine (SVM)",
    "text": "Support Vector Machine (SVM)\n\n\nAlgoritmo supervisionado usado para classifica√ß√£o e regress√£o.\n\n\n\nBusca uma fronteira √≥tima (hiperplano) que separa as classes com a maior margem poss√≠vel.\n\n\n\n\nDepende de poucos pontos cr√≠ticos: os vetores de suporte.\n\n\n\n\nPode trabalhar com dados n√£o linearmente separ√°veis atrav√©s da t√©cnica do kernel trick."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#por-que-aprender-svm",
    "href": "aulas/Analise_preditiva/preditiva.html#por-que-aprender-svm",
    "title": "An√°lise Preditiva",
    "section": "Por que aprender SVM?",
    "text": "Por que aprender SVM?\n\n\n√â um dos algoritmos mais eficazes para problemas de classifica√ß√£o com fronteiras complexas.\n\n\n\nFunciona bem em espa√ßos de alta dimens√£o.\n\n\n\n\nUtiliza o conceito de margem m√°xima, promovendo boa generaliza√ß√£o.\n\n\n\n\nAmplamente utilizado em √°reas como bioinform√°tica, finan√ßas, reconhecimento de padr√µes."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#princ√≠pio-b√°sico",
    "href": "aulas/Analise_preditiva/preditiva.html#princ√≠pio-b√°sico",
    "title": "An√°lise Preditiva",
    "section": "Princ√≠pio b√°sico",
    "text": "Princ√≠pio b√°sico\n\nO objetivo do SVM √© encontrar um hiperplano de separa√ß√£o √≥timo que maximize a margem entre as classes.\n\nPara problemas de classifica√ß√£o bin√°ria, um hiperplano de separa√ß√£o √© uma superf√≠cie que divide o espa√ßo de caracter√≠sticas em duas regi√µes, uma para cada classe. Neste caso, esse hiperplano √© uma reta!\n\n\nEm problemas multiclasse, pode ser um plano ou uma superf√≠cie mais complexa."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#hiperplano-de-separa√ß√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#hiperplano-de-separa√ß√£o",
    "title": "An√°lise Preditiva",
    "section": "Hiperplano de Separa√ß√£o",
    "text": "Hiperplano de Separa√ß√£o\n\n\n\n\n\nTemos aqui 4 hiperplanos (A, B, C e D). Qual o melhor?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#conceitos-de-vetores-de-suporte-e-margem-m√°xima",
    "href": "aulas/Analise_preditiva/preditiva.html#conceitos-de-vetores-de-suporte-e-margem-m√°xima",
    "title": "An√°lise Preditiva",
    "section": "Conceitos de vetores de suporte e Margem M√°xima",
    "text": "Conceitos de vetores de suporte e Margem M√°xima\n\nOs vetores de suporte s√£o os pontos de dados mais pr√≥ximos ao hiperplano de separa√ß√£o. Eles s√£o fundamentais para o SVM, pois definem a posi√ß√£o do hiperplano e a margem.\n\nA margem √© a dist√¢ncia entre o hiperplano de separa√ß√£o e os vetores de suporte mais pr√≥ximos. O objetivo do SVM √© maximizar a margem, pois isso aumenta a capacidade de generaliza√ß√£o do modelo."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#conceitos-de-vetores-de-suporte-e-margem-m√°xima-1",
    "href": "aulas/Analise_preditiva/preditiva.html#conceitos-de-vetores-de-suporte-e-margem-m√°xima-1",
    "title": "An√°lise Preditiva",
    "section": "Conceitos de vetores de suporte e Margem M√°xima",
    "text": "Conceitos de vetores de suporte e Margem M√°xima\n\n\n\n\n\n\n\n\nMargem = \\(d_1 + d_2\\)\n\n\n\n\nüëâ Para encontrar o hiperplano √≥timo, ca√≠mos em um problema de otimiza√ß√£o com restri√ß√£o, que pode ser resolvido utilizando a t√©cnica dos Multiplicadores de Lagrange!"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#kernel-trick-separando-o-insepar√°vel",
    "href": "aulas/Analise_preditiva/preditiva.html#kernel-trick-separando-o-insepar√°vel",
    "title": "An√°lise Preditiva",
    "section": "Kernel Trick: separando o insepar√°vel",
    "text": "Kernel Trick: separando o insepar√°vel\nMas e se tivermos problemas n√£o linearmente separ√°veis?"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#kernel-trick-separando-o-insepar√°vel-1",
    "href": "aulas/Analise_preditiva/preditiva.html#kernel-trick-separando-o-insepar√°vel-1",
    "title": "An√°lise Preditiva",
    "section": "Kernel Trick: separando o insepar√°vel",
    "text": "Kernel Trick: separando o insepar√°vel\nO SVM √© um algoritmo que inicialmente trabalha com dados linearmente separ√°veis. No entanto, muitas vezes encontramos conjuntos de dados que n√£o podem ser separados por um hiperplano linear. √â a√≠ que entra o Kernel Trick.\n\nO Kernel Trick (Truque do Kernel) nos permite mapear os dados para um espa√ßo de caracter√≠sticas de dimens√£o superior, onde se tornam linearmente separ√°veis.\n\n\nIsso √© feito atrav√©s de uma fun√ß√£o chamada kernel, que calcula o produto interno entre dois vetores nesse espa√ßo de caracter√≠sticas."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#kernel-trick-separando-o-insepar√°vel-2",
    "href": "aulas/Analise_preditiva/preditiva.html#kernel-trick-separando-o-insepar√°vel-2",
    "title": "An√°lise Preditiva",
    "section": "Kernel Trick: separando o insepar√°vel",
    "text": "Kernel Trick: separando o insepar√°vel"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#kernel-trick-separando-o-insepar√°vel-3",
    "href": "aulas/Analise_preditiva/preditiva.html#kernel-trick-separando-o-insepar√°vel-3",
    "title": "An√°lise Preditiva",
    "section": "Kernel Trick: separando o insepar√°vel",
    "text": "Kernel Trick: separando o insepar√°vel\n\n\n\nüëâ Exemplos comuns:\n\nLinear\nPolinomial\nRadial Basis Function (RBF ou Gaussiano)\nSigmoide"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#escolha-do-kernel",
    "href": "aulas/Analise_preditiva/preditiva.html#escolha-do-kernel",
    "title": "An√°lise Preditiva",
    "section": "Escolha do kernel",
    "text": "Escolha do kernel\n\nA escolha do kernel adequado √© importante para obter um bom desempenho do SVM. √â necess√°rio analisar as caracter√≠sticas dos dados e testar diferentes kernels para encontrar o que melhor se adapta ao problema em quest√£o\n\nO uso do Kernel Trick pode melhorar o desempenho do SVM ao permitir que ele modele rela√ß√µes n√£o lineares nos dados. No entanto, √© preciso ter cuidado para evitar o overfitting, garantindo que o modelo generalize bem para dados n√£o vistos."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#svm-para-problemas-multiclasse",
    "href": "aulas/Analise_preditiva/preditiva.html#svm-para-problemas-multiclasse",
    "title": "An√°lise Preditiva",
    "section": "SVM para problemas multiclasse",
    "text": "SVM para problemas multiclasse\nO SVM como definido funciona para duas classes. O que fazemos ent√£o se tivermos mais de duas classes?\n\n\n\n\n\nOne-vs-One (OVO)\n\nNessa abordagem, √© criado um classificador SVM para cada par de classes poss√≠vel.\nCada classificador √© usado para classificar uma inst√¢ncia de entrada e a classe com mais votos √© selecionada como a classe final."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#svm-para-problemas-multiclasse-1",
    "href": "aulas/Analise_preditiva/preditiva.html#svm-para-problemas-multiclasse-1",
    "title": "An√°lise Preditiva",
    "section": "SVM para problemas multiclasse",
    "text": "SVM para problemas multiclasse\n\n\n\n\nOne-vs-All (OVA)\n\nEssa abordagem consiste em treinar um classificador SVM para cada classe, onde cada classificador √© treinado para distinguir uma classe espec√≠fica das demais classes combinadas.\nDurante a fase de teste, cada classificador √© usado para classificar uma inst√¢ncia de entrada e a classe com a maior probabilidade √© selecionada como a classe final."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#svm-para-classifica√ß√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#svm-para-classifica√ß√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "SVM para classifica√ß√£o no R e Python",
    "text": "SVM para classifica√ß√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris &lt;- iris %&gt;% filter(Species != \"setosa\") %&gt;% mutate(Species = factor(Species))\niris_split &lt;- initial_split(iris, strata = Species)\ntrain_data &lt;- training(iris_split)\ntest_data &lt;- testing(iris_split)\n\nsvm_model &lt;- svm_rbf() %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\")\n\nsvm_fit &lt;- workflow() %&gt;%\n  add_model(svm_model) %&gt;%\n  add_formula(Species ~ .) %&gt;%\n  fit(data = train_data)\n\nsp_predict &lt;- predict(svm_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n\n[1] 0.8461538\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data[iris.target != 0, :2]\ny = iris.target[iris.target != 0]\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\nsvm_model = SVC(kernel=\"rbf\")\nsvm_model.fit(X_train, y_train)\n\n\n\n\nSVC()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nSVCSVC()\n\n\n\ny_pred = svm_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.6666666666666666"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#svm-para-regress√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#svm-para-regress√£o",
    "title": "An√°lise Preditiva",
    "section": "SVM para Regress√£o",
    "text": "SVM para Regress√£o\n\n\nTamb√©m pode ser usado para problemas de regress√£o, chamado de SVR (Support Vector Regression).\n\n\n\nBusca encontrar uma fun√ß√£o que tenha no m√°ximo \\(\\epsilon\\) de desvio dos pontos de treino, mantendo a maior margem poss√≠vel.\n\n\n\n\n√â sens√≠vel √† escolha do kernel e dos hiperpar√¢metros (ex: \\(C\\), \\(\\epsilon\\) )."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#svr-para-regress√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#svr-para-regress√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "SVR para regress√£o no R e Python",
    "text": "SVR para regress√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split &lt;- initial_split(mtcars, strata = mpg)\ntrain_data &lt;- training(mtcars_split)\ntest_data &lt;- testing(mtcars_split)\n\nsvr_reg &lt;- svm_rbf() %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"regression\")\n\nsvr_reg_fit &lt;- workflow() %&gt;%\n  add_model(svr_reg) %&gt;%\n  add_formula(mpg ~ .) %&gt;%\n  fit(data = train_data)\n\nmpg_predict &lt;- predict(svr_reg_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n\n[1] 8.124458\n\n\n\n\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\nsvr_reg = SVR(kernel='rbf', C=100, epsilon=0.1)\nsvr_reg.fit(X_train, y_train)\n\n\n\n\nSVR(C=100)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nSVRSVR(C=100)\n\n\n\ny_pred = svr_reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n\n0.6697473584156818"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#principais-hiperpar√¢metros-do-svm",
    "href": "aulas/Analise_preditiva/preditiva.html#principais-hiperpar√¢metros-do-svm",
    "title": "An√°lise Preditiva",
    "section": "Principais Hiperpar√¢metros do SVM",
    "text": "Principais Hiperpar√¢metros do SVM\n\nüîß C (Custo da penaliza√ß√£o)\n\nControla o equil√≠brio entre margem ampla e erros de classifica√ß√£o.\nValores baixos ‚Üí margem maior, mais toler√¢ncia ao erro.\n\nValores altos ‚Üí menos erros, margem mais estreita (pode causar overfitting).\n\n\n\n\n\nüåê Kernel\n\nFun√ß√£o que transforma os dados para um espa√ßo de maior dimens√£o:\n\n\n\"linear\": separa√ß√£o linear.\n\n\"rbf\" (radial basis function): n√£o linear, padr√£o.\n\n\"poly\": polinomial, √∫til para intera√ß√µes complexas."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#principais-hiperpar√¢metros-do-svm-1",
    "href": "aulas/Analise_preditiva/preditiva.html#principais-hiperpar√¢metros-do-svm-1",
    "title": "An√°lise Preditiva",
    "section": "Principais Hiperpar√¢metros do SVM",
    "text": "Principais Hiperpar√¢metros do SVM\n\n‚öôÔ∏è Gamma (usado com RBF e poly)\n\nDefine a influ√™ncia de um ponto de treino:\n\nValores altos ‚Üí influ√™ncia local ‚Üí mais complexidade.\nValores baixos ‚Üí influ√™ncia global ‚Üí modelo mais suave.\n\n\n\n\n\n\n\nüìè Epsilon (para regress√£o - SVR)\n\nDefine a zona de toler√¢ncia onde nenhuma penaliza√ß√£o √© aplicada.\nControla a sensibilidade do modelo a pequenas varia√ß√µes nos dados."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#pr√©-processamento-para-svm",
    "href": "aulas/Analise_preditiva/preditiva.html#pr√©-processamento-para-svm",
    "title": "An√°lise Preditiva",
    "section": "Pr√©-processamento para SVM",
    "text": "Pr√©-processamento para SVM\n\n\nO SVM √© sens√≠vel √† escala das vari√°veis:\n\nPadroniza√ß√£o (ex: StandardScaler no Python) √© altamente recomendada.\n\nEm tidymodels, use step_normalize() no recipe.\nRemover outliers pode ajudar na estabilidade da margem."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-8",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-8",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\nüëç\n\n\n\n\nAlta capacidade de generaliza√ß√£o\nFunciona bem em dados com muitas vari√°veis\nEficiente em fronteiras n√£o lineares com kernels\n\n\n\n\n\nüëé\n\n\n\n\nPode ser lento com bases muito grandes\nEscolha do kernel e par√¢metros pode exigir tuning intenso\nPouco interpret√°vel (modelo de ‚Äúcaixa-preta‚Äù)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#quando-usar-svm",
    "href": "aulas/Analise_preditiva/preditiva.html#quando-usar-svm",
    "title": "An√°lise Preditiva",
    "section": "Quando usar SVM?",
    "text": "Quando usar SVM?\n\n\nQuando h√° fronteiras complexas de decis√£o\n\n\n\nQuando se quer evitar overfitting em problemas com poucas amostras\n\n\n\n\nQuando outras abordagens lineares falharem\n\n\n\n\nEm tarefas onde o tempo de predi√ß√£o n√£o √© um gargalo cr√≠tico"
  },
  {
    "objectID": "analise_dados/data_science.html",
    "href": "analise_dados/data_science.html",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "",
    "text": "Voc√™ j√° se deparou com um conjunto de dados e se perguntou por onde come√ßar a extrair informa√ß√µes valiosas? A An√°lise Explorat√≥ria de Dados (EDA) √© a resposta para essa pergunta! Nesta an√°lise, apresentaremos um guia passo a passo que o ajudar√° a realizar uma EDA eficiente e obter insights significativos dos seus dados.\nPara isso, vamos utilizar a base de dados Data Science Salaries 2023 do Kaggle. Essa base de dados cont√©m informa√ß√µes sobre os sal√°rios de profissionais de ci√™ncia de dados em diferentes regi√µes e setores."
  },
  {
    "objectID": "analise_dados/data_science.html#sobre-os-dados",
    "href": "analise_dados/data_science.html#sobre-os-dados",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Sobre os dados",
    "text": "Sobre os dados\nO conjunto de dados refere-se aos sal√°rios de diferentes profissionais de ci√™ncia de dados, mensurados nas seguintes caracter√≠sticas:\n\nwork_year: O ano em que o sal√°rio foi pago.\nexperience_level: O n√≠vel de experi√™ncia no trabalho durante o ano.\nemployment_type: O tipo de emprego para a fun√ß√£o.\njob_title: O cargo exercido durante o ano.\nsalary: O valor total do sal√°rio bruto pago.\nsalary_currency: A moeda do sal√°rio pago, representada por um c√≥digo ISO 4217.\nsalary_in_usd: O sal√°rio em USD (d√≥lares americanos).\nemployee_residence: O pa√≠s de resid√™ncia principal do funcion√°rio durante o ano de trabalho, representado por um c√≥digo ISO 3166.\nremote_ratio: A quantidade geral de trabalho realizado remotamente.\ncompany_location: O pa√≠s da sede da empresa ou filial contratante.\ncompany_size: O n√∫mero m√©dio de pessoas que trabalharam para a empresa durante o ano."
  },
  {
    "objectID": "analise_dados/data_science.html#pacotes-utilizados",
    "href": "analise_dados/data_science.html#pacotes-utilizados",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Pacotes utilizados",
    "text": "Pacotes utilizados\n\npacman::p_load(\n  rio, here, skimr, tidyverse, gtsummary, rstatix, janitor, scales, flextable, descriptr, treemapify, ggpubr, funModeling, ggalluvial, vip, VIM, patchwork, monochromeR, leaflet, sf, rnaturalearth, rnaturalearthdata\n  )"
  },
  {
    "objectID": "analise_dados/data_science.html#leitura-dos-dados",
    "href": "analise_dados/data_science.html#leitura-dos-dados",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Leitura dos dados",
    "text": "Leitura dos dados\n\ndados = rio::import(\"https://raw.githubusercontent.com/tiagomartin/est125/refs/heads/main/dados/ds_salaries.csv\")\ndados %&gt;% \n  glimpse()\n\nRows: 3,755\nColumns: 11\n$ work_year          &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 202‚Ä¶\n$ experience_level   &lt;chr&gt; \"SE\", \"MI\", \"MI\", \"SE\", \"SE\", \"SE\", \"SE\", \"SE\", \"SE‚Ä¶\n$ employment_type    &lt;chr&gt; \"FT\", \"CT\", \"CT\", \"FT\", \"FT\", \"FT\", \"FT\", \"FT\", \"FT‚Ä¶\n$ job_title          &lt;chr&gt; \"Principal Data Scientist\", \"ML Engineer\", \"ML Engi‚Ä¶\n$ salary             &lt;int&gt; 80000, 30000, 25500, 175000, 120000, 222200, 136000‚Ä¶\n$ salary_currency    &lt;chr&gt; \"EUR\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"U‚Ä¶\n$ salary_in_usd      &lt;int&gt; 85847, 30000, 25500, 175000, 120000, 222200, 136000‚Ä¶\n$ employee_residence &lt;chr&gt; \"ES\", \"US\", \"US\", \"CA\", \"CA\", \"US\", \"US\", \"CA\", \"CA‚Ä¶\n$ remote_ratio       &lt;int&gt; 100, 100, 100, 100, 100, 0, 0, 0, 0, 0, 0, 100, 100‚Ä¶\n$ company_location   &lt;chr&gt; \"ES\", \"US\", \"US\", \"CA\", \"CA\", \"US\", \"US\", \"CA\", \"CA‚Ä¶\n$ company_size       &lt;chr&gt; \"L\", \"S\", \"S\", \"M\", \"M\", \"L\", \"L\", \"M\", \"M\", \"M\", \"‚Ä¶\n\n\nO primeiro passo √© entender qual √© o objetivo da nossa an√°lise. Neste caso, nosso objetivo pode ser identificar os principais fatores que influenciam os sal√°rios de profissionais de ci√™ncia de dados e explorar as tend√™ncias salariais em diferentes regi√µes e setores. Tentaremos responder √†s seguintes perguntas:\n\nQual a dimens√£o da base de dados?\nQuando foram pagos os sal√°rios?\nQual o n√≠vel de experi√™ncia da maioria dos profissionais?\nQuais os tipos de contrato de trabalho?\nComo est√° a distribui√ß√£o dos sal√°rios no mercado?\nQuais as modalidades de trabalho?\nQual o tamanho das empresas?\nonde as empresas est√£o alocadas?\nOnde se pagam os maiores sal√°rios m√©dios?\nOnde os empregados vivem?\nQuais s√£o os principais cargos de trabalho?\nQual a faixa salarial de acordo com o cargo?\nExiste alguma rela√ß√£o entre o tamanho da empresa e a faixa salarial?\nExiste alguma rela√ß√£o entre o n√≠vel de experi√™ncia e a faixa salarial?\nQual o valor que separa os 10% maiores salarios recebidos e qual a rela√ß√£o com o n√≠vel de experi√™ncia do profissional?\nQual a rela√ß√£o entre os maiores sal√°rios com o tipo de contrato do profissional?\nQual a rela√ß√£o entre os maiores sal√°rios com modalidade de trabalho do profissional?"
  },
  {
    "objectID": "analise_dados/data_science.html#qual-a-dimens√£o-da-base-de-dados",
    "href": "analise_dados/data_science.html#qual-a-dimens√£o-da-base-de-dados",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Qual a dimens√£o da base de dados?",
    "text": "Qual a dimens√£o da base de dados?\n\ndados %&gt;% \n  dim()\n\n[1] 3755   11\n\n\nNote que a base de dados possui 3755 registros mensurados nas 11 vari√°veis."
  },
  {
    "objectID": "analise_dados/data_science.html#quando-foram-pagos-os-sal√°rios",
    "href": "analise_dados/data_science.html#quando-foram-pagos-os-sal√°rios",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Quando foram pagos os sal√°rios?",
    "text": "Quando foram pagos os sal√°rios?\n\ndados %&gt;% \n  ggplot((aes(x=work_year))) +\n  geom_bar(stat = 'count', fill= 'steelblue')+\n  xlab(\"Ano\") +\n  ylab(\"Frequ√™ncia Absoluta\") +\n  stat_count(geom = \"text\", aes(label = after_stat(count)), vjust = -0.5) +\n  theme_pubclean()\n\n\n\n\n\n\n\n\nPerceba que a grande maioria dos sal√°rios foram pagos nos anos de 2022 e 2023."
  },
  {
    "objectID": "analise_dados/data_science.html#qual-o-n√≠vel-de-experi√™ncia-da-maioria-dos-profissionais",
    "href": "analise_dados/data_science.html#qual-o-n√≠vel-de-experi√™ncia-da-maioria-dos-profissionais",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Qual o n√≠vel de experi√™ncia da maioria dos profissionais?",
    "text": "Qual o n√≠vel de experi√™ncia da maioria dos profissionais?\n\ndados %&gt;% \n  ggplot((aes(x=experience_level))) +\n  geom_bar(stat = 'count', fill= 'steelblue')+\n  xlab(\"N√≠vel de experi√™ncia\") +\n  ylab(\"Frequ√™ncia Absoluta\") +\n  stat_count(geom = \"text\", aes(label = after_stat(count)), vjust = -0.5) +\n  theme_pubclean()\n\n\n\n\n\n\n\n\nO conjunto de dados cont√©m 4 n√≠veis de experi√™ncia, dos quais a categoria de n√≠vel s√™nior (SE) tem a representa√ß√£o mais alta, seguida pela categoria pleno (MI), a categoria j√∫nior (EN) √© a pr√≥xima na linha e a categoria de n√≠vel executivo (EX) tem a representa√ß√£o mais baixa."
  },
  {
    "objectID": "analise_dados/data_science.html#quais-os-tipos-de-contrato-de-trabalho",
    "href": "analise_dados/data_science.html#quais-os-tipos-de-contrato-de-trabalho",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Quais os tipos de contrato de trabalho?",
    "text": "Quais os tipos de contrato de trabalho?\n\ndados %&gt;% \n  ggplot((aes(x=employment_type))) +\n  geom_bar(stat = 'count', fill= 'steelblue')+\n  xlab(\"Modalidade\") +\n  ylab(\"Frequ√™ncia Absoluta\") +\n  stat_count(geom = \"text\", aes(label = after_stat(count)), vjust = -0.5) +\n  theme_pubclean()\n\n\n\n\n\n\n\n\nO conjunto de dados cont√©m 4 tipos contrato de trabalho: Contrato (CT), Freelance (FL), Full-Time (FT) e Part-Time (PT). √â evidente a partir dos gr√°ficos que a maioria das pessoas trabalha em regime de tempo integral."
  },
  {
    "objectID": "analise_dados/data_science.html#como-est√°-a-distribui√ß√£o-dos-sal√°rios-no-mercado",
    "href": "analise_dados/data_science.html#como-est√°-a-distribui√ß√£o-dos-sal√°rios-no-mercado",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Como est√° a distribui√ß√£o dos sal√°rios no mercado?",
    "text": "Como est√° a distribui√ß√£o dos sal√°rios no mercado?\n\ndados %&gt;% \n  select(salary_in_usd) %&gt;% \n  summary()\n\n salary_in_usd   \n Min.   :  5132  \n 1st Qu.: 95000  \n Median :135000  \n Mean   :137570  \n 3rd Qu.:175000  \n Max.   :450000  \n\n\n\nquantile(dados$salary_in_usd)\n\n    0%    25%    50%    75%   100% \n  5132  95000 135000 175000 450000 \n\n\n\ndados %&gt;%\n      mutate(\n          salary_range = cut_number(salary_in_usd, 4, labels = c(\"low\", \"medium\", \"high\", \"very high\"))\n      ) %&gt;%\n      group_by(salary_range) %&gt;%\n      summarise(\n          `Qtd de observa√ß√µes` = n(),\n          `Vlr Min` = min(salary_in_usd),\n          `Vlr Max` = max(salary_in_usd)\n      ) \n\n# A tibble: 4 √ó 4\n  salary_range `Qtd de observa√ß√µes` `Vlr Min` `Vlr Max`\n  &lt;fct&gt;                       &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n1 low                           956      5132     95000\n2 medium                        967     95386    135000\n3 high                          900    135446    175000\n4 very high                     932    175100    450000\n\n\n\np = dados %&gt;% \n    ggplot((aes(x=salary_in_usd))) +\n    geom_histogram(color = '#0F4B63', fill ='steelblue', bins=30) +\n    labs(title = \"Histograma de Sal√°rio ($)\",x = \"Sal√°rio\",y = \"Frequ√™ncia Absoluta\") +\n    theme_pubclean()\n\np + geom_vline(xintercept = quantile(dados$salary_in_usd), colour=\"red\", linewidth = 0.6, linetype = 2)\n\n\n\n\n\n\n\n\nA faixa salarial est√° entre 5132,00 d√≥lares e 450000,00 d√≥lares. O sal√°rio m√©dio dos profissionais de Data Science √© de aproximadamente 137570,00 d√≥lares. Considerando os quartis, podemos criar uma vari√°vel chamada salary_range com as faixas salariais, sendo que aproximadamente 25% dos empregados (956 observa√ß√µes) recebem entre 5132,00 e 9500,00 d√≥lares (low), aproximadamente 25% (967 observa√ß√µes) recebem entre 95386,00 e 135000,00 d√≥lares (medium). Aproximadamente 25% (900 observa√ß√µes) recebem entre 135446,00 e 175000,00 d√≥lares (high) e finalmente, aproximadamente 25% (932 observa√ß√µes) recebem entre 175100,00 e 450000,00 d√≥lares (very high)."
  },
  {
    "objectID": "analise_dados/data_science.html#quais-s√£o-os-principais-cargos-de-trabalho",
    "href": "analise_dados/data_science.html#quais-s√£o-os-principais-cargos-de-trabalho",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Quais s√£o os principais cargos de trabalho?",
    "text": "Quais s√£o os principais cargos de trabalho?\n\ndados %&gt;% \n  select(job_title) %&gt;% \n  group_by(job_title) %&gt;% \n  summarise(N = n()) %&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3)) %&gt;% \n  arrange(desc(Porcentagem))\n\n# A tibble: 93 √ó 3\n   job_title                     N Porcentagem\n   &lt;chr&gt;                     &lt;int&gt;       &lt;dbl&gt;\n 1 Data Engineer              1040       0.277\n 2 Data Scientist              840       0.224\n 3 Data Analyst                612       0.163\n 4 Machine Learning Engineer   289       0.077\n 5 Analytics Engineer          103       0.027\n 6 Data Architect              101       0.027\n 7 Research Scientist           82       0.022\n 8 Applied Scientist            58       0.015\n 9 Data Science Manager         58       0.015\n10 Research Engineer            37       0.01 \n# ‚Ñπ 83 more rows\n\n\n\ndados %&gt;% \n  select(job_title) %&gt;% \n  group_by(job_title) %&gt;% \n  summarise(N = n()) %&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3)) %&gt;% \n  ggplot(aes(area = N, fill = Porcentagem, label = paste(job_title, paste0(Porcentagem*100, '%'), sep = \"\\n\"))) +\n  geom_treemap()+\n  geom_treemap_text(colour = \"white\",\n                    place = \"centre\",\n                    size = 15) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nO conjunto de dados mostra que 27,7% (1040 observa√ß√µes) do total de empregados s√£o lotados como Data Engineer, 22,4% (840 observa√ß√µes) s√£o lotados como Data Scientist, 16,3% (612 observa√ß√µes) s√£o lotados como Data Analyst e 7,7% (289 observa√ß√µes) s√£o lotados como Machine Learning Engineer. Outros cargos aparecem na base de dados, mas com porcentagem menores."
  },
  {
    "objectID": "analise_dados/data_science.html#qual-a-faixa-salarial-de-acordo-com-o-cargo",
    "href": "analise_dados/data_science.html#qual-a-faixa-salarial-de-acordo-com-o-cargo",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Qual a faixa salarial de acordo com o cargo?",
    "text": "Qual a faixa salarial de acordo com o cargo?\nExistem muitos cargos citados na base de dados. Vamos visualizar apenas as mais citadas e sua rela√ß√£o com as faixas salariais.\n\ndados = dados %&gt;% \n  mutate(top10Salary = as.factor(ifelse(salary_in_usd &gt; quantile(dados$salary_in_usd, probs = 0.9), 'yes', 'no')),\n         salary_range = cut_number(salary_in_usd, 4, \n                                   labels = c(\"low\", \"medium\", \"high\", \"very high\")))\n\n\njob_freq = dados %&gt;% \n  group_by(job_title) %&gt;% \n  dplyr::summarise(N=n()) %&gt;% \n  arrange(desc(N)) %&gt;% \n  filter(N &gt; 20)\n\n\ndadosT = dados %&gt;% \n  inner_join(job_freq) %&gt;% \n  group_by(job_title, salary_range) %&gt;% \n  dplyr::summarise(N = n()) %&gt;% \n  arrange(salary_range, N)\n\n## Definir um n√∫mero de 'barras vazias' para adicionar no final de cada grupo\nempty_bar &lt;- 2\nto_add &lt;- data.frame( matrix(NA, empty_bar*nlevels(dadosT$salary_range), ncol(dadosT)) )\ncolnames(to_add) &lt;- colnames(dadosT)\nto_add$salary_range &lt;- rep(levels(dadosT$salary_range), each=empty_bar)\ndadosT &lt;- rbind(dadosT, to_add)\ndadosT &lt;- dadosT %&gt;% arrange(salary_range)\ndadosT$id &lt;- seq(1, nrow(dadosT))\n\n## Obter o nome e a posi√ß√£o y de cada r√≥tulo\nlabel_dadosT &lt;- dadosT\nnumber_of_bar &lt;- nrow(label_dadosT)\nangle &lt;- 90 - 360 * (label_dadosT$id-0.5) /number_of_bar     \nlabel_dadosT$hjust &lt;- ifelse( angle &lt; -90, 1, 0)\nlabel_dadosT$angle &lt;- ifelse(angle &lt; -90, angle+180, angle)\n\n## Gr√°fico\np &lt;- ggplot(dadosT, aes(x=as.factor(id), y=N, fill=salary_range)) +       \n  geom_bar(stat=\"identity\", alpha=0.5) +\n  ylim(-100,120) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid = element_blank(),\n    plot.margin = unit(rep(-1,4), \"cm\") \n  ) +\n  coord_polar() + \n  geom_text(data=label_dadosT, aes(x=id, y=N+10, label=job_title, hjust=hjust), color=\"black\", fontface=\"bold\",alpha=0.6, size=2.5, angle= label_dadosT$angle, inherit.aes = FALSE ) \np\n\n\n\n\n\n\n\n\nNote que, em todas as faixas salariais, o cargo mais frequente √© o de Machine Learning Engineer, exceto para quem ganha entre 135446,00 e 175000,00 d√≥lares (high) cujo cargo mais frequente √© Data Analyst."
  },
  {
    "objectID": "analise_dados/data_science.html#qual-o-tamanho-das-empresas",
    "href": "analise_dados/data_science.html#qual-o-tamanho-das-empresas",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Qual o tamanho das empresas?",
    "text": "Qual o tamanho das empresas?\n\ndados %&gt;% \n  group_by(company_size) %&gt;% \n  summarise(N = n()) %&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3)) %&gt;% \n  arrange(desc(Porcentagem))\n\n# A tibble: 3 √ó 3\n  company_size     N Porcentagem\n  &lt;chr&gt;        &lt;int&gt;       &lt;dbl&gt;\n1 M             3153       0.84 \n2 L              454       0.121\n3 S              148       0.039\n\n\n\ndados %&gt;% \n  ggplot((aes(x=company_size))) +\n  geom_bar(stat = 'count', fill= 'steelblue')+\n  xlab(\"Tamanho\") +\n  ylab(\"Frequ√™ncia Absoluta\") +\n  stat_count(geom = \"text\", aes(label = after_stat(count)), vjust = -0.5) +\n  theme_pubclean()\n\n\n\n\n\n\n\n\nO conjunto de dados mostra que 3,9% (148 observa√ß√µes) das empresas s√£o de pequeno porte (S), 84,0% (3153 observa√ß√µes) s√£o empresas de m√©dio porte (M) e 12,1% (454 observa√ß√µes) s√£o empresas de grande porte (L)."
  },
  {
    "objectID": "analise_dados/data_science.html#existe-alguma-rela√ß√£o-entre-o-tamanho-da-empresa-e-a-faixa-salarial",
    "href": "analise_dados/data_science.html#existe-alguma-rela√ß√£o-entre-o-tamanho-da-empresa-e-a-faixa-salarial",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Existe alguma rela√ß√£o entre o tamanho da empresa e a faixa salarial?",
    "text": "Existe alguma rela√ß√£o entre o tamanho da empresa e a faixa salarial?\n\ndados %&gt;% \n  group_by(company_size, salary_range) %&gt;% \n  dplyr::summarise(N=n())%&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3))\n\n# A tibble: 12 √ó 4\n# Groups:   company_size [3]\n   company_size salary_range     N Porcentagem\n   &lt;chr&gt;        &lt;fct&gt;        &lt;int&gt;       &lt;dbl&gt;\n 1 L            low            193       0.425\n 2 L            medium          98       0.216\n 3 L            high            68       0.15 \n 4 L            very high       95       0.209\n 5 M            low            658       0.209\n 6 M            medium         844       0.268\n 7 M            high           826       0.262\n 8 M            very high      825       0.262\n 9 S            low            105       0.709\n10 S            medium          25       0.169\n11 S            high             6       0.041\n12 S            very high       12       0.081\n\n\n\ndados %&gt;% \n  group_by(company_size, salary_range) %&gt;% \n  dplyr::summarise(qtd = n()) %&gt;% \n  ggplot(aes(axis1=company_size, axis2=salary_range, y=qtd, fill= company_size))+\n  geom_alluvium()+\n  geom_stratum()+\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)))+\n  scale_x_discrete(limits = c(\"company_size\", \"salary_range\"))+\n  theme_pubclean()\n\n\n\n\n\n\n\n\nPodemos observar que, para as empresas de grande porte (L), 42,5% (193 observa√ß√µes) dos sal√°rios pagos s√£o baixos, 21,6% (98 observa√ß√µes) dos sal√°rios pagos s√£o medianos, 15% (68 obseva√ß√µes) s√£o altos e 20,9% (95 observa√ß√µes) s√£o muito altos. J√° para as empresas de pequeno porte, a grande maioria (70,9 %) pagam sal√°rios considerados baixos. As empresas de porte m√©dio pagam os sal√°rios de forma bem distribu√≠da entre as quatro faixas salariais."
  },
  {
    "objectID": "analise_dados/data_science.html#existe-alguma-rela√ß√£o-entre-o-n√≠vel-de-experi√™ncia-e-a-faixa-salarial",
    "href": "analise_dados/data_science.html#existe-alguma-rela√ß√£o-entre-o-n√≠vel-de-experi√™ncia-e-a-faixa-salarial",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Existe alguma rela√ß√£o entre o n√≠vel de experi√™ncia e a faixa salarial?",
    "text": "Existe alguma rela√ß√£o entre o n√≠vel de experi√™ncia e a faixa salarial?\n\ndados %&gt;% \n  group_by(experience_level, salary_range) %&gt;% \n  summarise(N = n()) %&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3)) \n\n# A tibble: 16 √ó 4\n# Groups:   experience_level [4]\n   experience_level salary_range     N Porcentagem\n   &lt;chr&gt;            &lt;fct&gt;        &lt;int&gt;       &lt;dbl&gt;\n 1 EN               low            217       0.678\n 2 EN               medium          62       0.194\n 3 EN               high            26       0.081\n 4 EN               very high       15       0.047\n 5 EX               low              6       0.053\n 6 EX               medium          18       0.158\n 7 EX               high            24       0.211\n 8 EX               very high       66       0.579\n 9 MI               low            379       0.471\n10 MI               medium         229       0.284\n11 MI               high           133       0.165\n12 MI               very high       64       0.08 \n13 SE               low            354       0.141\n14 SE               medium         658       0.262\n15 SE               high           717       0.285\n16 SE               very high      787       0.313\n\n\n\ndados %&gt;% \n  group_by(experience_level, salary_range) %&gt;% \n  dplyr::summarise(qtd = n()) %&gt;% \n  ggplot(aes(axis1=experience_level, axis2=salary_range, y=qtd, fill= experience_level))+\n  geom_alluvium()+\n  geom_stratum()+\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)))+\n  scale_x_discrete(limits = c(\"experience_level\", \"salary_range\"))+\n  theme_pubclean()\n\n\n\n\n\n\n\n\nPodemos observar que a maioria dos empregados da categoria j√∫nior (EN) (67,8% ou 217 observa√ß√µes) recebem sal√°rios considerados baixos (low). O mesmo acontece com os empregados da categoria pleno (MI) (47,1% ou 379 observa√ß√µes). Para os n√≠veis s√™nior (SE) e executivo (EX), o padr√£o salarial se inverte. A maioria (31,3% e 57,9%, respectivamente) recebem sal√°rios considerados muito altos."
  },
  {
    "objectID": "analise_dados/data_science.html#quais-as-modalidades-de-trabalho",
    "href": "analise_dados/data_science.html#quais-as-modalidades-de-trabalho",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Quais as modalidades de trabalho?",
    "text": "Quais as modalidades de trabalho?\n\ndados %&gt;% \n  ggplot((aes(x=factor(remote_ratio, labels = c('Presencial', 'H√≠brido', 'Remoto'))))) +\n  geom_bar(stat = 'count', fill= 'steelblue')+\n  xlab(\"Modalidade\") +\n  ylab(\"Frequ√™ncia Absoluta\") +\n  stat_count(geom = \"text\", aes(label = after_stat(count)), vjust = -0.5) +\n  theme_pubclean()\n\n\n\n\n\n\n\n\n\ndados %&gt;%\n    mutate(\n        remote_ratio = factor(remote_ratio, labels = c('Presencial', 'H√≠brido', 'Remoto'))\n    ) %&gt;%\n    group_by(remote_ratio) %&gt;%\n    summarise(`Qtd de observa√ß√µes` = n()) %&gt;% \n    mutate(Porcentagem = round(`Qtd de observa√ß√µes` / sum(`Qtd de observa√ß√µes`), 3)) %&gt;% \n    arrange(desc(Porcentagem))\n\n# A tibble: 3 √ó 3\n  remote_ratio `Qtd de observa√ß√µes` Porcentagem\n  &lt;fct&gt;                       &lt;int&gt;       &lt;dbl&gt;\n1 Presencial                   1923       0.512\n2 Remoto                       1643       0.438\n3 H√≠brido                       189       0.05 \n\n\nO conjunto de dados mostra que cerca de 51,2% (1.923 observa√ß√µes) est√£o trabalhando presencialmente, 43,8% (1.643 observa√ß√µes) est√£o trabalhando no modo h√≠brido e 5% (189 observa√ß√µes) est√£o trabalhando remotamente."
  },
  {
    "objectID": "analise_dados/data_science.html#onde-os-empregados-vivem",
    "href": "analise_dados/data_science.html#onde-os-empregados-vivem",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Onde os empregados vivem?",
    "text": "Onde os empregados vivem?\n\ndados_g = dados %&gt;% \n  group_by(employee_residence) %&gt;% \n  summarise(N = n()) %&gt;% \n  mutate(tooltip_text = paste(\"Total de empregados:\", N)) %&gt;%\n  select(employee_residence, N, tooltip_text) %&gt;% \n  ungroup()\n\n\ndados_g %&gt;%\n  select(employee_residence, N) %&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3)) %&gt;% \n  arrange(desc(Porcentagem))\n\n# A tibble: 78 √ó 3\n   employee_residence     N Porcentagem\n   &lt;chr&gt;              &lt;int&gt;       &lt;dbl&gt;\n 1 US                  3004       0.8  \n 2 GB                   167       0.044\n 3 CA                    85       0.023\n 4 ES                    80       0.021\n 5 IN                    71       0.019\n 6 DE                    48       0.013\n 7 FR                    38       0.01 \n 8 BR                    18       0.005\n 9 PT                    18       0.005\n10 GR                    16       0.004\n# ‚Ñπ 68 more rows\n\n\n\nworld_map &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmap_data_joined_interactive &lt;- world_map %&gt;%\n  left_join(dados_g, by = c(\"iso_a2\" = \"employee_residence\")) %&gt;%\n  mutate(\n    N_fill = ifelse(is.na(N), 0, N), # Valor 0 para pa√≠ses sem dados, para que apare√ßam na legenda se quiser\n    popup_content = ifelse(is.na(N),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", \"Dados n√£o dispon√≠veis\"),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", tooltip_text)\n                           )\n  )\n\npal &lt;- colorNumeric(\n  palette = \"Greens\", \n  domain = c(min(map_data_joined_interactive$N_fill, na.rm = TRUE),\n             max(map_data_joined_interactive$N_fill, na.rm = TRUE)),\n  na.color = \"gray95\" \n)\n\n# Criar o mapa interativo com Leaflet\nleaflet(map_data_joined_interactive) %&gt;%\n  addTiles() %&gt;% # Adiciona uma camada base (OpenStreetMap)\n  addPolygons(\n    fillColor = ~pal(N_fill), # Preenche os pa√≠ses com base na cor da paleta\n    weight = 1, # Espessura da borda do pol√≠gono\n    opacity = 1, # Opacidade da borda\n    color = \"white\", # Cor da borda\n    dashArray = \"3\", # Estilo da linha (tracejada)\n    fillOpacity = 0.7, # Transpar√™ncia do preenchimento\n    highlightOptions = highlightOptions( # Efeito ao passar o mouse\n      weight = 3,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.9,\n      bringToFront = TRUE),\n    popup = ~popup_content \n  ) %&gt;%\n  addLegend(pal = pal, values = ~N_fill, opacity = 0.7, title = \"N¬∫ de Empregados\",\n            position = \"bottomright\") \n\n\n\n\n\nPodemos observar que a grande maioria (80,0%) dos empregados moram nos Estados Unidos. Talvez seria interessante avaliar o gr√°fico acima fora dos Estados Unidos.\n\ndados_sUS = dados %&gt;% \n  filter(employee_residence != 'US') %&gt;% \n  group_by(employee_residence) %&gt;% \n  summarise(N = n()) %&gt;% \n  mutate(tooltip_text = paste(\"Total de empregados:\", N)) %&gt;%\n  select(employee_residence, N, tooltip_text) %&gt;% \n  ungroup()\n\n\ndados_sUS %&gt;%\n  select(employee_residence, N) %&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3)) %&gt;% \n  arrange(desc(Porcentagem))\n\n# A tibble: 77 √ó 3\n   employee_residence     N Porcentagem\n   &lt;chr&gt;              &lt;int&gt;       &lt;dbl&gt;\n 1 GB                   167       0.222\n 2 CA                    85       0.113\n 3 ES                    80       0.107\n 4 IN                    71       0.095\n 5 DE                    48       0.064\n 6 FR                    38       0.051\n 7 BR                    18       0.024\n 8 PT                    18       0.024\n 9 GR                    16       0.021\n10 NL                    15       0.02 \n# ‚Ñπ 67 more rows\n\n\n\nworld_map &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmap_data_joined_interactive &lt;- world_map %&gt;%\n  left_join(dados_sUS, by = c(\"iso_a2\" = \"employee_residence\")) %&gt;%\n  mutate(\n    N_fill = ifelse(is.na(N), 0, N), # Valor 0 para pa√≠ses sem dados, para que apare√ßam na legenda se quiser\n    popup_content = ifelse(is.na(N),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", \"Dados n√£o dispon√≠veis\"),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", tooltip_text)\n                           )\n  )\n\npal &lt;- colorNumeric(\n  palette = \"Greens\", \n  domain = c(min(map_data_joined_interactive$N_fill, na.rm = TRUE),\n             max(map_data_joined_interactive$N_fill, na.rm = TRUE)),\n  na.color = \"gray95\" \n)\n\n# Criar o mapa interativo com Leaflet\nleaflet(map_data_joined_interactive) %&gt;%\n  addTiles() %&gt;% # Adiciona uma camada base (OpenStreetMap)\n  addPolygons(\n    fillColor = ~pal(N_fill), # Preenche os pa√≠ses com base na cor da paleta\n    weight = 1, # Espessura da borda do pol√≠gono\n    opacity = 1, # Opacidade da borda\n    color = \"white\", # Cor da borda\n    dashArray = \"3\", # Estilo da linha (tracejada)\n    fillOpacity = 0.7, # Transpar√™ncia do preenchimento\n    highlightOptions = highlightOptions( # Efeito ao passar o mouse\n      weight = 3,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.9,\n      bringToFront = TRUE),\n    popup = ~popup_content \n  ) %&gt;%\n  addLegend(pal = pal, values = ~N_fill, opacity = 0.7, title = \"N¬∫ de Empregados\",\n            position = \"bottomright\") \n\n\n\n\n\nNote que, fora dos Estados Unidos, a maioria dos empregados vivem na Inglaterra (22,2%), Canad√° (11,3%), Espanha (10,7%), √çndia (9,5%), Alemanha (6,4%) e Fran√ßa (5,1%)."
  },
  {
    "objectID": "analise_dados/data_science.html#onde-as-empresas-est√£o-alocadas",
    "href": "analise_dados/data_science.html#onde-as-empresas-est√£o-alocadas",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Onde as empresas est√£o alocadas?",
    "text": "Onde as empresas est√£o alocadas?\n\ndados_c = dados %&gt;% \n  group_by(company_location) %&gt;% \n  summarise(N = n()) %&gt;% \n  mutate(tooltip_text = paste(\"Total de empresas:\", N)) %&gt;%\n  select(company_location, N, tooltip_text) %&gt;% \n  ungroup()\n\n\ndados_c %&gt;%\n  select(company_location, N) %&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3)) %&gt;% \n  arrange(desc(Porcentagem))\n\n# A tibble: 72 √ó 3\n   company_location     N Porcentagem\n   &lt;chr&gt;            &lt;int&gt;       &lt;dbl&gt;\n 1 US                3040       0.81 \n 2 GB                 172       0.046\n 3 CA                  87       0.023\n 4 ES                  77       0.021\n 5 DE                  56       0.015\n 6 IN                  58       0.015\n 7 FR                  34       0.009\n 8 AU                  14       0.004\n 9 BR                  15       0.004\n10 GR                  14       0.004\n# ‚Ñπ 62 more rows\n\n\n\nworld_map &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmap_data_joined_interactive &lt;- world_map %&gt;%\n  left_join(dados_c, by = c(\"iso_a2\" = \"company_location\")) %&gt;%\n  mutate(\n    N_fill = ifelse(is.na(N), 0, N), # Valor 0 para pa√≠ses sem dados, para que apare√ßam na legenda se quiser\n    popup_content = ifelse(is.na(N),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", \"Dados n√£o dispon√≠veis\"),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", tooltip_text)\n                           )\n  )\n\npal &lt;- colorNumeric(\n  palette = \"Greens\", \n  domain = c(min(map_data_joined_interactive$N_fill, na.rm = TRUE),\n             max(map_data_joined_interactive$N_fill, na.rm = TRUE)),\n  na.color = \"gray95\" \n)\n\n# Criar o mapa interativo com Leaflet\nleaflet(map_data_joined_interactive) %&gt;%\n  addTiles() %&gt;% # Adiciona uma camada base (OpenStreetMap)\n  addPolygons(\n    fillColor = ~pal(N_fill), # Preenche os pa√≠ses com base na cor da paleta\n    weight = 1, # Espessura da borda do pol√≠gono\n    opacity = 1, # Opacidade da borda\n    color = \"white\", # Cor da borda\n    dashArray = \"3\", # Estilo da linha (tracejada)\n    fillOpacity = 0.7, # Transpar√™ncia do preenchimento\n    highlightOptions = highlightOptions( # Efeito ao passar o mouse\n      weight = 3,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.9,\n      bringToFront = TRUE),\n    popup = ~popup_content \n  ) %&gt;%\n  addLegend(pal = pal, values = ~N_fill, opacity = 0.7, title = \"N¬∫ de Empresas\",\n            position = \"bottomright\") \n\n\n\n\n\nPodemos observar que a grande maioria (81,0%) das empresas est√£o situadas nos Estados Unidos. Talvez seria interessante avaliar o gr√°fico acima fora dos Estados Unidos.\n\ndados_sUS = dados %&gt;% \n  filter(company_location != 'US') %&gt;% \n  group_by(company_location) %&gt;% \n  summarise(N = n()) %&gt;% \n  mutate(tooltip_text = paste(\"Total de empresas:\", N)) %&gt;%\n  select(company_location, N, tooltip_text) %&gt;% \n  ungroup()\n\n\ndados_sUS %&gt;%\n  select(company_location, N) %&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3)) %&gt;% \n  arrange(desc(Porcentagem))\n\n# A tibble: 71 √ó 3\n   company_location     N Porcentagem\n   &lt;chr&gt;            &lt;int&gt;       &lt;dbl&gt;\n 1 GB                 172       0.241\n 2 CA                  87       0.122\n 3 ES                  77       0.108\n 4 IN                  58       0.081\n 5 DE                  56       0.078\n 6 FR                  34       0.048\n 7 BR                  15       0.021\n 8 AU                  14       0.02 \n 9 GR                  14       0.02 \n10 PT                  14       0.02 \n# ‚Ñπ 61 more rows\n\n\n\nworld_map &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmap_data_joined_interactive &lt;- world_map %&gt;%\n  left_join(dados_sUS, by = c(\"iso_a2\" = \"company_location\")) %&gt;%\n  mutate(\n    N_fill = ifelse(is.na(N), 0, N), # Valor 0 para pa√≠ses sem dados, para que apare√ßam na legenda se quiser\n    popup_content = ifelse(is.na(N),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", \"Dados n√£o dispon√≠veis\"),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", tooltip_text)\n                           )\n  )\n\npal &lt;- colorNumeric(\n  palette = \"Greens\", \n  domain = c(min(map_data_joined_interactive$N_fill, na.rm = TRUE),\n             max(map_data_joined_interactive$N_fill, na.rm = TRUE)),\n  na.color = \"gray95\" \n)\n\n# Criar o mapa interativo com Leaflet\nleaflet(map_data_joined_interactive) %&gt;%\n  addTiles() %&gt;% # Adiciona uma camada base (OpenStreetMap)\n  addPolygons(\n    fillColor = ~pal(N_fill), # Preenche os pa√≠ses com base na cor da paleta\n    weight = 1, # Espessura da borda do pol√≠gono\n    opacity = 1, # Opacidade da borda\n    color = \"white\", # Cor da borda\n    dashArray = \"3\", # Estilo da linha (tracejada)\n    fillOpacity = 0.7, # Transpar√™ncia do preenchimento\n    highlightOptions = highlightOptions( # Efeito ao passar o mouse\n      weight = 3,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.9,\n      bringToFront = TRUE),\n    popup = ~popup_content \n  ) %&gt;%\n  addLegend(pal = pal, values = ~N_fill, opacity = 0.7, title = \"N¬∫ de Empresas\",\n            position = \"bottomright\") \n\n\n\n\n\nNote que, fora dos Estados Unidos, a maioria das empresas est√£o situadas na Inglaterra (24,1%), Canad√° (12,2%), Espanha (10,8%), √çndia (8,1%), Alemanha (7,8%) e Fran√ßa (4,8%)."
  },
  {
    "objectID": "analise_dados/data_science.html#onde-se-pagam-os-maiores-sal√°rios-m√©dios",
    "href": "analise_dados/data_science.html#onde-se-pagam-os-maiores-sal√°rios-m√©dios",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Onde se pagam os maiores sal√°rios m√©dios?",
    "text": "Onde se pagam os maiores sal√°rios m√©dios?\n\ndf = dados %&gt;% \n  group_by(company_location) %&gt;% \n  dplyr::summarise(mean_salary = mean(salary_in_usd))\n\ndf %&gt;% arrange(desc(mean_salary))\n\n# A tibble: 72 √ó 2\n   company_location mean_salary\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 IL                   271446.\n 2 PR                   167500 \n 3 US                   151822.\n 4 RU                   140333.\n 5 CA                   131918.\n 6 NZ                   125000 \n 7 BA                   120000 \n 8 IE                   114943.\n 9 JP                   114127.\n10 SE                   105000 \n# ‚Ñπ 62 more rows\n\n\n\nworld_map &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmap_data_joined_interactive &lt;- world_map %&gt;%\n  left_join(df, by = c(\"iso_a2\" = \"company_location\")) %&gt;%\n  mutate(\n    N_fill = ifelse(is.na(mean_salary), 0, mean_salary), # Valor 0 para pa√≠ses sem dados, para que apare√ßam na legenda se quiser\n    popup_content = ifelse(is.na(mean_salary),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", \"Dados n√£o dispon√≠veis\"),\n                           paste0(\"&lt;b&gt;Pa√≠s:&lt;/b&gt; \", name, \"&lt;br/&gt;\", round(mean_salary, 2))\n                           )\n  )\n\npal &lt;- colorNumeric(\n  palette = \"Greens\", \n  domain = c(min(map_data_joined_interactive$N_fill, na.rm = TRUE),\n             max(map_data_joined_interactive$N_fill, na.rm = TRUE)),\n  na.color = \"gray95\" \n)\n\n# Criar o mapa interativo com Leaflet\nleaflet(map_data_joined_interactive) %&gt;%\n  addTiles() %&gt;% # Adiciona uma camada base (OpenStreetMap)\n  addPolygons(\n    fillColor = ~pal(N_fill), # Preenche os pa√≠ses com base na cor da paleta\n    weight = 1, # Espessura da borda do pol√≠gono\n    opacity = 1, # Opacidade da borda\n    color = \"white\", # Cor da borda\n    dashArray = \"3\", # Estilo da linha (tracejada)\n    fillOpacity = 0.7, # Transpar√™ncia do preenchimento\n    highlightOptions = highlightOptions( # Efeito ao passar o mouse\n      weight = 3,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.9,\n      bringToFront = TRUE),\n    popup = ~popup_content \n  ) %&gt;%\n  addLegend(pal = pal, values = ~N_fill, opacity = 0.7, title = \"M√©dia Salarial Anual\",\n            position = \"bottomright\") \n\n\n\n\n\nNote que o pa√≠s em que se paga o maior sal√°rio m√©dio (em m√©dia 271446,00 d√≥lares) para profissionais de Data Science √© Israel (IL), seguido por Porto Rico (PR) (em m√©dia 167500,00 d√≥lares) e Estados Unidos, que em m√©dia paga 151822,01 d√≥lares para esses profissionais. No Brasil, o sal√°rio m√©dio pago √© de 40579,20 d√≥lares."
  },
  {
    "objectID": "analise_dados/data_science.html#qual-o-valor-que-separa-os-10-maiores-salarios-recebidos-e-qual-a-rela√ß√£o-com-o-n√≠vel-de-experi√™ncia-do-profissional",
    "href": "analise_dados/data_science.html#qual-o-valor-que-separa-os-10-maiores-salarios-recebidos-e-qual-a-rela√ß√£o-com-o-n√≠vel-de-experi√™ncia-do-profissional",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Qual o valor que separa os 10% maiores salarios recebidos e qual a rela√ß√£o com o n√≠vel de experi√™ncia do profissional?",
    "text": "Qual o valor que separa os 10% maiores salarios recebidos e qual a rela√ß√£o com o n√≠vel de experi√™ncia do profissional?\n\ndados %&gt;% \n  summarise(top10 = quantile(dados$salary_in_usd, prob = 0.9))\n\n   top10\n1 219000\n\n\nPodemos observar que o sal√°rio que separa os 10% maiores sal√°rios √© de 219000,00 d√≥lares. Qual o n√≠vel de experi√™ncia dos empregados que ganham mais que esse valor?\n\ndados %&gt;% \n  group_by(experience_level,top10Salary) %&gt;% \n  dplyr::summarise(N=n())%&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3))\n\n# A tibble: 8 √ó 4\n# Groups:   experience_level [4]\n  experience_level top10Salary     N Porcentagem\n  &lt;chr&gt;            &lt;fct&gt;       &lt;int&gt;       &lt;dbl&gt;\n1 EN               no            315       0.984\n2 EN               yes             5       0.016\n3 EX               no             75       0.658\n4 EX               yes            39       0.342\n5 MI               no            784       0.974\n6 MI               yes            21       0.026\n7 SE               no           2206       0.877\n8 SE               yes           310       0.123\n\n\n\ndados %&gt;% \n  group_by(experience_level,top10Salary) %&gt;% \n  dplyr::summarise(N=n()) %&gt;% \n  ggplot(aes(fill=top10Salary, y=N, x=experience_level))+ \n  geom_bar(position=\"stack\", stat=\"identity\")+\n  xlab('N√≠vel de Experi√™ncia')+\n  ylab('Frequ√™ncia')+\n  theme_pubclean() \n\n\n\n\n\n\n\n\nObserve que, para a categoria j√∫nior (EN), apenas 1,6% (5 observa√ß√µes) dos empregados que recebem sal√°rios na faixa dos 10% maiores. Para a categoria pleno (MI), 2,6% (21 observa√ß√µes) recebem sal√°rios nessa faixa, enquanto que, para as categorias s√™nior (SE) e executivo (EX), essa faixa salarial representa 12,3% e 34,2% dos sal√°rios recebidos, respectivamente."
  },
  {
    "objectID": "analise_dados/data_science.html#qual-a-rela√ß√£o-entre-os-maiores-sal√°rios-com-o-tipo-de-contrato-do-profissional",
    "href": "analise_dados/data_science.html#qual-a-rela√ß√£o-entre-os-maiores-sal√°rios-com-o-tipo-de-contrato-do-profissional",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Qual a rela√ß√£o entre os maiores sal√°rios com o tipo de contrato do profissional?",
    "text": "Qual a rela√ß√£o entre os maiores sal√°rios com o tipo de contrato do profissional?\n\ndados %&gt;% \n  group_by(employment_type,top10Salary) %&gt;% \n  dplyr::summarise(N=n())%&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3))\n\n# A tibble: 6 √ó 4\n# Groups:   employment_type [4]\n  employment_type top10Salary     N Porcentagem\n  &lt;chr&gt;           &lt;fct&gt;       &lt;int&gt;       &lt;dbl&gt;\n1 CT              no              8         0.8\n2 CT              yes             2         0.2\n3 FL              no             10         1  \n4 FT              no           3345         0.9\n5 FT              yes           373         0.1\n6 PT              no             17         1  \n\n\n\ndados %&gt;% \n  group_by(employment_type,top10Salary) %&gt;% \n  dplyr::summarise(N=n()) %&gt;% \n  ggplot(aes(fill=top10Salary, y=N, x=employment_type))+ \n  geom_bar(position=\"stack\", stat=\"identity\")+\n  xlab('Tipo de contrato de trabalho')+\n  ylab('Frequ√™ncia')+\n  theme_pubclean() \n\n\n\n\n\n\n\n\nObserve que, para o tipo de trabalho por contrato (CT), 20% dos empregados recebem sal√°rios na faixa dos 10% maiores. J√° para quem trabalha em regime de tempo integral (FT), 10% recebem sal√°rios nessa faixa. Para quem trabalha em regime de meio per√≠odo (PT) e Freelance (FL), nenhum funcion√°rio recebe sal√°rios na faixa dos 10% maiores."
  },
  {
    "objectID": "analise_dados/data_science.html#qual-a-rela√ß√£o-entre-os-maiores-sal√°rios-com-modalidade-de-trabalho-do-profissional",
    "href": "analise_dados/data_science.html#qual-a-rela√ß√£o-entre-os-maiores-sal√°rios-com-modalidade-de-trabalho-do-profissional",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Qual a rela√ß√£o entre os maiores sal√°rios com modalidade de trabalho do profissional?",
    "text": "Qual a rela√ß√£o entre os maiores sal√°rios com modalidade de trabalho do profissional?\n\ndados %&gt;% \n  group_by(remote_ratio,top10Salary) %&gt;% \n  dplyr::summarise(N=n())%&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3))\n\n# A tibble: 6 √ó 4\n# Groups:   remote_ratio [3]\n  remote_ratio top10Salary     N Porcentagem\n         &lt;int&gt; &lt;fct&gt;       &lt;int&gt;       &lt;dbl&gt;\n1            0 no           1698       0.883\n2            0 yes           225       0.117\n3           50 no            182       0.963\n4           50 yes             7       0.037\n5          100 no           1500       0.913\n6          100 yes           143       0.087\n\n\n\ndados %&gt;% \n  group_by(remote_ratio,top10Salary) %&gt;% \n  dplyr::summarise(N=n()) %&gt;% \n  ggplot(aes(fill=top10Salary, y=N, x= factor(remote_ratio, labels = c('Presencial', 'H√≠brido', 'Remoto'))))+ \n  geom_bar(position=\"stack\", stat=\"identity\")+\n  xlab('Regime de trabalho')+\n  ylab('Frequ√™ncia')+\n  theme_pubclean() \n\n\n\n\n\n\n\n\nAparentemente o regime de trabalho n√£o possui rela√ß√£o com o fato de se ganhar sal√°rios na faixa dos 10% maiores."
  },
  {
    "objectID": "analise_dados/data_science.html#qual-a-rela√ß√£o-entre-os-maiores-sal√°rios-com-o-tamanho-da-empresa",
    "href": "analise_dados/data_science.html#qual-a-rela√ß√£o-entre-os-maiores-sal√°rios-com-o-tamanho-da-empresa",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Qual a rela√ß√£o entre os maiores sal√°rios com o tamanho da empresa?",
    "text": "Qual a rela√ß√£o entre os maiores sal√°rios com o tamanho da empresa?\n\ndados %&gt;% \n  group_by(company_size,top10Salary) %&gt;% \n  dplyr::summarise(N=n())%&gt;% \n  mutate(Porcentagem = round(N / sum(N), 3))\n\n# A tibble: 6 √ó 4\n# Groups:   company_size [3]\n  company_size top10Salary     N Porcentagem\n  &lt;chr&gt;        &lt;fct&gt;       &lt;int&gt;       &lt;dbl&gt;\n1 L            no            411       0.905\n2 L            yes            43       0.095\n3 M            no           2825       0.896\n4 M            yes           328       0.104\n5 S            no            144       0.973\n6 S            yes             4       0.027\n\n\n\ndados %&gt;% \n  group_by(company_size,top10Salary) %&gt;% \n  dplyr::summarise(N=n()) %&gt;% \n  ggplot(aes(fill=top10Salary, y=N, x= company_size))+ \n  geom_bar(position=\"stack\", stat=\"identity\")+\n  xlab('Tamanho da empresa')+\n  ylab('Frequ√™ncia')+\n  theme_pubclean() \n\n\n\n\n\n\n\n\nObserve que a grande minoria dos funcion√°rios das empresas, independente do tamanho, recebem sal√°rios na faixa dos 10% maiores. Aparentemente o tamanho da empresa n√£o influencia no pagamento de sal√°rios nessa faixa."
  },
  {
    "objectID": "analise_dados/data_science.html#conclus√£o",
    "href": "analise_dados/data_science.html#conclus√£o",
    "title": "An√°lise de dados: Explorando a base de dados Data Science Salaries 2023 do Kaggle",
    "section": "Conclus√£o",
    "text": "Conclus√£o\nA √°rea de ci√™ncia de dados oferece uma gama diversificada de oportunidades de trabalho, com sal√°rios variados, dependendo de v√°rios fatores. Baseados na an√°lise explorat√≥ria de dados (EDA), temos as seguintes informa√ß√µes acerca dos sal√°rios de profissionais de Data Science em diferentes regi√µes:\n\nA maioria dos profissionais possuem n√≠vel de experi√™ncia s√™nior e trabalham em hor√°rio integral.\nA grande maioria dos profissionais ganham menos de 200000,00 d√≥lares. O sal√°rio m√©dio fica em torno de 137570,00 d√≥lares.\nOs cargos de Data Engineer, Data Scientist, Data Analyst e Machine Learning Engineer s√£o os mais frequentes no mercado de trabalho.\nA maioria das empresas s√£o de m√©dio porte. Al√©m disso, o tamanho da empresa nem sempre determina a faixa salarial, com empresas de m√©dio porte oferecendo sal√°rios competitivos para atrair trabalhadores qualificados.\nA experi√™ncia do profissional, aparentemente √© um fator importante na defini√ß√£o do sal√°rio. Profissionais da categoria s√™nior e executivo tendem a ganhar sal√°rios mais altos.\nA grande maioria das empresas est√£o situadas nos Estados Unidos. A grande maioria dos empregados residem neste pa√≠s. Por√©m, os maiores sal√°rios, em m√©dia, s√£o pagos por empresas localizadas em Israel e Porto Rico.\nOs 10% maiores sal√°rios s√£o pagos, em sua maioria, a profissionais das categorias s√™nior e executivo que trabalham por contrato ou em tempo integral.\nAparentemente, o regime de trabalho (presencial, remoto ou h√≠brido) n√£o impacta significativamente na determina√ß√£o do sal√°rio.\n\nCom o uso de t√©cnicas de EDA, tivemos descobertas que enfatizam a import√¢ncia de alguns fatores relacionados ao trabalho na determina√ß√£o da remunera√ß√£o e que podem orientar aspirantes a profissionais de dados na negocia√ß√£o de pacotes salariais melhores."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Disciplina oferecida em car√°ter eletivo no semestre 2025/1"
  },
  {
    "objectID": "analises.html",
    "href": "analises.html",
    "title": "An√°lise de Dados",
    "section": "",
    "text": "EDA: Sal√°rios em Ci√™ncia de Dados\nPr√©-processamento usando a base de dados Space Titanic",
    "crumbs": [
      "An√°lise de Dados"
    ]
  },
  {
    "objectID": "analise_dados/space_titanic.html",
    "href": "analise_dados/space_titanic.html",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "",
    "text": "Bem-vindo ao ano de 2912, onde suas habilidades em ci√™ncia de dados s√£o necess√°rias para resolver um mist√©rio c√≥smico. Recebemos uma transmiss√£o de quatro anos-luz de dist√¢ncia e as coisas n√£o parecem boas.\nA espa√ßonave Titanic foi um transatl√¢ntico interestelar lan√ßado h√° um m√™s. Com quase 13.000 passageiros a bordo, a embarca√ß√£o partiu em sua viagem inaugural transportando emigrantes de nosso sistema solar para tr√™s exoplanetas rec√©m-habit√°veis orbitando estrelas pr√≥ximas.\nAo contornar Alpha Centauri a caminho de seu primeiro destino - o t√≥rrido 55 Cancri E - a incauta nave espacial Titanic colidiu com uma anomalia do espa√ßo-tempo escondida dentro de uma nuvem de poeira. Infelizmente, ele teve um destino semelhante ao de seu hom√¥nimo de 1000 anos antes. Embora a nave tenha permanecido intacta, quase metade dos passageiros foram transportados para uma dimens√£o alternativa!\nPara ajudar as equipes de resgate e recuperar os passageiros perdidos, voc√™ √© desafiado a prever quais passageiros foram transportados pela anomalia usando registros recuperados do sistema de computador danificado da espa√ßonave.\nAjude a salv√°-los e mude a hist√≥ria!\nO conjunto de dados ‚ÄúSpace Titanic‚Äù dispon√≠vel no Kaggle √© uma cole√ß√£o de informa√ß√µes sobre uma hipot√©tica trag√©dia espacial em uma nave chamada ‚ÄúStarship Titanic‚Äù e sua tarefa √© prever se um passageiro foi transportado para uma dimens√£o alternativa durante a colis√£o da espa√ßonave com a anomalia do espa√ßo-tempo. Para ajud√°-lo a fazer essas previs√µes, voc√™ recebe um conjunto de registros pessoais recuperados do sistema de computador danificado da nave."
  },
  {
    "objectID": "analise_dados/space_titanic.html#sobre-os-dados",
    "href": "analise_dados/space_titanic.html#sobre-os-dados",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Sobre os dados",
    "text": "Sobre os dados\nO conjunto de dados refere-se aos registros pessoais de cerca de 8700 passageiros, mensurados nas seguintes caracter√≠sticas:\n\nPassengerId - Um ID exclusivo para cada passageiro. Cada Id assume a forma gggg_pp onde gggg indica um grupo com o qual o passageiro est√° viajando e pp √© seu n√∫mero dentro do grupo. As pessoas em um grupo geralmente s√£o membros da fam√≠lia, mas nem sempre.\nHomePlanet - O planeta do qual o passageiro partiu, normalmente seu planeta de resid√™ncia permanente.\nCryoSleep - Indica se o passageiro optou por ser colocado em anima√ß√£o suspensa durante a viagem. Os passageiros em sono criog√™nico est√£o confinados em suas cabines.\nCabine - O n√∫mero da cabine onde o passageiro est√° hospedado. Assume a forma deck/num/side, onde side pode ser P para bombordo ou S para estibordo.\nDestination - O planeta para o qual o passageiro ir√° desembarcar.\nIdade - A idade do passageiro.\nVIP - Se o passageiro pagou pelo servi√ßo VIP especial durante a viagem.\nRoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Valor que o passageiro pagou em cada uma das muitas comodidades de luxo do Titanic.\nNome - O nome e o sobrenome do passageiro.\nTransported - Se o passageiro foi transportado para outra dimens√£o. Este √© o alvo, a coluna que voc√™ est√° tentando prever.\n\nApesar desse conjunto de dados destinar-se ao aprendizado de m√°quina supervisionado, faremos, neste script, apenas a parte de pr√©-processamento, incluindo algumas an√°lises explorat√≥rias. Tentaremos responder √†s seguintes quest√µes:\n\nDe onde vieram os passageiros e qual a rela√ß√£o disso com ser transportado ou n√£o?\nEm que cabine estavam os passageiros e qual a rela√ß√£o disso com ser transportado ou n√£o?\nPara onde iriam os passageiros e qual a rela√ß√£o disso com ser transportado ou n√£o?\nQuem estava sozinho e quem estava em grupo?\nEstar em sono profundo era uma boa ideia?\nPassageiros VIP tem menor risco de ser transportado?\n\nQue fatores ajudaram algu√©m a n√£o ser transportado?"
  },
  {
    "objectID": "analise_dados/space_titanic.html#pacotes-utilizados",
    "href": "analise_dados/space_titanic.html#pacotes-utilizados",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Pacotes utilizados",
    "text": "Pacotes utilizados\n\nload &lt;- function(pkg){\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg))\n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n} \n\n## Pacotes utilizados nessa an√°lise\n\npackages &lt;- c(\"tidyverse\",'tidymodels', 'janitor', 'ggpubr', 'funModeling', 'ggalluvial', 'vip', 'skimr', 'VIM', 'patchwork', 'rio')\n\nload(packages)\n\n  tidyverse  tidymodels     janitor      ggpubr funModeling  ggalluvial \n       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE \n        vip       skimr         VIM   patchwork         rio \n       TRUE        TRUE        TRUE        TRUE        TRUE"
  },
  {
    "objectID": "analise_dados/space_titanic.html#leitura-dos-dados",
    "href": "analise_dados/space_titanic.html#leitura-dos-dados",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Leitura dos dados",
    "text": "Leitura dos dados\n\ndados = rio::import(\"https://raw.githubusercontent.com/tiagomartin/est125/refs/heads/main/dados/space_titanic.csv\")\ndados %&gt;% \n  glimpse()\n\nRows: 8,693\nColumns: 14\n$ PassengerId  &lt;chr&gt; \"0001_01\", \"0002_01\", \"0003_01\", \"0003_02\", \"0004_01\", \"0‚Ä¶\n$ HomePlanet   &lt;chr&gt; \"Europa\", \"Earth\", \"Europa\", \"Europa\", \"Earth\", \"Earth\", ‚Ä¶\n$ CryoSleep    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FA‚Ä¶\n$ Cabin        &lt;chr&gt; \"B/0/P\", \"F/0/S\", \"A/0/S\", \"A/0/S\", \"F/1/S\", \"F/0/P\", \"F/‚Ä¶\n$ Destination  &lt;chr&gt; \"TRAPPIST-1e\", \"TRAPPIST-1e\", \"TRAPPIST-1e\", \"TRAPPIST-1e‚Ä¶\n$ Age          &lt;dbl&gt; 39, 24, 58, 33, 16, 44, 26, 28, 35, 14, 34, 45, 32, 48, 2‚Ä¶\n$ VIP          &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FA‚Ä¶\n$ RoomService  &lt;dbl&gt; 0, 109, 43, 0, 303, 0, 42, 0, 0, 0, 0, 39, 73, 719, 8, 32‚Ä¶\n$ FoodCourt    &lt;dbl&gt; 0, 9, 3576, 1283, 70, 483, 1539, 0, 785, 0, 0, 7295, 0, 1‚Ä¶\n$ ShoppingMall &lt;dbl&gt; 0, 25, 0, 371, 151, 0, 3, 0, 17, 0, NA, 589, 1123, 65, 12‚Ä¶\n$ Spa          &lt;dbl&gt; 0, 549, 6715, 3329, 565, 291, 0, 0, 216, 0, 0, 110, 0, 0,‚Ä¶\n$ VRDeck       &lt;dbl&gt; 0, 44, 49, 193, 2, 0, 0, NA, 0, 0, 0, 124, 113, 24, 7, 0,‚Ä¶\n$ Name         &lt;chr&gt; \"Maham Ofracculy\", \"Juanna Vines\", \"Altark Susent\", \"Sola‚Ä¶\n$ Transported  &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n\n\n\ndados %&gt;% df_status()\n\n       variable q_zeros p_zeros q_na p_na q_inf p_inf      type unique\n1   PassengerId       0    0.00    0 0.00     0     0 character   8693\n2    HomePlanet       0    0.00    0 0.00     0     0 character      4\n3     CryoSleep    5439   62.57  217 2.50     0     0   logical      2\n4         Cabin       0    0.00    0 0.00     0     0 character   6561\n5   Destination       0    0.00    0 0.00     0     0 character      4\n6           Age     178    2.05  179 2.06     0     0   numeric     80\n7           VIP    8291   95.38  203 2.34     0     0   logical      2\n8   RoomService    5577   64.16  181 2.08     0     0   numeric   1273\n9     FoodCourt    5456   62.76  183 2.11     0     0   numeric   1507\n10 ShoppingMall    5587   64.27  208 2.39     0     0   numeric   1115\n11          Spa    5324   61.24  183 2.11     0     0   numeric   1327\n12       VRDeck    5495   63.21  188 2.16     0     0   numeric   1306\n13         Name       0    0.00    0 0.00     0     0 character   8474\n14  Transported    4315   49.64    0 0.00     0     0   logical      2\n\n\nPodemos observar que, com exce√ß√£o das vari√°veis PassengerId e Transported, todas as outras possuem dados ausentes, representando, em cada uma delas, aproximadamente 2,5% das observa√ß√µes."
  },
  {
    "objectID": "analise_dados/space_titanic.html#explorando-os-dados",
    "href": "analise_dados/space_titanic.html#explorando-os-dados",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Explorando os dados",
    "text": "Explorando os dados\nNa pr√≥pria apresenta√ß√£o da base de dados, na descri√ß√£o das vari√°veis, foi informado que o atributo PassengerId assume a forma gggg_pp onde gggg indica um grupo com o qual o passageiro est√° viajando e pp √© seu n√∫mero dentro do grupo. Dessa forma, vamos criar duas novas vari√°veis, PassGroup para o grupo e nGroup para o n√∫mero do passageiro no grupo.\n\ndados = dados %&gt;%\n  separate(PassengerId, c(\"PassGroup\", \"nGroup\"), sep = \"_\", remove = FALSE) %&gt;% \n  mutate(PassGroup = as.numeric(PassGroup))\n\nOutra vari√°vel que possui mais de uma informa√ß√£o embutida √© a vari√°vel Cabin. Vamos separ√°-la em CabinDeck, representando o deck da cabine, podendo assumir os valores A, B, C, D, E, F e T, e CabinSide, podendo assumir os valores P para bombordo e S para estibordo. Tamb√©m criaremos a vari√°vel CabinNum, que representa os n√∫meros das cabines.\n\ndados = dados %&gt;%\n  separate(Cabin, c(\"CabinDeck\", \"CabinNum\", \"CabinSide\"), sep = \"/\") %&gt;% \n  mutate(CabinDeck = as.factor(CabinDeck),\n         CabinSide = as.factor(CabinSide))\n\nA fun√ß√£o ggscatmat() √© uma fun√ß√£o do pacote GGally do R que cria uma matriz de gr√°ficos de dispers√£o para visualizar a rela√ß√£o entre v√°rias vari√°veis num√©ricas em um conjunto de dados. Essa fun√ß√£o √© √∫til para visualizar rapidamente as correla√ß√µes entre as vari√°veis, bem como para detectar padr√µes e tend√™ncias. No gr√°fico abaixo, ainda separamos os elementos de acordo com a sua classe, descrita pela vari√°vel Transported.\n\ndados %&gt;% \n  mutate(Transported = as.numeric(Transported)) %&gt;% \n  select_if(is.numeric) %&gt;% \n  GGally::ggscatmat(color = \"Transported\", corMethod = \"pearson\")+\n  theme_pubclean()\n\n\n\n\n\n\n\n\nNote que fica clara a necessidade de transforma√ß√µes para as vari√°veis RoomService, FoodCourt, ShoppingMall, Spa e VRDeck, devido √† sua distribui√ß√£o assim√©trica. Um fato interessante acontece com a vari√°vel Age:\n\n# Age \ndados %&gt;% \n  ggplot(aes(x=Age, fill = Transported))+\n  geom_density(alpha =0.5)+\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nObserve que, para passageiros com idade menor de 10 anos, aproximadamente, a probabilidade do passageiro ser transportado √© maior que n√£o ser transportado. Isso sugere, talvez, a cria√ß√£o de um atributo captando essa informa√ß√£o.\n\nDe onde vieram os passageiros e qual a rela√ß√£o disso com ser transportado ou n√£o?\n\ndados %&gt;% \n  ggplot(aes(x=HomePlanet))+\n  geom_bar(stat = \"count\", fill=\"steelblue\")\n\n\n\n\n\n\n\n\nNote que a maioria dos passageiros embarcaram na Terra (Earth). Al√©m disso, existem observa√ß√µes ausentes. Qual a rela√ß√£o da origem dos passageiros com o fato de ser transportado ou n√£o?\n\ndados %&gt;% \n  ggplot(aes(x=HomePlanet, fill = Transported))+\n  geom_bar(stat = \"count\")+\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nQuem veio da Terra tem menos chances de ser transportado?"
  },
  {
    "objectID": "analise_dados/space_titanic.html#em-que-cabine-estavam-os-passageiros-e-qual-a-rela√ß√£o-disso-com-ser-transportado-ou-n√£o",
    "href": "analise_dados/space_titanic.html#em-que-cabine-estavam-os-passageiros-e-qual-a-rela√ß√£o-disso-com-ser-transportado-ou-n√£o",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Em que cabine estavam os passageiros e qual a rela√ß√£o disso com ser transportado ou n√£o?",
    "text": "Em que cabine estavam os passageiros e qual a rela√ß√£o disso com ser transportado ou n√£o?\n\np1 = dados %&gt;% \n  ggplot((aes(x=CabinDeck))) +\n  geom_bar(stat = 'count', fill= 'steelblue')\n\np2 = dados %&gt;% \n  ggplot((aes(x=CabinSide))) +\n  geom_bar(stat = 'count', fill= 'steelblue')\n\np1 + p2\n\n\n\n\n\n\n\n\nA maioria dos passageiros estavam alocados nos decks F e G. Existem observa√ß√µes faltantes. Os passageiros foram alocados em ambos os lados (estibordo e bombordo) de forma semelhante. Qual a rela√ß√£o disso com ser transportado ou n√£o?\n\np1 = dados %&gt;% \n  ggplot((aes(x=CabinDeck, fill = Transported))) +\n  geom_bar(stat = 'count')\n\np2 = dados %&gt;% \n  ggplot((aes(x=CabinSide, fill = Transported))) +\n  geom_bar(stat = 'count')\n\np1 + p2 + plot_layout(guides = \"collect\") &\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nAparentemente, quem est√° alocando nos decks B e C tem mais chance de ser transportado. O mesmo acontece para quem est√° situado no estibordo."
  },
  {
    "objectID": "analise_dados/space_titanic.html#para-onde-iriam-os-passageiros-e-qual-a-rela√ß√£o-disso-com-ser-transportado-ou-n√£o",
    "href": "analise_dados/space_titanic.html#para-onde-iriam-os-passageiros-e-qual-a-rela√ß√£o-disso-com-ser-transportado-ou-n√£o",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Para onde iriam os passageiros e qual a rela√ß√£o disso com ser transportado ou n√£o?",
    "text": "Para onde iriam os passageiros e qual a rela√ß√£o disso com ser transportado ou n√£o?\n\ndados %&gt;% \n  ggplot((aes(x=Destination))) +\n  geom_bar(stat = 'count', fill= 'steelblue')\n\n\n\n\n\n\n\n\nA maioria dos passageiros tinham como destino o Planeta ‚ÄúTRAPPIST-1e‚Äù. Qual a rela√ß√£o disso com ser transportado ou n√£o?\n\ndados %&gt;% \n  ggplot((aes(x=Destination, fill= Transported))) +\n  geom_bar(stat = 'count')+\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nQuem iria para o Planeta ‚Äú55 Cancri e‚Äù teria mais chances de ser transportado?"
  },
  {
    "objectID": "analise_dados/space_titanic.html#quem-estava-sozinho-e-quem-estava-em-grupo",
    "href": "analise_dados/space_titanic.html#quem-estava-sozinho-e-quem-estava-em-grupo",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Quem estava sozinho e quem estava em grupo?",
    "text": "Quem estava sozinho e quem estava em grupo?\n\ndados %&gt;% add_count(PassGroup, name = \"GroupSize\") %&gt;% \n  mutate(IsAlone = ifelse(GroupSize==1, 'yes', 'no')) %&gt;% \n  ggplot(aes(x=IsAlone))+\n  geom_bar(stat = \"count\", fill= 'steelblue')\n\n\n\n\n\n\n\n\nA maioria dos passageiros estavam viajando sozinho. Viajar sozinho influencia no fato de ser transportado?\n\ndados %&gt;% add_count(PassGroup, name = \"GroupSize\") %&gt;% \n  mutate(IsAlone = ifelse(GroupSize==1, 'yes', 'no')) %&gt;% \n  ggplot(aes(x=IsAlone, fill= Transported))+\n  geom_bar(stat = \"count\")+\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nTalvez seja uma boa estar sozinho neste momento! üòÉ"
  },
  {
    "objectID": "analise_dados/space_titanic.html#estar-em-sono-profundo-era-uma-boa-ideia",
    "href": "analise_dados/space_titanic.html#estar-em-sono-profundo-era-uma-boa-ideia",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Estar em sono profundo era uma boa ideia?",
    "text": "Estar em sono profundo era uma boa ideia?\n\n\n\n\n\n\ndados %&gt;% \n  ggplot(aes(x=CryoSleep))+\n  geom_bar(stat = \"count\", fill= 'steelblue')\n\n\n\n\n\n\n\n\nA maioria dos passageiros n√£o estavam em sono profundo. Isso influenciou no fato de ser transportado?\n\ndados %&gt;% \n  ggplot(aes(x=CryoSleep, fill= Transported))+\n  geom_bar(stat = \"count\")+\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nEstar em sono profundo n√£o me pereceu ser uma boa ideia!"
  },
  {
    "objectID": "analise_dados/space_titanic.html#passageiros-vip-tem-menor-risco-de-ser-transportado",
    "href": "analise_dados/space_titanic.html#passageiros-vip-tem-menor-risco-de-ser-transportado",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Passageiros VIP tem menor risco de ser transportado?",
    "text": "Passageiros VIP tem menor risco de ser transportado?\n\ndados %&gt;% \n  ggplot(aes(x=VIP))+\n  geom_bar(stat = \"count\", fill= 'steelblue')\n\n\n\n\n\n\n\n\nA grande maioria dos passageiros n√£o eram VIPs. Isso influenciou no fato de ser transportado?\n\ndados %&gt;% \n  ggplot(aes(x=VIP, fill= Transported))+\n  geom_bar(stat = \"count\")+\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nO fato de ser VIP aparentemente n√£o possui rela√ß√£o com o fato de ser transportado!"
  },
  {
    "objectID": "analise_dados/space_titanic.html#que-fatores-ajudaram-algu√©m-a-n√£o-ser-transportado",
    "href": "analise_dados/space_titanic.html#que-fatores-ajudaram-algu√©m-a-n√£o-ser-transportado",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Que fatores ajudaram algu√©m a n√£o ser transportado?",
    "text": "Que fatores ajudaram algu√©m a n√£o ser transportado?\nVamos analisar os gastos dos passageiros.\n\n# RoomService \np1 = dados %&gt;% \n  mutate(RoomService = log10(RoomService)) %&gt;% \n  ggplot(aes(x=RoomService, fill = Transported))+\n  geom_density(alpha =0.5) \n\n# Spa \np2 = dados %&gt;% \n  mutate(Spa = log10(Spa)) %&gt;% \n  ggplot(aes(x=Spa, fill = Transported))+\n  geom_density(alpha =0.5) \n\n# VRDeck \np3 = dados %&gt;% \n  mutate(VRDeck = log10(VRDeck)) %&gt;% \n  ggplot(aes(x=VRDeck, fill = Transported))+\n  geom_density(alpha =0.5) \n\np1 + p2 + p3 + plot_layout(guides = \"collect\") &\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nGastos com RoomService, Spa e VRDeck aumentam a chance do passageiro n√£o ser transportado!\n\n# FoodCourt\np1 = dados %&gt;% \n  mutate(FoodCourt = log10(FoodCourt)) %&gt;% \n  ggplot(aes(x=FoodCourt, fill = Transported))+\n  geom_density(alpha =0.5)\n\n# ShoppingMall\np2 = dados %&gt;% \n  mutate(ShoppingMall = log10(ShoppingMall)) %&gt;% \n  ggplot(aes(x=ShoppingMall, fill = Transported))+\n  geom_density(alpha =0.5)\n\np1 + p2 + plot_layout(guides = \"collect\") &\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nJ√° os gastos com FoodCourt e ShoppingMall aumentam a chance do passageiro ser transportado. Outra informa√ß√£o interessante pode ser obtida quando analisamos as vari√°veis HomePlanet e CryoSleep.\n\n# HomePlanet e CryoSleep\ndados %&gt;% \n  ggplot(aes(x=HomePlanet, fill= Transported))+\n  geom_bar(stat = \"count\")+\n  facet_grid(~CryoSleep)+\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\nNote que, para os passageiros em suspens√£o profunda, o fato de n√£o terem embarcados em Earth aumenta muito a chance de ser transportado para a dimes√£o alternativa!\n\n\n\n\n\n\nResumindo‚Ä¶\n\n\n\nAp√≥s a fase de explora√ß√£o dos dados, algumas sugest√µes de feature engineering foram aparecendo:\n\nCriar uma vari√°vel indicando se o passageiro √© menor de 14 anos, por exemplo;\nCriar uma vari√°vel indicando se o passageiro √© origin√°rio da Terra;\nCriar uma vari√°vel indicando se o passageiro viajava sozinho;\nRealizar transforma√ß√£o (ra√≠z quadrada) nas vari√°veis RoomService, Spa, VRDeck, FoodCourt e ShoppingMall.\nCriar uma vari√°vel com o total de ‚Äúdespesas sup√©rfluas‚Äù (RoomService + Spa + VRDeck);\nCriar uma vari√°vel com o total de ‚Äúdespesas b√°sicas‚Äù (FoodCourt + ShoppingMall);\n\n\n\nPara evitar problema de vazamento de dados, o passo inicial de qualquer an√°lise √© separar o conjunto de treinamento, valida√ß√£o e teste: os dados de treinamento devem ser usados exclusivamente para treinar o modelo, enquanto os dados de valida√ß√£o e teste devem ser usados para avaliar sua performance. Isso evita que informa√ß√µes de teste ou valida√ß√£o sejam usadas para treinar o modelo e comprometam os resultados.\n\nset.seed(12345)\nsplits = dados %&gt;% initial_split(strata = Transported)\n\ndados_treino = training(splits)\ndados_teste = testing(splits)\n\nCriando a vari√°vel GroupSize apenas para o conjunto de dados de treinamento:\n\ndados_treino = dados_treino %&gt;% add_count(PassGroup, name = \"GroupSize\")"
  },
  {
    "objectID": "analise_dados/space_titanic.html#tratamento-de-dados-faltantes",
    "href": "analise_dados/space_titanic.html#tratamento-de-dados-faltantes",
    "title": "Pr√©-processamento usando a base de dados Space Titanic",
    "section": "Tratamento de dados faltantes",
    "text": "Tratamento de dados faltantes\n\ndados_treino %&gt;% df_status()\n\n       variable q_zeros p_zeros q_na p_na q_inf p_inf      type unique\n1   PassengerId       0    0.00    0 0.00     0     0 character   6519\n2     PassGroup       0    0.00    0 0.00     0     0   numeric   4949\n3        nGroup       0    0.00    0 0.00     0     0 character      8\n4    HomePlanet       0    0.00    0 0.00     0     0 character      4\n5     CryoSleep    4075   62.51  164 2.52     0     0   logical      2\n6     CabinDeck       0    0.00    0 0.00     0     0    factor      9\n7      CabinNum      13    0.20  153 2.35     0     0 character   1722\n8     CabinSide       0    0.00  153 2.35     0     0    factor      2\n9   Destination       0    0.00    0 0.00     0     0 character      4\n10          Age     135    2.07  133 2.04     0     0   numeric     80\n11          VIP    6213   95.31  158 2.42     0     0   logical      2\n12  RoomService    4178   64.09  133 2.04     0     0   numeric   1088\n13    FoodCourt    4096   62.83  137 2.10     0     0   numeric   1247\n14 ShoppingMall    4187   64.23  161 2.47     0     0   numeric    951\n15          Spa    3980   61.05  146 2.24     0     0   numeric   1103\n16       VRDeck    4155   63.74  134 2.06     0     0   numeric   1086\n17         Name       0    0.00    0 0.00     0     0 character   6356\n18  Transported    3236   49.64    0 0.00     0     0   logical      2\n19    GroupSize       0    0.00    0 0.00     0     0   integer      8\n\n\n\nmiss = summary(aggr(dados_treino, sortVar=TRUE))$combinations\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n     Variable      Count\n    CryoSleep 0.02515723\n ShoppingMall 0.02469704\n          VIP 0.02423685\n     CabinNum 0.02346986\n    CabinSide 0.02346986\n          Spa 0.02239607\n    FoodCourt 0.02101549\n       VRDeck 0.02055530\n          Age 0.02040190\n  RoomService 0.02040190\n  PassengerId 0.00000000\n    PassGroup 0.00000000\n       nGroup 0.00000000\n   HomePlanet 0.00000000\n    CabinDeck 0.00000000\n  Destination 0.00000000\n         Name 0.00000000\n  Transported 0.00000000\n    GroupSize 0.00000000\n\n\nO gr√°fico acima mostra (√† esquerda) mostra a propor√ß√£o de missing values para cada vari√°vel da base de dados e todas as combina√ß√µes de valores ausentes (vermelho) e observados (azul) presentes nos dados (gr√°fico √† direita).\n\nhead(miss[rev(order(miss[,2])),])\n\n                            Combinations Count   Percent\n1  0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0  5306 81.392852\n38 0:0:0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0   136  2.086210\n5  0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0   136  2.086210\n17 0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0:0:0:0   133  2.040190\n3  0:0:0:0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0   127  1.948152\n28 0:0:0:0:0:0:1:1:0:0:0:0:0:0:0:0:0:0:0   125  1.917472\n\n\nNote que, apesar de termos aproximadamente 2,5% de dados faltantes em cada vari√°vel, temos na base de dados aproximadamente 76% de casos completos, inviabilizando o descarte dessas observa√ß√µes incompletas.\n\n\n\n\n\n\nDica\n\n\n\nUm consenso entre especialistas √© que, se uma vari√°vel tem mais de 20% de seus dados faltantes, talvez seja melhor exclu√≠-la e entender as raz√µes dos valores ausentes. Se a propor√ß√£o de valores ausentes √© menor do que isso, √© poss√≠vel preencher esses dados utilizando alguma t√©cnica de imputa√ß√£o, dependendo do tipo de dado. J√° quando h√° menos de 2% de valores faltantes, a exclus√£o desses registros pode ser uma op√ß√£o, j√° que n√£o afetar√° significativamente a base de dados.\n\n\nNo entanto, √© importante avaliar os efeitos do tratamento dos valores ausentes na an√°lise e nos resultados finais, para garantir que as decis√µes tomadas sejam embasadas e coerentes com os objetivos do estudo. Esse processo dever√° ser feito com parcim√¥nia, sob pena de causar um vi√©s de informa√ß√£o. A imputa√ß√£o pode introduzir vi√©s de informa√ß√£o na medida em que os valores imputados podem n√£o ser representativos da realidade ou podem n√£o capturar a variabilidade dos dados ausentes.\n\n\n\n\n\n\nNota\n\n\n\nPara realizar a imputa√ß√£o das vari√°veis usaremos as seguintes estrat√©gias:\n\nPara as vari√°veis CryoSleep, RoomService, FoodCourt, ShoppingMall, Spa e VRDeck: Se CryoSleep √© missing e pelo menos uma das vari√°veis de gastos for maior que zero, ent√£o imputaremos CryoSleep como FALSE (Se tem gasto, significa que n√£o est√° em suspens√£o profunda). Se CryoSleep √© TRUE e alguma vari√°vel de gastos for missing, ent√£o essa vari√°vel ser√° imputada com o valor zero (Quem est√° em suspens√£o profunda n√£o tem gastos). Al√©m disso, imputaremos o restante dos missings utilizando imputa√ß√£o knn, considerando 2 vizinhos e as vari√°veis PassGroup, CabinDeck, CabinNum, CabinSide.\nAs demais vari√°veis ser√£o imputadas utilizando bagged trees.\n\n\n\nPodemos ent√£o construir nossa receita para realizar todo o pr√©-processamento:\n\nSTRecipe = recipe(Transported ~ ., data = dados_treino) %&gt;% \n  update_role(PassengerId, Name, new_role = \"ID\") %&gt;%\n  step_mutate(\n    IsAlone = ifelse(GroupSize==1, 1, 0) %&gt;% as.factor(),\n    CabinNum = as.numeric(CabinNum),\n    nGroup = as.numeric(nGroup),\n    VIP = as.factor(VIP),\n    CryoSleep = ifelse((RoomService &gt; 0 | FoodCourt &gt; 0 | ShoppingMall &gt; 0 | Spa &gt; 0 | VRDeck &gt; 0), replace_na(CryoSleep, FALSE),TRUE),\n    CryoSleep = as.factor(CryoSleep)) %&gt;% \n  step_impute_knn(CryoSleep, neighbors = 2, impute_with = imp_vars(PassGroup, CabinDeck, CabinNum, CabinSide)) %&gt;% \n  step_mutate(\n    RoomService = ifelse(CryoSleep==TRUE, replace_na(RoomService, 0),RoomService),\n    FoodCourt = ifelse(CryoSleep==TRUE, replace_na(FoodCourt, 0),FoodCourt),\n    ShoppingMall = ifelse(CryoSleep==TRUE, replace_na(ShoppingMall, 0), ShoppingMall),\n    Spa = ifelse(CryoSleep==TRUE, replace_na(Spa, 0), Spa),\n    VRDeck = ifelse(CryoSleep==TRUE, replace_na(VRDeck, 0), VRDeck)\n  ) %&gt;% \n  step_impute_knn(RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, neighbors = 2, \n                  impute_with = imp_vars(PassGroup, CabinDeck, CabinNum, CabinSide)) %&gt;%\n  step_impute_bag(Age, VIP, CryoSleep, Destination, HomePlanet, CabinDeck, CabinNum, \n                  CabinSide) %&gt;%\n  step_mutate_at(RoomService:VRDeck, fn= ~log10(. + 1)) %&gt;% \n  step_mutate(despesasSuperfluas = RoomService + Spa + VRDeck,\n              despesasBasicas = FoodCourt + ShoppingMall,\n              Isearth = ifelse(HomePlanet=='Earth', 1, 0) %&gt;% as.factor(),\n              Iskid = ifelse(Age &lt; 14, 1, 0) %&gt;% as.factor()) %&gt;% \n  prep(verbose=TRUE)\n\noper 1 step mutate [training] \noper 2 step impute knn [training] \noper 3 step mutate [training] \noper 4 step impute knn [training] \noper 5 step impute bag [training] \noper 6 step mutate at [training] \noper 7 step mutate [training] \nThe retained training set is ~ 1.74 Mb  in memory.\n\ndados_prep = bake(STRecipe, new_data = NULL)\n\n\ndados_prep %&gt;% \n  summary()\n\n  PassengerId     PassGroup        nGroup       HomePlanet   CryoSleep   \n 0002_01:   1   Min.   :   2   Min.   :1.000         : 133   FALSE:3910  \n 0003_01:   1   1st Qu.:2306   1st Qu.:1.000   Earth :3482   TRUE :2609  \n 0004_01:   1   Median :4605   Median :1.000   Europa:1610               \n 0005_01:   1   Mean   :4620   Mean   :1.515   Mars  :1294               \n 0006_01:   1   3rd Qu.:6880   3rd Qu.:2.000                             \n 0006_02:   1   Max.   :9280   Max.   :8.000                             \n (Other):6513                                                            \n   CabinDeck       CabinNum      CabinSide        Destination        Age       \n F      :2085   Min.   :   1.0   P:3273                 : 131   Min.   : 0.00  \n G      :1916   1st Qu.: 487.0   S:3246    55 Cancri e  :1345   1st Qu.:19.00  \n E      : 644   Median : 922.0             PSO J318.5-22: 604   Median :27.00  \n B      : 587   Mean   : 889.1             TRAPPIST-1e  :4439   Mean   :28.72  \n C      : 584   3rd Qu.:1256.5                                  3rd Qu.:37.00  \n D      : 361   Max.   :1722.0                                  Max.   :79.00  \n (Other): 342                                                                  \n    VIP        RoomService      FoodCourt       ShoppingMall   \n FALSE:6371   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n TRUE : 148   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n              Median :0.000   Median :0.0000   Median :0.0000  \n              Mean   :0.778   Mean   :0.8493   Mean   :0.7138  \n              3rd Qu.:1.724   3rd Qu.:1.9085   3rd Qu.:1.4698  \n              Max.   :4.156   Max.   :4.4744   Max.   :4.3709  \n                                                               \n      Spa             VRDeck                       Name        GroupSize    \n Min.   :0.0000   Min.   :0.0000                     : 150   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   Alraium Disivering:   2   1st Qu.:1.000  \n Median :0.0000   Median :0.0000   Ankalik Nateansive:   2   Median :1.000  \n Mean   :0.8192   Mean   :0.7762   Anton Woody       :   2   Mean   :1.781  \n 3rd Qu.:1.7817   3rd Qu.:1.6532   Apix Wala         :   2   3rd Qu.:2.000  \n Max.   :4.3504   Max.   :4.3826   Asch Stradick     :   2   Max.   :8.000  \n                                   (Other)           :6359                  \n Transported     IsAlone  despesasSuperfluas despesasBasicas Isearth  Iskid   \n Mode :logical   0:2563   Min.   : 0.000     Min.   :0.000   0:3037   0:5795  \n FALSE:3236      1:3956   1st Qu.: 0.000     1st Qu.:0.000   1:3482   1: 724  \n TRUE :3283               Median : 2.196     Median :0.699                    \n                          Mean   : 2.373     Mean   :1.563                    \n                          3rd Qu.: 4.397     3rd Qu.:2.945                    \n                          Max.   :10.508     Max.   :7.635                    \n                                                                              \n\n\nOutras transforma√ß√µes se fazem necess√°rias e ser√£o realizadas em momento oportuno."
  },
  {
    "objectID": "aulas/Analise_preditiva/python-env/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "aulas/Analise_preditiva/python-env/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm‚Äôs designer. Component licenses are located with the component code."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#pr√©-processamento-de-dados",
    "href": "aulas/Pre_processamento/pre_processamento.html#pr√©-processamento-de-dados",
    "title": "Pr√©-processamento de dados",
    "section": "Pr√©-processamento de dados",
    "text": "Pr√©-processamento de dados\nüí° Pr√©-processamento de dados se define como um conjunto de t√©cnicas e procedimentos aplicados aos dados antes de sua utiliza√ß√£o em um modelo ou algoritmo de an√°lise.\n\n√â uma etapa fundamental na an√°lise de dados, pois visa garantir que os dados estejam corretos, completos, coerentes e em um formato apropriado para serem analisados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#pr√©-processamento-de-dados-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#pr√©-processamento-de-dados-1",
    "title": "Pr√©-processamento de dados",
    "section": "Pr√©-processamento de dados",
    "text": "Pr√©-processamento de dados\n\n\n\n\n\n\n\n\n\nO pr√©-processamento de dados inclui diversas atividades, como limpeza de dados, transforma√ß√£o de dados, redu√ß√£o de dimens√£o, sele√ß√£o de recursos, normaliza√ß√£o e amostragem.\n\n\n\nüëâ O objetivo geral do pr√©-processamento de dados √© aumentar a qualidade e a precis√£o da an√°lise de dados, minimizando a influ√™ncia de ru√≠dos e inconsist√™ncias nos resultados finais."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#import√¢ncia-do-pr√©-processamento",
    "href": "aulas/Pre_processamento/pre_processamento.html#import√¢ncia-do-pr√©-processamento",
    "title": "Pr√©-processamento de dados",
    "section": "Import√¢ncia do pr√©-processamento",
    "text": "Import√¢ncia do pr√©-processamento\nO pr√©-processamento de dados √© uma etapa importante e cr√≠tica no processo de an√°lise de dados. Algumas raz√µes pelas quais √© importante realizar o pr√©-processamento s√£o:\n\nüëâ Melhoria da qualidade dos dados: O pr√©-processamento de dados pode ajudar a melhorar a qualidade dos dados, eliminando dados duplicados, ausentes ou inconsistentes. Isso resulta em dados mais precisos e confi√°veis para an√°lise.\n\n\nüëâ Ajuste dos dados para a an√°lise: Muitos modelos e algoritmos de an√°lise de dados requerem que os dados estejam em um formato espec√≠fico. Na etapa do pr√©-processamento de dados ajusta-se os dados para que estejam em um formato adequado para a an√°lise."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#import√¢ncia-do-pr√©-processamento-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#import√¢ncia-do-pr√©-processamento-1",
    "title": "Pr√©-processamento de dados",
    "section": "Import√¢ncia do pr√©-processamento",
    "text": "Import√¢ncia do pr√©-processamento\nüëâ Elimina√ß√£o de ru√≠dos: Os dados geralmente cont√™m ru√≠dos, como outliers e valores extremos, que podem afetar negativamente a precis√£o dos resultados finais. Nesta etapa, elimina-se esses ru√≠dos, tornando os resultados mais precisos.\n\nüëâ Redu√ß√£o da complexidade dos dados: Podemos reduzir a complexidade dos dados, eliminando vari√°veis desnecess√°rias e reduzindo o n√∫mero de dimens√µes do conjunto de dados. Isso torna mais f√°cil a an√°lise e interpreta√ß√£o dos dados.\n\n\nüëâ Melhoria do desempenho do modelo: O pr√©-processamento de dados pode ajudar a melhorar o desempenho do modelo, ao remover dados redundantes, normalizar os dados e selecionar os recursos mais importantes. Isso resulta em modelos mais precisos e confi√°veis."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#import√¢ncia-do-pr√©-processamento-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#import√¢ncia-do-pr√©-processamento-2",
    "title": "Pr√©-processamento de dados",
    "section": "Import√¢ncia do pr√©-processamento",
    "text": "Import√¢ncia do pr√©-processamento\n\nFonte: Forbes"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#import√¢ncia-do-pr√©-processamento-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#import√¢ncia-do-pr√©-processamento-3",
    "title": "Pr√©-processamento de dados",
    "section": "Import√¢ncia do pr√©-processamento",
    "text": "Import√¢ncia do pr√©-processamento\n\nFonte: Giphy"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#o-que-h√°-de-errado-nessa-base",
    "href": "aulas/Pre_processamento/pre_processamento.html#o-que-h√°-de-errado-nessa-base",
    "title": "Pr√©-processamento de dados",
    "section": "O que h√° de errado nessa base?",
    "text": "O que h√° de errado nessa base?\n\n\n\n\n\nUsu√°rio\n\n\nIdade\n\n\nCidade\n\n\nData\n\n\nSal√°rio\n\n\nNp√°ginas\n\n\nNsess√µes\n\n\nNprodutos\n\n\nClique\n\n\nComprou\n\n\n\n\n\nuser1\n\n\n22\n\n\nBelo Horizonte\n\n\n22/10/21\n\n\n1200\n\n\n5\n\n\n2\n\n\n5\n\n\nno\n\n\nno\n\n\n\n\nuser2\n\n\n1\n\n\nS√£o paulo\n\n\n20/09/22\n\n\n5000\n\n\n21\n\n\n4\n\n\n11\n\n\nyes\n\n\nyes\n\n\n\n\nuser3\n\n\n41\n\n\nBH\n\n\n23 de dezembro 2022\n\n\n12000\n\n\n2\n\n\n1\n\n\n3\n\n\nNA\n\n\nyes\n\n\n\n\nuser4\n\n\n32\n\n\nMiami\n\n\n22/10/01\n\n\n50000\n\n\nNA\n\n\n5\n\n\n2\n\n\nyes\n\n\nno\n\n\n\n\nuser5\n\n\n20\n\n\nTokyo\n\n\n23/01/25\n\n\n12220\n\n\n5\n\n\n5\n\n\n8\n\n\nno\n\n\nyes\n\n\n\n\nuser6\n\n\n28\n\n\nNY\n\n\nMarch 14, 2018\n\n\n250000\n\n\n8\n\n\nNA\n\n\nNA\n\n\nno\n\n\nyes"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#principais-problemas-com-dados-reais",
    "href": "aulas/Pre_processamento/pre_processamento.html#principais-problemas-com-dados-reais",
    "title": "Pr√©-processamento de dados",
    "section": "Principais problemas com dados reais",
    "text": "Principais problemas com dados reais"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tipos-de-dados",
    "href": "aulas/Pre_processamento/pre_processamento.html#tipos-de-dados",
    "title": "Pr√©-processamento de dados",
    "section": "Tipos de dados",
    "text": "Tipos de dados\nOs dados podem ser classificados em tr√™s tipos principais: estruturados, semiestruturados e n√£o estruturados\nüëâ Estruturados\nS√£o dados organizados em uma estrutura definida, como tabelas, colunas e linhas em um banco de dados.\n\n\nExemplos: informa√ß√µes de vendas, cadastros de clientes, registros de transa√ß√µes financeiras, entre outros.\n\n\nüíª Para dados estruturados, a t√©cnica de pr√©-processamento mais comum √© a limpeza de dados, al√©m da sele√ß√£o de atributos relevantes, normaliza√ß√£o e transforma√ß√£o de dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tipos-de-dados-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#tipos-de-dados-1",
    "title": "Pr√©-processamento de dados",
    "section": "Tipos de dados",
    "text": "Tipos de dados\nüëâ Semiestruturados\nS√£o dados que n√£o possuem uma estrutura r√≠gida como os dados estruturados, mas possuem algumas formas de organiza√ß√£o, como marcadores, tags ou atributos.\n\n\nExemplos: arquivos XML, JSON, HTML, entre outros.\n\n\nüíª Para dados semiestruturados, a t√©cnica de pr√©-processamento mais comum √© a extra√ß√£o de informa√ß√µes relevantes, que envolve a identifica√ß√£o de padr√µes e estruturas em dados como XML, JSON ou HTML, al√©m da extra√ß√£o de dados a partir dessas estruturas."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tipos-de-dados-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#tipos-de-dados-2",
    "title": "Pr√©-processamento de dados",
    "section": "Tipos de dados",
    "text": "Tipos de dados\nüëâ N√£o estruturados\nS√£o dados que n√£o possuem uma estrutura definida e est√£o em formato livre, como texto, imagens, v√≠deos, √°udios, e-mails, redes sociais, entre outros.\n\nüíª Exigem o uso de t√©cnicas mais avan√ßadas de processamento de linguagem natural, reconhecimento de padr√µes e aprendizado de m√°quina.\n\n\nCom o aumento da quantidade de dados gerados diariamente, a an√°lise de dados semiestruturados e n√£o estruturados tem se tornado cada vez mais importante para empresas que desejam obter insights valiosos sobre seus clientes, mercado e tend√™ncias."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#processo-de-prepara√ß√£o-da-base-de-dados",
    "href": "aulas/Pre_processamento/pre_processamento.html#processo-de-prepara√ß√£o-da-base-de-dados",
    "title": "Pr√©-processamento de dados",
    "section": "Processo de prepara√ß√£o da base de dados",
    "text": "Processo de prepara√ß√£o da base de dados"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#limpeza-de-dados",
    "href": "aulas/Pre_processamento/pre_processamento.html#limpeza-de-dados",
    "title": "Pr√©-processamento de dados",
    "section": "Limpeza de dados",
    "text": "Limpeza de dados\nü§î O que √© limpeza de dados? üßπ\n\n\nLimpeza de dados √© o processo de identificar, remover ou corrigir dados imprecisos, incompletos, inconsistentes, duplicados ou irrelevantes em um conjunto de dados.\n\n\n\n√â uma etapa importante no processo de an√°lise de dados, uma vez que dados sujos podem afetar negativamente a qualidade e confiabilidade dos resultados da an√°lise."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas",
    "href": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas",
    "title": "Pr√©-processamento de dados",
    "section": "Identifica√ß√£o de problemas",
    "text": "Identifica√ß√£o de problemas\nüëâ Existem v√°rias t√©cnicas para identificar problemas de limpeza de dados em um conjunto de dados. Algumas das t√©cnicas mais comuns incluem:\n\n\nAn√°lise visual: pode-se inspecionar o conjunto de dados para identificar erros √≥bvios, como valores ausentes, valores que n√£o fazem sentido ou valores extremos.\n\n\nFonte:Nagwa"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas-1",
    "title": "Pr√©-processamento de dados",
    "section": "Identifica√ß√£o de problemas",
    "text": "Identifica√ß√£o de problemas\n\n\nEstat√≠stica descritiva: realizar uma boa an√°lise descritiva. Valores extremos ou valores que est√£o muito longe do intervalo interquartil, por exemplo, podem ser indicativos de problemas de limpeza de dados.\n\n\nFonte:Statistics Cartoons by Ben Shabad"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas-2",
    "title": "Pr√©-processamento de dados",
    "section": "Identifica√ß√£o de problemas",
    "text": "Identifica√ß√£o de problemas\n\n\nGr√°ficos de distribui√ß√£o: analisar os gr√°ficos de distribui√ß√£o para cada vari√°vel e observar padr√µes incomuns na distribui√ß√£o dos valores, como bimodalidade, assimetria ou outliers."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas-3",
    "title": "Pr√©-processamento de dados",
    "section": "Identifica√ß√£o de problemas",
    "text": "Identifica√ß√£o de problemas\n\n\nAn√°lise de correla√ß√£o: analisar as correla√ß√µes entre as vari√°veis e procurar rela√ß√µes que n√£o fazem sentido ou que n√£o deveriam existir.\n\n\nFonte:Medium"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas-4",
    "href": "aulas/Pre_processamento/pre_processamento.html#identifica√ß√£o-de-problemas-4",
    "title": "Pr√©-processamento de dados",
    "section": "Identifica√ß√£o de problemas",
    "text": "Identifica√ß√£o de problemas\n\n\nAn√°lise de consist√™ncia: comparar valores em diferentes vari√°veis para ver se eles s√£o consistentes entre si.\n\nPor exemplo, se uma pessoa √© descrita como tendo 2 metros de altura e 30 kg de peso, pode haver um problema de limpeza de dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-dados-faltantes",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-dados-faltantes",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de dados faltantes",
    "text": "Tratamento de dados faltantes\n\nFonte: Giphy"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-dados-faltantes-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-dados-faltantes-1",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de dados faltantes",
    "text": "Tratamento de dados faltantes\n\n\n\n\n‚ÄúThe idea of imputation is both seductive and dangerous‚Äù.\n\n\nMadow et al. (1983)\n\n\n\n\n\n\n\n\nA escolha do m√©todo de tratamento de valor ausente depende do tipo de valor ausente identificado. Basicamente, existem dois tipos de valores ausentes: aleat√≥rios e n√£o aleat√≥rios"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#mecanismo-gerador-dos-dados-faltantes",
    "href": "aulas/Pre_processamento/pre_processamento.html#mecanismo-gerador-dos-dados-faltantes",
    "title": "Pr√©-processamento de dados",
    "section": "Mecanismo gerador dos dados faltantes",
    "text": "Mecanismo gerador dos dados faltantes\n\n\n\n\n\n\n\n\nMCAR (missing completely at random): ocorrem de forma completamente aleat√≥ria e n√£o est√£o relacionados com as outras vari√°veis do conjunto de dados.\n\n\n\nüëâ Por exemplo, em uma pesquisa por telefone algumas das respostas n√£o foram registradas devido a problemas t√©cnicos que ocorreram aleatoriamente durante a coleta dos dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#mecanismo-gerador-dos-dados-faltantes-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#mecanismo-gerador-dos-dados-faltantes-1",
    "title": "Pr√©-processamento de dados",
    "section": "Mecanismo gerador dos dados faltantes",
    "text": "Mecanismo gerador dos dados faltantes\n\n\n\nMNAR (missing not at random): s√£o valores ausentes que est√£o relacionados com outras vari√°veis do conjunto de dados e podem levar a enviesamento na an√°lise dos dados.\n\n\n\n\n\n\n\n\nüëâ Por exemplo, em uma pesquisa sobre sa√∫de mental, as pessoas que sofrem de depress√£o tendem a n√£o responder perguntas sobre sua condi√ß√£o de sa√∫de mental."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes",
    "href": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes",
    "title": "Pr√©-processamento de dados",
    "section": "Como identificar o mecanismo gerador dos dados faltantes?",
    "text": "Como identificar o mecanismo gerador dos dados faltantes?\n\n\n\n\n\n\n\n\nPara identificar valores ausentes MCAR, uma t√©cnica √© analisar a distribui√ß√£o de valores ausentes em rela√ß√£o √†s outras vari√°veis do conjunto de dados.\n\n\n\nüëâ Se n√£o houver correla√ß√£o entre a aus√™ncia de valores e outras vari√°veis, √© prov√°vel que os valores ausentes sejam MCAR."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes-1",
    "title": "Pr√©-processamento de dados",
    "section": "Como identificar o mecanismo gerador dos dados faltantes?",
    "text": "Como identificar o mecanismo gerador dos dados faltantes?\nüëâ Tamb√©m √© poss√≠vel realizar testes estat√≠sticos para verificar se a distribui√ß√£o dos valores ausentes √© aleat√≥ria ou n√£o.\n\nüôÅ J√° a identifica√ß√£o de valores ausentes MNAR √© mais complexa, pois eles est√£o relacionados a outras vari√°veis do conjunto de dados e, portanto, √© mais dif√≠cil determinar se eles s√£o completamente aleat√≥rios ou n√£o."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes-2",
    "title": "Pr√©-processamento de dados",
    "section": "Como identificar o mecanismo gerador dos dados faltantes?",
    "text": "Como identificar o mecanismo gerador dos dados faltantes?\nüëâ Algumas t√©cnicas para identificar valores ausentes MNAR incluem:\n\n\n\nAn√°lise de padr√µes de resposta: √© poss√≠vel analisar como objetos com valores ausentes se diferenciam daqueles que n√£o t√™m valores ausentes, em termos dos atributos.\n\n\n\n\n\n\n\n\n\nSe os indiv√≠duos com valores ausentes s√£o significativamente diferentes em rela√ß√£o a essas vari√°veis, √© prov√°vel que os valores ausentes sejam MNAR."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes-3",
    "title": "Pr√©-processamento de dados",
    "section": "Como identificar o mecanismo gerador dos dados faltantes?",
    "text": "Como identificar o mecanismo gerador dos dados faltantes?\n\n\n\nAn√°lise de correla√ß√£o: √© poss√≠vel analisar a correla√ß√£o entre as vari√°veis com valores ausentes e outras vari√°veis no conjunto de dados.\n\nSe houver uma correla√ß√£o significativa entre as vari√°veis, √© poss√≠vel que os valores ausentes sejam MNAR."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes-4",
    "href": "aulas/Pre_processamento/pre_processamento.html#como-identificar-o-mecanismo-gerador-dos-dados-faltantes-4",
    "title": "Pr√©-processamento de dados",
    "section": "Como identificar o mecanismo gerador dos dados faltantes?",
    "text": "Como identificar o mecanismo gerador dos dados faltantes?\n\n\n\nAn√°lise visual: √© poss√≠vel realizar an√°lises gr√°ficas para verificar se h√° algum padr√£o de valores ausentes que sugira que eles n√£o s√£o aleat√≥rios.\n\nPor exemplo, pode-se criar um gr√°fico de dispers√£o de uma vari√°vel em rela√ß√£o a outra, marcando as observa√ß√µes com valores ausentes. Se houver um padr√£o na localiza√ß√£o dos valores ausentes no gr√°fico, √© poss√≠vel que eles sejam MNAR."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-completamente-aleat√≥rios",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-completamente-aleat√≥rios",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores ausentes completamente aleat√≥rios",
    "text": "Tratamento de valores ausentes completamente aleat√≥rios\n\n\n\nO tratamento de valores ausentes completamente aleat√≥rios (MCAR) √© relativamente simples, pois os valores ausentes s√£o independentes das demais vari√°veis e podem ser tratados sem vi√©s."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-completamente-aleat√≥rios-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-completamente-aleat√≥rios-1",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores ausentes completamente aleat√≥rios",
    "text": "Tratamento de valores ausentes completamente aleat√≥rios\n\n\n\nExclus√£o de registros com valores ausentes: Se a propor√ß√£o de valores ausentes √© pequena em rela√ß√£o ao tamanho do conjunto de dados, √© poss√≠vel excluir os registros que cont√™m valores ausentes sem comprometer a an√°lise.\n\n\n\n\n\n\n\n\n\n\nNo entanto, isso pode levar a uma perda de informa√ß√µes valiosas e reduzir o tamanho do conjunto de dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-completamente-aleat√≥rios-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-completamente-aleat√≥rios-2",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores ausentes completamente aleat√≥rios",
    "text": "Tratamento de valores ausentes completamente aleat√≥rios\n\n\n\nImputa√ß√£o de valores: Se o n√∫mero de valores ausentes √© grande, √© poss√≠vel imputar os valores ausentes usando m√©todos estat√≠sticos, como a m√©dia, mediana ou regress√£o.\n\n\n\n\n\n\n\n\n\n\nEsses m√©todos s√£o simples e r√°pidos, mas podem introduzir um vi√©s nos resultados, dependendo da distribui√ß√£o dos valores ausentes e da escolha do m√©todo de imputa√ß√£o."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-completamente-aleat√≥rios-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-completamente-aleat√≥rios-3",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores ausentes completamente aleat√≥rios",
    "text": "Tratamento de valores ausentes completamente aleat√≥rios\n\n\n\n\n\n\n\n\n\n\nModelagem com t√©cnicas de aprendizado de m√°quina: As t√©cnicas de aprendizado de m√°quina podem ajudar a prever valores ausentes com base em outros dados dispon√≠veis.\n\n\n\nMais precisa do que a imputa√ß√£o de valores, mas tamb√©m pode ser mais complexa e exigir um conjunto de dados de treinamento grande o suficiente para o modelo aprender a rela√ß√£o entre as vari√°veis."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-n√£o-aleat√≥rios",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-n√£o-aleat√≥rios",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores ausentes n√£o aleat√≥rios",
    "text": "Tratamento de valores ausentes n√£o aleat√≥rios\n\n\nO tratamento de valores ausentes n√£o aleat√≥rios (MNAR) √© mais complexo do que o tratamento de valores ausentes completamente aleat√≥rios (MCAR), pois os valores ausentes est√£o relacionados a outras vari√°veis do conjunto de dados e podem levar a um vi√©s na an√°lise se n√£o forem tratados adequadamente."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-n√£o-aleat√≥rios-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-n√£o-aleat√≥rios-1",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores ausentes n√£o aleat√≥rios",
    "text": "Tratamento de valores ausentes n√£o aleat√≥rios\n\n\nModelagem com t√©cnicas de aprendizado de m√°quina: Como os valores ausentes MNAR est√£o relacionados a outras vari√°veis do conjunto de dados, a modelagem com t√©cnicas de aprendizado de m√°quina pode ser uma abordagem √∫til para prever os valores ausentes com base nas informa√ß√µes dispon√≠veis.\n\n\n\n\nImputa√ß√£o baseada em modelos: A imputa√ß√£o baseada em modelos √© uma t√©cnica que envolve a constru√ß√£o de um modelo estat√≠stico para prever os valores ausentes com base nas informa√ß√µes dispon√≠veis.\n\nEssa abordagem requer um conhecimento pr√©vio da rela√ß√£o entre as vari√°veis e pode ser mais complexa do que outros m√©todos de imputa√ß√£o."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-n√£o-aleat√≥rios-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-n√£o-aleat√≥rios-2",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores ausentes n√£o aleat√≥rios",
    "text": "Tratamento de valores ausentes n√£o aleat√≥rios\n\n\n\nAn√°lise de sensibilidade: A an√°lise de sensibilidade √© uma t√©cnica que envolve testar diferentes cen√°rios e valores de imputa√ß√£o para avaliar o impacto nos resultados da an√°lise.\n\nEssa abordagem pode ajudar a identificar os valores mais plaus√≠veis para a imputa√ß√£o e a avaliar a robustez dos resultados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-n√£o-aleat√≥rios-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-ausentes-n√£o-aleat√≥rios-3",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores ausentes n√£o aleat√≥rios",
    "text": "Tratamento de valores ausentes n√£o aleat√≥rios\nUma vez que os valores ausentes est√£o relacionados a outras vari√°veis do conjunto de dados, h√° um padr√£o de aus√™ncia definido em MNAR, que pode ser importante.\n\n\n\n\nUma maneira de reter esse padr√£o de aus√™ncia √© adicionando uma vari√°vel bin√°ria, indicando se a caracter√≠stica foi imputada."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-inconsistentes",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-inconsistentes",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores inconsistentes",
    "text": "Tratamento de valores inconsistentes\nO tratamento de valores inconsistentes √© importante para garantir a qualidade e a confiabilidade dos dados ‚Üí podem afetar negativamente a an√°lise e a interpreta√ß√£o dos resultados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-inconsistentes-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-inconsistentes-1",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores inconsistentes",
    "text": "Tratamento de valores inconsistentes\nAlgumas t√©cnicas comuns para lidar com valores inconsistentes incluem:\n\n\nIdentifica√ß√£o de valores inconsistentes: Verifica-se os valores em cada vari√°vel, comparando-os com os valores esperados ou com outras fontes de dados. Tamb√©m √© poss√≠vel usar t√©cnicas de detec√ß√£o de outliers para identificar valores inconsistentes automaticamente.\n\n\n\n\nCorre√ß√£o manual: Para valores inconsistentes que podem ser facilmente corrigidos, a corre√ß√£o manual pode ser uma abordagem eficaz.\n\nIsso pode incluir a revis√£o manual dos dados e a corre√ß√£o de erros, como erros de digita√ß√£o ou valores fora do intervalo esperado."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-inconsistentes-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-valores-inconsistentes-2",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de valores inconsistentes",
    "text": "Tratamento de valores inconsistentes\n\n\nImputa√ß√£o de valores: Para valores inconsistentes que n√£o podem ser facilmente corrigidos, a imputa√ß√£o de valores pode ser uma abordagem √∫til.\n\nIsso envolve a substitui√ß√£o de valores inconsistentes por valores estimados com base em outras informa√ß√µes dispon√≠veis no conjunto de dados.\n\n\n\n\n\n\nExclus√£o de registros: Para valores inconsistentes que n√£o podem ser corrigidos e n√£o podem ser imputados com precis√£o, a exclus√£o de registros pode ser uma abordagem apropriada.\n\nIsso envolve a remo√ß√£o dos registros com valores inconsistentes do conjunto de dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de ru√≠dos",
    "text": "Tratamento de ru√≠dos\n\nFonte: Giphy"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-1",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de ru√≠dos",
    "text": "Tratamento de ru√≠dos\nRu√≠do, no contexto da estat√≠stica e an√°lise de dados, refere-se a qualquer tipo de interfer√™ncia ou distor√ß√£o que pode obscurecer ou alterar a interpreta√ß√£o dos dados coletados.\n\nEsse fen√¥meno pode surgir de diversas fontes, como erros de entrada de dados, erros de medi√ß√£o, valores at√≠picos, variabilidade aleat√≥ria ou vi√©s e influ√™ncias externas que n√£o est√£o diretamente relacionadas ao fen√¥meno em estudo.\n\n\nO ru√≠do pode ser considerado um fator indesejado que compromete a precis√£o e a confiabilidade das an√°lises estat√≠sticas."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-2",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de ru√≠dos",
    "text": "Tratamento de ru√≠dos"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-3",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de ru√≠dos",
    "text": "Tratamento de ru√≠dos\n\n\n\nIdentifica√ß√£o e remo√ß√£o de valores at√≠picos: valores que s√£o muito diferentes dos outros valores no conjunto de dados podem ser identificados como valores at√≠picos.\n\nEsses valores podem ser removidos do conjunto de dados ou tratados separadamente para minimizar o impacto no resultado da an√°lise.\n\n\n\n\n\n\nCorre√ß√£o de erros de entrada de dados: erros de entrada de dados podem ser corrigidos por meio da revis√£o manual dos dados ou por meio de algoritmos de detec√ß√£o e corre√ß√£o autom√°tica de erros."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-4",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-4",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de ru√≠dos",
    "text": "Tratamento de ru√≠dos\n\n\n\nNormaliza√ß√£o ou padroniza√ß√£o de dados: a normaliza√ß√£o ou a padroniza√ß√£o de dados pode ser usada para tornar os dados compar√°veis e interpret√°veis, especialmente quando os dados t√™m unidades diferentes ou escalas diferentes.\n\n\n\n\nUtiliza√ß√£o de t√©cnicas de agrupamento: t√©cnicas de agrupamento, como an√°lise de cluster, podem ser usadas para identificar padr√µes em um conjunto de dados e reduzir o impacto do ru√≠do."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-5",
    "href": "aulas/Pre_processamento/pre_processamento.html#tratamento-de-ru√≠dos-5",
    "title": "Pr√©-processamento de dados",
    "section": "Tratamento de ru√≠dos",
    "text": "Tratamento de ru√≠dos\n\n\nUtiliza√ß√£o de t√©cnicas de suaviza√ß√£o de dados: t√©cnicas de suaviza√ß√£o, como m√©dia m√≥vel ou suaviza√ß√£o exponencial, podem ser usadas para reduzir o impacto do ru√≠do nos dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#como-identificar-um-valor-at√≠pico",
    "href": "aulas/Pre_processamento/pre_processamento.html#como-identificar-um-valor-at√≠pico",
    "title": "Pr√©-processamento de dados",
    "section": "Como identificar um valor at√≠pico?",
    "text": "Como identificar um valor at√≠pico?\n\nGr√°fico box-plot\n\n\n\n\nZ-score: valores com um z-score maior do que um limite (geralmente 3) s√£o considerados valores at√≠picos.\n\n\n\n\n\nIntervalo interquartil (IQR): valores abaixo de Q1 - 1,5 x IQR ou acima de Q3 + 1,5 x IQR s√£o considerados valores at√≠picos.\n\n\n\n\n\nM√©todo de DBSCAN: √© um algoritmo de clusteriza√ß√£o que agrupa pontos de dados que est√£o pr√≥ximos uns dos outros e identifica valores que n√£o est√£o pr√≥ximos a nenhum cluster, o que pode ser um sinal de valores at√≠picos."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#como-identificar-um-valor-at√≠pico-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#como-identificar-um-valor-at√≠pico-1",
    "title": "Pr√©-processamento de dados",
    "section": "Como identificar um valor at√≠pico?",
    "text": "Como identificar um valor at√≠pico?\n\nQuaisquer que sejam as t√©cnicas empregadas, a documenta√ß√£o e o registro das etapas de limpeza de dados s√£o essenciais para garantir a reprodutibilidade, transpar√™ncia, colabora√ß√£o, auditabilidade e controle de qualidade em an√°lises de dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#integra√ß√£o-de-dados",
    "href": "aulas/Pre_processamento/pre_processamento.html#integra√ß√£o-de-dados",
    "title": "Pr√©-processamento de dados",
    "section": "Integra√ß√£o de dados",
    "text": "Integra√ß√£o de dados\n\n\nIntegra√ß√£o de dados √© o processo de combinar dados de m√∫ltiplas fontes em um √∫nico conjunto de dados coerente e integrado.\n\n\n\n\n\n\n\nO objetivo da integra√ß√£o de dados √© criar um conjunto de dados mais completo e preciso, permitindo an√°lises mais abrangentes e informadas."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#integra√ß√£o-de-dados-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#integra√ß√£o-de-dados-1",
    "title": "Pr√©-processamento de dados",
    "section": "Integra√ß√£o de dados",
    "text": "Integra√ß√£o de dados\n\n\nFus√£o de dados: √© o processo de combinar dois ou mais conjuntos de dados com base em uma chave comum.\n\nPor exemplo, dados de clientes de uma empresa podem ser fundidos com dados de vendas, com base no n√∫mero de identifica√ß√£o de cliente.\n\n\n\n\n\n\nAn√°lise de correspond√™ncia: √© uma t√©cnica de integra√ß√£o de dados que permite combinar informa√ß√µes de diferentes fontes, com base em uma an√°lise de correspond√™ncia de vari√°veis.\n\nPor exemplo, dados de vendas de uma empresa podem ser combinados com dados demogr√°ficos, com base em correspond√™ncias de vari√°veis, como localiza√ß√£o geogr√°fica ou faixa et√°ria."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#integra√ß√£o-de-dados-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#integra√ß√£o-de-dados-2",
    "title": "Pr√©-processamento de dados",
    "section": "Integra√ß√£o de dados",
    "text": "Integra√ß√£o de dados\n\n\nüí° Independentemente da t√©cnica usada, √© importante lembrar que a integra√ß√£o de dados pode ser complexa e demorada e pode envolver a identifica√ß√£o e corre√ß√£o de inconsist√™ncias nos dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#redu√ß√£o-de-dados",
    "href": "aulas/Pre_processamento/pre_processamento.html#redu√ß√£o-de-dados",
    "title": "Pr√©-processamento de dados",
    "section": "Redu√ß√£o de dados",
    "text": "Redu√ß√£o de dados\nüí° A redu√ß√£o de dados √© uma t√©cnica usada para diminuir a dimens√£o de um conjunto de dados, ou seja, reduzir o n√∫mero de vari√°veis ou caracter√≠sticas que comp√µem o conjunto de dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#redu√ß√£o-de-dados-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#redu√ß√£o-de-dados-1",
    "title": "Pr√©-processamento de dados",
    "section": "Redu√ß√£o de dados",
    "text": "Redu√ß√£o de dados\n\n\nExistem v√°rias raz√µes para reduzir os dados:\n\n\nSimplificar a an√°lise de dados, tornando-a mais f√°cil e mais r√°pida de ser executada;\n\nReduzir o tempo de processamento e armazenamento dos dados;\n\nMelhorar a qualidade dos dados, eliminando vari√°veis irrelevantes ou redundantes que possam afetar negativamente a an√°lise;\n\nPreparar os dados para a entrada em um modelo de aprendizado de m√°quina."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#an√°lise-de-componentes-principais",
    "href": "aulas/Pre_processamento/pre_processamento.html#an√°lise-de-componentes-principais",
    "title": "Pr√©-processamento de dados",
    "section": "An√°lise de Componentes Principais",
    "text": "An√°lise de Componentes Principais"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#amostragem",
    "href": "aulas/Pre_processamento/pre_processamento.html#amostragem",
    "title": "Pr√©-processamento de dados",
    "section": "Amostragem",
    "text": "Amostragem"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#sele√ß√£o-de-atributos",
    "href": "aulas/Pre_processamento/pre_processamento.html#sele√ß√£o-de-atributos",
    "title": "Pr√©-processamento de dados",
    "section": "Sele√ß√£o de atributos",
    "text": "Sele√ß√£o de atributos\n\n\nSele√ß√£o de atributos √© um processo de pr√©-processamento de dados que envolve a escolha dos atributos (vari√°veis) mais relevantes para a an√°lise.\n\n\n\nO objetivo da sele√ß√£o de atributos √© reduzir a dimensionalidade dos dados, eliminando caracter√≠sticas irrelevantes e redundantes, e, assim, melhorar a precis√£o e a efici√™ncia da an√°lise."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#sele√ß√£o-de-atributos-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#sele√ß√£o-de-atributos-1",
    "title": "Pr√©-processamento de dados",
    "section": "Sele√ß√£o de atributos",
    "text": "Sele√ß√£o de atributos\nSele√ß√£o baseada em filtro: Essa t√©cnica usa m√©tricas estat√≠sticas para avaliar a relev√¢ncia dos atributos e seleciona aqueles que t√™m maior correla√ß√£o com a vari√°vel de interesse.\n\n\nM√©todos de filtragem: coeficiente de correla√ß√£o, ganho de informa√ß√£o e o teste de qui-quadrado."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#sele√ß√£o-de-atributos-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#sele√ß√£o-de-atributos-2",
    "title": "Pr√©-processamento de dados",
    "section": "Sele√ß√£o de atributos",
    "text": "Sele√ß√£o de atributos\nSele√ß√£o baseada em wrapper: Essa t√©cnica envolve a sele√ß√£o de atributos por meio de um modelo de aprendizado de m√°quina, que √© treinado em um conjunto de atributos selecionados. O modelo √© avaliado usando valida√ß√£o cruzada e as atributos s√£o selecionados com base no desempenho do modelo.\n\n\nM√©todos wrapper: Forward stepwise selection, Backward Elimination."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#sele√ß√£o-de-atributos-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#sele√ß√£o-de-atributos-3",
    "title": "Pr√©-processamento de dados",
    "section": "Sele√ß√£o de atributos",
    "text": "Sele√ß√£o de atributos\nSele√ß√£o baseada em incorpora√ß√£o: Essa t√©cnica envolve a sele√ß√£o de atributos durante o processo de treinamento do modelo de aprendizado de m√°quina. O modelo √© treinado com todas os atributos e, em seguida, os atributos s√£o removidos com base em sua import√¢ncia para o modelo.\n\n\nM√©todos de incorpora√ß√£o: Regress√£o LASSO, √Årvores de decis√£o‚Ä¶"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\n\nFonte: Giphy"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-1",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\nAs bases de dados brutas e integradas a partir de bases distintas podem sofrer, al√©m de valores ausentes, ru√≠dos e inconsist√™ncias, de dados n√£o ou pouco padronizados.\n\n\nPor exemplo, pode haver valores de um mesmo atributo escritos em mai√∫sculo e outros em min√∫sculo, e os formatos e as unidades podem ser diferentes.\n\n\n\n\nOutro tipo de problema encontrado √© a n√£o uniformidade dos atributos, ou seja, alguns atributos podem ser num√©ricos, outros categ√≥ricos, e os dom√≠nios de cada atributo podem ser muito diferentes."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-2",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\nComo podemos resolver esses problemas?\n\n\n\nPadroniza√ß√£o: Resolver as diferen√ßas de unidades e escalas dos dados.\n\n\nCapitaliza√ß√£o: √© usual padronizar as fontes, normalmente para mai√∫sculo.\n\nPadroniza√ß√£o de formatos: observar e padronizar o formato de cada atributo da base, principalmente quando diferentes bases precisam ser integradas.\n\nConvers√£o de unidades: todos os dados devem ser convertidos e padronizados em uma mesma unidade de medida.\n\nNormaliza√ß√£o: tornar os dados mais apropriados √† aplica√ß√£o de algum algoritmo de minera√ß√£o."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tipos-de-normaliza√ß√£o",
    "href": "aulas/Pre_processamento/pre_processamento.html#tipos-de-normaliza√ß√£o",
    "title": "Pr√©-processamento de dados",
    "section": "Tipos de normaliza√ß√£o",
    "text": "Tipos de normaliza√ß√£o\n\n\nNormaliza√ß√£o Max-Min: Realiza uma transforma√ß√£o linear nos dados originais\n\n\\[X' = \\dfrac{X - \\min(X)}{\\max(X) - \\min(X)}\\]\n\n\n\nNormaliza√ß√£o pelo escore-z: √ötil quando os valores m√°ximo e m√≠nimo de um atributo forem desconhecidos ou quando existe outliers.\n\n\\[X' = \\dfrac{X - \\bar{X}}{s_X}\\]"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#tipos-de-normaliza√ß√£o-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#tipos-de-normaliza√ß√£o-1",
    "title": "Pr√©-processamento de dados",
    "section": "Tipos de normaliza√ß√£o",
    "text": "Tipos de normaliza√ß√£o\n\n\nNormaliza√ß√£o pelo escalonamento decimal: Move a casa decimal de um atributo \\(X\\).\n\n\\[X' = \\dfrac{X}{10^j}\\]\nem que \\(j\\) √© o menor inteiro tal que \\(\\max(|X'|)&lt; 1\\).\n\n\n\nNormaliza√ß√£o pela dist√¢ncia interquartilica: Toma o valor do atributo, subtrai a mediana e divide pela dist√¢ncia interquart√≠lica (\\(DIQ = Q_3 - Q_1\\))\n\n\\[X' =  \\dfrac{X - Q_2}{DIQ}\\]"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#exemplo-de-uso-das-diferentes-normaliza√ß√µes",
    "href": "aulas/Pre_processamento/pre_processamento.html#exemplo-de-uso-das-diferentes-normaliza√ß√µes",
    "title": "Pr√©-processamento de dados",
    "section": "Exemplo de uso das diferentes normaliza√ß√µes",
    "text": "Exemplo de uso das diferentes normaliza√ß√µes\n\n\n\n\nID\n\n\nValor original\n\n\nMax-Min\n\n\nEscore-z\n\n\nEscalonamento decimal\n\n\nDist√¢ncia interquart√≠lica\n\n\n\n\n\n1\n\n\n67\n\n\n0,85\n\n\n0,73\n\n\n0,67\n\n\n0,40\n\n\n\n\n2\n\n\n43\n\n\n0,33\n\n\n-0,92\n\n\n0,43\n\n\n-0,80\n\n\n\n\n3\n\n\n58\n\n\n0,65\n\n\n0,11\n\n\n0,58\n\n\n-0,05\n\n\n\n\n4\n\n\n28\n\n\n0,00\n\n\n-1,96\n\n\n0,28\n\n\n-1,55\n\n\n\n\n5\n\n\n74\n\n\n1,00\n\n\n1,21\n\n\n0,74\n\n\n0,75\n\n\n\n\n6\n\n\n65\n\n\n0,80\n\n\n0,59\n\n\n0,65\n\n\n0,30\n\n\n\n\n7\n\n\n70\n\n\n0,91\n\n\n0,94\n\n\n0,70\n\n\n0,55\n\n\n\n\n8\n\n\n42\n\n\n0,30\n\n\n-0,99\n\n\n0,42\n\n\n-0,85\n\n\n\n\n9\n\n\n57\n\n\n0,63\n\n\n0,04\n\n\n0,57\n\n\n-0,10\n\n\n\n\n10\n\n\n60\n\n\n0,70\n\n\n0,25\n\n\n0,60\n\n\n0,05"
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-3",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\n\n\nüëâ Transforma√ß√£o logar√≠tmica: Usada para reduzir a assimetria e a vari√¢ncia dos dados.\n\n\n\n\n\n\n\n\n\n\n\n√â especialmente √∫til quando os dados est√£o distribu√≠dos de forma exponencial ou quando h√° um grande intervalo de valores entre as observa√ß√µes."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-4",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-4",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\nA transforma√ß√£o logar√≠tmica √© definida pela seguinte equa√ß√£o:\n\\[\ny = \\log(x)\n\\]\nonde \\(x\\) √© a vari√°vel a ser transformada e \\(y\\) √© a vari√°vel transformada.\n\nTrata-se de uma transforma√ß√£o monot√¥nica, isto √©, n√£o afeta a ordem dos dados. Isso √© importante para modelos preditivos porque garante que a transforma√ß√£o n√£o afete a capacidade do modelo de capturar a rela√ß√£o entre as vari√°veis."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-5",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-5",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\n\n\nüëâ Transforma√ß√£o de raiz quadrada: Similar √† transforma√ß√£o logar√≠tmica, com o benef√≠cio de manter a escala original dos dados, facilitando a interpreta√ß√£o.\n\n\n\n\n\n\n\n\n\n\n\nA transforma√ß√£o logar√≠tmica √© mais adequada para dados com caudas longas e uma grande varia√ß√£o, enquanto a transforma√ß√£o da raiz quadrada √© mais adequada para dados com uma varia√ß√£o moderada. Al√©m disso, a o uso de logar√≠tmos tem o efeito de estabilizar a vari√¢ncia dos dados, enquanto a o uso da raiz quadrada tem o efeito de reduzir a vari√¢ncia dos dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-6",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-6",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\nA transforma√ß√£o de raiz quadrada √© definida pela seguinte equa√ß√£o:\n\\[\ny = \\sqrt x\n\\]\nonde \\(x\\) √© a vari√°vel a ser transformada e \\(y\\) √© a vari√°vel transformada.\n\nAssim como a transforma√ß√£o logar√≠timica, a transforma√ß√£o de raiz quadrada √© uma transforma√ß√£o monot√¥nica, o que significa que n√£o afeta a ordem dos dados."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-7",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-7",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\n\n\nüëâ A transforma√ß√£o de Box-Cox √© uma t√©cnica para transformar vari√°veis em modelos preditivos que √© amplamente utilizada para melhorar a qualidade da modelagem e a precis√£o das previs√µes.\n\n\n\n\n\n\n\n\n\n\n\n√â uma t√©cnica param√©trica que depende de um par√¢metro lambda que √© estimado a partir dos dados, podendo ser usada apenas com vari√°veis n√£o negativas."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-8",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-8",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\nA transforma√ß√£o de Box-Cox √© definida pela seguinte equa√ß√£o:\n\\[\ny =\n  \\begin{cases}\n      \\dfrac{(x^\\lambda - 1)}{\\lambda}, & se \\ \\lambda \\neq  0 \\\\\n      \\log(x), & se \\ \\lambda = 0\n  \\end{cases}\n\\]\nonde \\(x\\) √© a vari√°vel a ser transformada e \\(y\\) √© a vari√°vel transformada.\n\nEsta √© uma t√©cnica de transforma√ß√£o de dados √∫til usada para estabilizar a vari√¢ncia, tornar os dados mais semelhantes √† distribui√ß√£o normal.\n\nPor exemplo, quando um atributo tem a apar√™ncia de uma curva normal mas est√° descolado para a direita ou esquerda."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-9",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-9",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\n\n\n üëâ A transforma√ß√£o Yeo-Johnson √© uma t√©cnica de transforma√ß√£o semelhante √† transforma√ß√£o de Box-Cox.\n\n\n\n\n\n\n\n\n\n\n\nNo entanto, a transforma√ß√£o Yeo-Johnson √© mais flex√≠vel porque pode ser usada para vari√°veis que podem assumir valores negativos."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-10",
    "href": "aulas/Pre_processamento/pre_processamento.html#transforma√ß√£o-de-dados-10",
    "title": "Pr√©-processamento de dados",
    "section": "Transforma√ß√£o de dados",
    "text": "Transforma√ß√£o de dados\nA transforma√ß√£o Yeo-Johnson √© definida pela seguinte equa√ß√£o:\n\\[\ny =\n  \\begin{cases}\n      \\dfrac{(x + 1)^\\lambda - 1}{\\lambda}, & se \\ x \\geq  0, \\ \\lambda \\neq 0 \\\\\n      \\ln(x + 1), & se \\ x \\geq 0 \\ \\lambda = 0 \\\\\n      \\dfrac{-((-x + 1)^{(2 - \\lambda)} - 1)}{(2 - \\lambda)}, & se \\ x &lt;  0, \\ \\lambda \\neq 2 \\\\\n      -\\ln(-x+1) & se \\ x &lt;  0, \\ \\lambda = 2\n  \\end{cases}\n\\]\nonde \\(x\\) √© a vari√°vel a ser transformada e \\(y\\) √© a vari√°vel transformada."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#discretiza√ß√£o-de-dados",
    "href": "aulas/Pre_processamento/pre_processamento.html#discretiza√ß√£o-de-dados",
    "title": "Pr√©-processamento de dados",
    "section": "Discretiza√ß√£o de dados",
    "text": "Discretiza√ß√£o de dados\nDiscretiza√ß√£o √© o processo de transformar uma vari√°vel cont√≠nua em uma vari√°vel categ√≥rica ou discreta.\n\n√â √∫til quando se deseja agrupar valores cont√≠nuos em categorias ou intervalos para simplificar a an√°lise ou reduzir o efeito de valores extremos."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#discretiza√ß√£o-de-dados-1",
    "href": "aulas/Pre_processamento/pre_processamento.html#discretiza√ß√£o-de-dados-1",
    "title": "Pr√©-processamento de dados",
    "section": "Discretiza√ß√£o de dados",
    "text": "Discretiza√ß√£o de dados\n\nT√©cnicas comuns de discretiza√ß√£o:\n\n\n\nDiscretiza√ß√£o equi-probabil√≠stica: Essa t√©cnica envolve a divis√£o dos dados em intervalos de tamanho igual, de modo que cada intervalo contenha aproximadamente a mesma quantidade de observa√ß√µes.\n\n\n\n\n\nDiscretiza√ß√£o equi-distante: Nessa t√©cnica, os dados s√£o divididos em intervalos de tamanho igual, com base em uma dist√¢ncia predefinida."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#discretiza√ß√£o-de-dados-2",
    "href": "aulas/Pre_processamento/pre_processamento.html#discretiza√ß√£o-de-dados-2",
    "title": "Pr√©-processamento de dados",
    "section": "Discretiza√ß√£o de dados",
    "text": "Discretiza√ß√£o de dados\n\n\n\nDiscretiza√ß√£o por quartis: Os dados s√£o divididos em quartis, com base nos valores de corte que dividem os dados em quatro partes iguais.\n\n\n\n\nDiscretiza√ß√£o por frequ√™ncia: Os dados s√£o divididos em intervalos com base na frequ√™ncia de observa√ß√µes em cada intervalo."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#discretiza√ß√£o-de-dados-3",
    "href": "aulas/Pre_processamento/pre_processamento.html#discretiza√ß√£o-de-dados-3",
    "title": "Pr√©-processamento de dados",
    "section": "Discretiza√ß√£o de dados",
    "text": "Discretiza√ß√£o de dados\n\n\nBinariza√ß√£o: A binariza√ß√£o √© uma t√©cnica de transforma√ß√£o de dados que converte valores quantitativos em valores bin√°rios (0 ou 1) com base em um valor de corte."
  },
  {
    "objectID": "aulas/Pre_processamento/pre_processamento.html#references",
    "href": "aulas/Pre_processamento/pre_processamento.html#references",
    "title": "Pr√©-processamento de dados",
    "section": "References",
    "text": "References\n\n\nMadow, W. G., H. Nisselson, I. Olkin, D. B. Rubin, National Research Council (U.S.). Panel on Incomplete Data, Assembly of Behavioral, and Social Sciences (U.S.). Panel on Incomplete Data. 1983. Incomplete Data in Sample Surveys: Theory and Bibliographies. Incomplete Data in Sample Surveys. Academic Press."
  },
  {
    "objectID": "cronograma.html",
    "href": "cronograma.html",
    "title": "Cronograma da Disciplina",
    "section": "",
    "text": "Esta p√°gina cont√©m um esbo√ßo dos t√≥picos, conte√∫dos e tarefas para o semestre. Este cronograma ser√° atualizado conforme o semestre avan√ßa.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemana\nData\nT√≥pico\nArtigo\nSlides\nEC\nLE\nScript\nMC\nProjeto\n\n\n\n\n1\nTer, 06/05\nApresenta√ß√£o da disciplina\n\n\n\n\n\n\n\n\n\n\nQui, 08/05\nN√£o haver√° aula\n\n\n\n\n\n\n\n\n\n2\nTer, 13/05\nConceitos iniciais\n\n\n\n\n\n\n\n\n\n\nQui, 15/05\nConceitos iniciais\n\n\n\n\n\n\n\n\n\n3\nTer, 20/05\nPr√©-processamento de dados\n\n\n\n\n\n\n\n\n\n\nQui, 22/05\nN√£o haver√° aula!\n\n\n\n\n\n\n\n\n\n4\nTer, 27/05\nPr√©-processamento de dados\n\n\n\n\n\n\n\n\n\n\nQui, 29/05\nPr√©-processamento de dados\n\n\n\n\n\n\n\n\n\n5\nTer, 03/06\nPr√©-processamento usando a base de dados Space Titanic\n\n\n\n\n\n\n\n\n\n\nQui, 05/06\nAn√°lise Preditiva\n\n\n\n\n\n\n\n\n\n6\nTer, 10/06\nAn√°lise Preditiva\n\n\n\n\n\n\n\n\n\n\nQui, 12/06\nAn√°lise Preditiva\n\n\n\n\n\n\n\n\n\n7\nTer, 17/06\nAn√°lise Preditiva\n\n\n\n\n\n\n\n\n\n\nQui, 19/06\nFeriado: n√£o haver√° aula\n\n\n\n\n\n\n\n\n\n8\nTer, 24/06\nAn√°lise Preditiva\n\n\n\n\n\n\n\n\n\n\nQui, 26/06\nAn√°lise Preditiva",
    "crumbs": [
      "Cronograma da Disciplina"
    ]
  },
  {
    "objectID": "material.html",
    "href": "material.html",
    "title": "Material Complementar",
    "section": "",
    "text": "Desvendando o pacote tidymodels\nDesbalanceamento de Classes: Um problema real em Ci√™ncia de Dados\nVazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel",
    "crumbs": [
      "Material Complementar"
    ]
  },
  {
    "objectID": "material_complementar/tidymodels.html",
    "href": "material_complementar/tidymodels.html",
    "title": "Desvendando o pacote tidymodels",
    "section": "",
    "text": "Hoje vamos desvendar um dos pacotes mais poderosos e vers√°teis no universo do R para quem trabalha com Ci√™ncia de Dados: o tidymodels. Se voc√™ j√° se aventurou em construir modelos preditivos, sabe que o processo pode ser um pouco‚Ä¶ artesanal. O tidymodels chega para organizar essa bagun√ßa e transformar a constru√ß√£o de modelos em algo mais intuitivo, padronizado e, claro, tidy!"
  },
  {
    "objectID": "material_complementar/tidymodels.html#o-que-√©-o-tidymodels",
    "href": "material_complementar/tidymodels.html#o-que-√©-o-tidymodels",
    "title": "Desvendando o pacote tidymodels",
    "section": "O que √© o tidymodels?",
    "text": "O que √© o tidymodels?\nPense no tidymodels como uma cole√ß√£o de pacotes que trabalham em conjunto para oferecer uma estrutura unificada e consistente para o machine learning em R. Assim como o tidyverse revolucionou a manipula√ß√£o de dados, o tidymodels faz o mesmo para a modelagem. Ele segue a filosofia tidy do R, o que significa que as fun√ß√µes s√£o projetadas para serem encadeadas, facilitando a leitura e a escrita do c√≥digo.\nEle cobre todas as etapas do fluxo de trabalho de machine learning, desde a prepara√ß√£o dos dados at√© a avalia√ß√£o do modelo, passando pela sele√ß√£o de modelos e ajuste de hiperpar√¢metros."
  },
  {
    "objectID": "material_complementar/tidymodels.html#por-que-usar-o-tidymodels",
    "href": "material_complementar/tidymodels.html#por-que-usar-o-tidymodels",
    "title": "Desvendando o pacote tidymodels",
    "section": "Por que usar o tidymodels?",
    "text": "Por que usar o tidymodels?\n\nConsist√™ncia: Esque√ßa a necessidade de aprender sintaxes diferentes para cada algoritmo. O tidymodels oferece uma interface unificada.\nOrganiza√ß√£o: Ele incentiva a cria√ß√£o de um fluxo de trabalho claro e modular, o que facilita a replica√ß√£o e a manuten√ß√£o do seu c√≥digo.\nFlexibilidade: Embora padronizado, ele √© incrivelmente flex√≠vel, permitindo que voc√™ experimente diferentes modelos e abordagens.\nIntegra√ß√£o: Nascido e criado no ecossistema tidy, ele se integra perfeitamente com pacotes como dplyr e ggplot2."
  },
  {
    "objectID": "material_complementar/tidymodels.html#componentes-chave-do-tidymodels",
    "href": "material_complementar/tidymodels.html#componentes-chave-do-tidymodels",
    "title": "Desvendando o pacote tidymodels",
    "section": "Componentes Chave do tidymodels",
    "text": "Componentes Chave do tidymodels\nO tidymodels √© composto por diversos pacotes que desempenham fun√ß√µes espec√≠ficas. Os principais que voc√™ precisa conhecer s√£o:\n\nrsample: Para criar amostras de dados (treino/teste, valida√ß√£o cruzada).\nrecipes: Para pr√©-processamento de dados (transforma√ß√µes, engenharia de features).\nparsnip: Para especificar e ajustar diferentes tipos de modelos (regress√£o linear, √°rvores, SVMs, etc.) com uma sintaxe consistente.\ntune: Para ajuste de hiperpar√¢metros de modelos.\nworkflows: Para empacotar modelos e recipes em um √∫nico objeto.\nyardstick: Para medir o desempenho do modelo com diversas m√©tricas.\ndials: Para gerenciar espa√ßos de tuning de hiperpar√¢metros."
  },
  {
    "objectID": "material_complementar/tidymodels.html#m√£os-na-massa-um-exemplo-pr√°tico",
    "href": "material_complementar/tidymodels.html#m√£os-na-massa-um-exemplo-pr√°tico",
    "title": "Desvendando o pacote tidymodels",
    "section": "M√£os na Massa: Um Exemplo Pr√°tico!",
    "text": "M√£os na Massa: Um Exemplo Pr√°tico!\nO conjunto de dados da Ames Housing √© um conjunto de dados bem conhecido no campo da aprendizagem de m√°quina e an√°lise de dados. Ele cont√©m v√°rios recursos e atributos de casas residenciais em Ames, Iowa, EUA. O conjunto de dados √© frequentemente usado para tarefas de regress√£o, principalmente para prever os pre√ßos da habita√ß√£o.\nPrimeiro, vamos carregar os pacotes necess√°rios:\n\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Configura√ß√µes para reprodutibilidade\nset.seed(123)\n\n\n1. Prepara√ß√£o dos Dados com rsample\nO pacote rsample √© o seu ponto de partida para a divis√£o estrat√©gica dos seus dados. √â fundamental para garantir que seu modelo seja avaliado de forma imparcial e que generalize bem para dados novos.\n\n\nPor que usar?\n\nSepara√ß√£o Treino/Teste: Essencial para avaliar a capacidade de generaliza√ß√£o do modelo. O modelo √© treinado apenas nos dados de treino e avaliado no conjunto de teste ‚Äúinvis√≠vel‚Äù.\nValida√ß√£o Cruzada (Cross-Validation): Uma t√©cnica mais robusta para estimar o desempenho do modelo e ajustar hiperpar√¢metros. Ajuda a reduzir a vari√¢ncia da estimativa de desempenho em compara√ß√£o com uma √∫nica divis√£o treino/teste.\nReamostragem (Bootstrapping): √ötil para estimar a incerteza das estimativas do modelo.\n\n\n\nComo usar?\ninitial_split(): Para dividir seus dados em conjuntos de treino e teste.\n\ndata(ames)\n\n# Cria um conjunto de dados tibble para o tidymodels\names_tbl &lt;- as_tibble(ames)\n\n# Divis√£o dos dados em treino e teste\nsplit_data &lt;- initial_split(ames_tbl, prop = 0.80)\ntrain_data &lt;- training(split_data)\ntest_data  &lt;- testing(split_data)\n\n# Verificando as dimens√µes\ndim(train_data)\ndim(test_data)\n\nvfold_cv(): Para criar conjuntos para valida√ß√£o cruzada.\n\names_folds &lt;- vfold_cv(train_data, v = 10, strata = Sale_Price) # 10-fold cross-validation\names_folds\n\n\n\n2. Pr√©-processamento e Engenharia de Features com recipes\nO pacote recipes oferece uma sintaxe declarativa e encade√°vel para definir todas as etapas de pr√©-processamento e engenharia de features que voc√™ deseja aplicar aos seus dados.\n\n\nPor que usar?\n\nPadroniza√ß√£o e Consist√™ncia: Garante que as mesmas transforma√ß√µes sejam aplicadas consistentemente nos conjuntos de treino, teste e novos dados.\nEvitar Vazamento de Dados (Data Leakage): As opera√ß√µes de pr√©-processamento (como normaliza√ß√£o) s√£o ‚Äútreinadas‚Äù apenas nos dados de treino e depois aplicadas aos dados de teste, evitando que informa√ß√µes do conjunto de teste influenciem o pr√©-processamento.\nReplicabilidade: Facilita a reprodu√ß√£o de todas as etapas de prepara√ß√£o dos dados.\nEngenharia de Features: Permite criar novas vari√°veis a partir das existentes (e.g., intera√ß√µes, polin√¥mios).\n\n\n\nComo usar?\n\nrecipe(formula, data): Define a base da receita.\nstep_*(): Adiciona as etapas de pr√©-processamento.\n\nstep_normalize(): Normaliza vari√°veis num√©ricas (m√©dia 0, desvio padr√£o 1).\nstep_dummy(): Cria vari√°veis dummy (one-hot encoding) para vari√°veis categ√≥ricas.\nstep_impute_mean() / step_impute_knn(): Lida com valores ausentes.\nstep_log(): Aplica transforma√ß√£o logar√≠tmica.\nstep_other(): Agrupa n√≠veis raros de fatores em uma categoria ‚Äúoutros‚Äù.\nstep_interact(): Cria intera√ß√µes entre vari√°veis.\nstep_zv(): Remove vari√°veis com vari√¢ncia zero.\n\nMais steps do pacote recipes podem ser encontrados aqui.\n\n\names_recipe &lt;-\n  recipe(Sale_Price ~ ., data = ames) %&gt;%\n  step_log(Sale_Price, base = 10) %&gt;% # Transformar a vari√°vel resposta\n  step_other(Neighborhood, threshold = 0.05) %&gt;% # Agrupar bairros raros\n  step_dummy(all_nominal_predictors()) %&gt;% # Dummy para todas as vari√°veis categ√≥ricas\n  step_impute_knn(all_predictors()) %&gt;% # Imputar valores ausentes com KNN\n  step_normalize(all_numeric_predictors()) %&gt;%  # Normalizar vari√°veis num√©ricas\n  step_zv(all_predictors()) # Remover vari√°veis com vari√¢ncia zero\n  \n# Uma receita √© uma \"planta\". Para aplic√°-la, voc√™ precisa `prep()` e `bake()`.\n# `prep()` calcula as estat√≠sticas necess√°rias para as transforma√ß√µes (e.g., m√©dia para normaliza√ß√£o).\n# `bake()` aplica essas transforma√ß√µes aos dados.\nprepared_recipe &lt;- prep(ames_recipe, training = ames)\nbaked_data &lt;- bake(prepared_recipe, new_data = ames)\n\n\n\n3. Especifica√ß√£o do Modelo com parsnip\nO pacote parsnip fornece uma interface consistente para especificar diferentes tipos de modelos de machine learning, independentemente do ‚Äúmotor‚Äù (o pacote R que realmente implementa o algoritmo) que voc√™ deseja usar.\n\n\nPor que usar?\n\nInterface Unificada: Voc√™ usa a mesma sintaxe para especificar uma regress√£o linear, uma √°rvore de decis√£o, um SVM, uma rede neural, etc.\nFlexibilidade de Motores: Permite alternar facilmente entre diferentes implementa√ß√µes do mesmo algoritmo (e.g., glmnet ou keras para regress√£o log√≠stica).\nSepara√ß√£o de Preocupa√ß√µes: Voc√™ define o tipo de modelo e seu modo (classifica√ß√£o/regress√£o) antes de se preocupar com os detalhes do fitting.\n\n\n\nComo usar?\n\nmodel_type(): Fun√ß√£o para o tipo de modelo (e.g., linear_reg(), rand_forest(), boost_tree(), svm_rbf()).\nset_engine(): Define qual pacote ser√° usado para implementar o modelo (e.g., \"lm\", \"ranger\", \"xgboost\", \"kernlab\").\nset_mode(): Define se √© um problema de \"regression\" ou \"classification\".\nset_args(): Define hiperpar√¢metros que podem ser ajustados ou passados diretamente (e.g., trees = 1000 para rand_forest).\n\n\n# Regress√£o Linear\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# √Årvore de Decis√£o de Classifica√ß√£o\ntree_spec &lt;- decision_tree(cost_complexity = tune()) %&gt;% # `tune()` indica um hiperpar√¢metro para ajuste\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\n# Especificar Random Forest com mtry para ajuste\nrf_spec &lt;- rand_forest(mtry = tune(), trees = 1000, min_n = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\n\n4. workflows: Combinando recipes e parsnip\nO pacote workflows √© a ‚Äúcola‚Äù do tidymodels. Ele permite que voc√™ empacote uma recipe e uma especifica√ß√£o de modelo em um √∫nico objeto, tornando o processo de fitting e previs√£o mais eficiente e menos propenso a erros.\n\n\nPor que usar?\n\nConsolida√ß√£o: Re√∫ne todas as etapas de pr√©-processamento e o modelo em um √∫nico objeto coerente.\nFacilita o fit() e predict(): Voc√™ treina e faz previs√µes no workflow como um todo, em vez de gerenciar a recipe e o modelo separadamente.\nIntegridade: Garante que o pr√©-processamento correto seja aplicado antes do treinamento e das previs√µes.\n\n\n\nComo usar?\n\nworkflow(): Inicia o objeto de workflow.\nadd_recipe(): Adiciona a recipe criada.\nadd_model(): Adiciona a especifica√ß√£o do modelo.\n\n\n# Usando a recipe e o modelo da sess√£o anterior\names_workflow &lt;-\n  workflow() %&gt;%\n  add_recipe(ames_recipe) %&gt;%\n  add_model(rf_spec)\n\names_workflow # Visualize o workflow\n\n\n\n5. Ajuste de Hiperpar√¢metros com tune (Opcional, mas Altamente Recomendado!)\nO pacote tune √© o cora√ß√£o do tidymodels para encontrar os melhores hiperpar√¢metros para o seu modelo. Ele trabalha em conjunto com rsample para realizar o tuning de forma sistem√°tica.\n\n\nPor que usar?\n\nOtimiza√ß√£o do Modelo: Encontrar os hiperpar√¢metros ideais pode melhorar significativamente o desempenho do seu modelo.\nMetodologias Robustas: Suporta tuning por grade (tune_grid) e tuning Bayesiano (tune_bayes), que s√£o m√©todos eficientes para explorar o espa√ßo de hiperpar√¢metros.\nValida√ß√£o Cruzada Integrada: O tune se integra perfeitamente com os folds de valida√ß√£o cruzada do rsample para avaliar o desempenho de cada combina√ß√£o de hiperpar√¢metros de forma robusta.\n\n\n\nComo usar?\n\nDefina os hiperpar√¢metros a serem ajustados com tune() na especifica√ß√£o do modelo (parsnip).\ntune_grid(): Realiza uma busca em grade (grid search) sobre um conjunto predefinido de hiperpar√¢metros.\ntune_bayes(): Realiza uma busca Bayesiana, que √© mais eficiente para espa√ßos de hiperpar√¢metros grandes.\ncollect_metrics(): Coleta as m√©tricas de desempenho para cada combina√ß√£o de hiperpar√¢metros.\nselect_best() / select_by_one_std_err(): Ajuda a escolher o melhor conjunto de hiperpar√¢metros.\n\n\n# Realizar o ajuste de hiperpar√¢metros\ntune_results &lt;-\n  ames_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = 10 # Tentar 10 combina√ß√µes aleat√≥rias de mtry e min_n\n  )\n\n# Analisar os resultados do tuning\ntune_results %&gt;% collect_metrics()\ntune_results %&gt;% show_best(metric =\"rmse\")\n\n# Selecionar o melhor modelo e finalizar o workflow\nbest_params &lt;- tune_results %&gt;% select_best(metric =\"rmse\")\nfinal_workflow &lt;- ames_workflow %&gt;% finalize_workflow(best_params)\n\n\n\n6. Treinamento Final do Modelo com fit\nAp√≥s todas as etapas de prepara√ß√£o e, opcionalmente, ajuste de hiperpar√¢metros, fit() √© a fun√ß√£o que treina o modelo final nos dados de treino completos (ou no conjunto de dados completo se voc√™ estiver fazendo um treinamento final antes da implanta√ß√£o).\n\n\nPor que usar?\n\nCria o Modelo Final: √â o passo onde o algoritmo de machine learning aprende os padr√µes dos dados.\nIntegra√ß√£o com o Workflow: Treina o workflow completo, garantindo que a recipe seja preparada e aplicada aos dados de treino antes do modelo ser ajustado.\n\n\n\nComo usar?\n\nfit(workflow, data): Treina o workflow com os dados especificados.\n\n\n# Usando o workflow finalizado do tuning (ou o workflow original se n√£o houver tuning)\nfinal_model &lt;- fit(final_workflow, data = train_data)\n\n\n\n7. Fazendo Previs√µes com predict\nA fun√ß√£o predict() √© usada para gerar previs√µes em novos dados, sejam eles o conjunto de teste ou dados de produ√ß√£o.\n\n\nPor que usar?\n\nAvalia√ß√£o de Desempenho: Previs√µes no conjunto de teste s√£o cruciais para avaliar o qu√£o bem o modelo generaliza.\nAplica√ß√£o em Produ√ß√£o: Uma vez que o modelo √© implantado, predict() √© usado para gerar novas previs√µes.\n\n\n\nComo usar?\n-predict(trained_model, new_data): Gera previs√µes para um novo conjunto de dados.\n\npredictions &lt;- predict(final_model, new_data = test_data) %&gt;%\n  bind_cols(test_data) # Juntando com os valores reais para avalia√ß√£o\n\n\n\n8. Avalia√ß√£o de Desempenho com yardstick\nO pacote yardstick fornece uma ampla gama de m√©tricas para avaliar o desempenho do seu modelo, tanto para problemas de regress√£o quanto de classifica√ß√£o.\n\n\nPor que usar?\n\nM√©tricas Abrangentes: Oferece m√©tricas padr√£o e avan√ßadas (e.g., RMSE, R-quadrado, AUC, acur√°cia, precis√£o, recall, F1-score).\nConsist√™ncia: As fun√ß√µes de m√©tricas t√™m uma interface consistente.\nPronto para dplyr: As fun√ß√µes metric_set() e metrics() se integram bem com o tidyverse.\n\n\n\nComo usar?\n\nmetric_set(): Cria um conjunto de m√©tricas para avalia√ß√£o.\nmetrics(data, truth, estimate): Calcula as m√©tricas para um conjunto de dados.\n\n\n# Para regress√£o\npredictions %&gt;%\n  metrics(truth = Sale_Price, estimate = .pred)\n\n# Para classifica√ß√£o \n# classification_metrics &lt;- metric_set(accuracy, roc_auc, sens, spec)\n# predictions_class %&gt;% classification_metrics(truth = actual_class, estimate = .pred_class, .pred_positive_prob)\n\n\n\nConclus√£o\nEntender cada uma dessas etapas e o papel de cada pacote no tidymodels √© a chave para construir fluxos de trabalho de machine learning eficientes, robustos e reproduz√≠veis em R. A beleza est√° na modularidade e na capacidade de encadear essas opera√ß√µes, transformando um processo complexo em algo sistem√°tico e elegante.\nAo dominar essas etapas, voc√™ estar√° bem equipado para enfrentar uma variedade de desafios de modelagem de dados!"
  },
  {
    "objectID": "plano.html",
    "href": "plano.html",
    "title": "Plano de Ensino",
    "section": "",
    "text": "Leia com aten√ß√£o o plano de ensino da disciplina que ser√° oferecida neste per√≠odo. Nele est√£o as regras do jogo.\n\nPlano de ensino",
    "crumbs": [
      "Plano de Ensino"
    ]
  },
  {
    "objectID": "programacao/semana-2.html",
    "href": "programacao/semana-2.html",
    "title": "Semana 02",
    "section": "",
    "text": "Conceitos iniciais"
  },
  {
    "objectID": "programacao/semana-2.html#slides",
    "href": "programacao/semana-2.html#slides",
    "title": "Semana 02",
    "section": "",
    "text": "Conceitos iniciais"
  },
  {
    "objectID": "programacao/semana-4.html",
    "href": "programacao/semana-4.html",
    "title": "Semana 04",
    "section": "",
    "text": "Pr√©-processamento de dados"
  },
  {
    "objectID": "programacao/semana-4.html#slides",
    "href": "programacao/semana-4.html#slides",
    "title": "Semana 04",
    "section": "",
    "text": "Pr√©-processamento de dados"
  },
  {
    "objectID": "programacao/semana-6.html",
    "href": "programacao/semana-6.html",
    "title": "Semana 06",
    "section": "",
    "text": "Desbalanceamento de Classes: Um problema real em Ci√™ncia de Dados"
  },
  {
    "objectID": "programacao/semana-6.html#material-complementar",
    "href": "programacao/semana-6.html#material-complementar",
    "title": "Semana 06",
    "section": "",
    "text": "Desbalanceamento de Classes: Um problema real em Ci√™ncia de Dados"
  },
  {
    "objectID": "programacao/semana-6.html#slides",
    "href": "programacao/semana-6.html#slides",
    "title": "Semana 06",
    "section": "Slides",
    "text": "Slides\n An√°lise preditiva"
  },
  {
    "objectID": "projeto/projeto_avaliativo.html",
    "href": "projeto/projeto_avaliativo.html",
    "title": "Projeto Avaliativo: An√°lise de churn em uma Empresa de Telecomunica√ß√µes",
    "section": "",
    "text": "A ConecteMais Corp.¬†√© uma das maiores empresas de telecomunica√ß√µes da Calif√≥rnia, oferecendo servi√ßos de internet, telefonia fixa, telefonia m√≥vel e TV por assinatura. Nos √∫ltimos meses, a empresa tem notado um aumento preocupante na taxa de churn de clientes, especialmente em determinados segmentos. Isso impacta diretamente a receita e a sustentabilidade do neg√≥cio. A diretoria executiva solicitou uma reuni√£o com a equipe de an√°lise de dados para entender melhor o problema e definir estrat√©gias para reduzir o churn.\n\n\nComo parte do time de An√°lise de Dados da ConecteMais, sua equipe foi convocada para participar de uma reuni√£o estrat√©gica de emerg√™ncia com os principais executivos da empresa.\nO objetivo? Descobrir por que os clientes est√£o deixando a empresa e ajudar a prever quem corre risco de sair no futuro.\nSua miss√£o ser√° dividida em fases:"
  },
  {
    "objectID": "projeto/projeto_avaliativo.html#cen√°rio-a-empresa-conectemais-corp.",
    "href": "projeto/projeto_avaliativo.html#cen√°rio-a-empresa-conectemais-corp.",
    "title": "Projeto Avaliativo: An√°lise de churn em uma Empresa de Telecomunica√ß√µes",
    "section": "",
    "text": "A ConecteMais Corp.¬†√© uma das maiores empresas de telecomunica√ß√µes da Calif√≥rnia, oferecendo servi√ßos de internet, telefonia fixa, telefonia m√≥vel e TV por assinatura. Nos √∫ltimos meses, a empresa tem notado um aumento preocupante na taxa de churn de clientes, especialmente em determinados segmentos. Isso impacta diretamente a receita e a sustentabilidade do neg√≥cio. A diretoria executiva solicitou uma reuni√£o com a equipe de an√°lise de dados para entender melhor o problema e definir estrat√©gias para reduzir o churn.\n\n\nComo parte do time de An√°lise de Dados da ConecteMais, sua equipe foi convocada para participar de uma reuni√£o estrat√©gica de emerg√™ncia com os principais executivos da empresa.\nO objetivo? Descobrir por que os clientes est√£o deixando a empresa e ajudar a prever quem corre risco de sair no futuro.\nSua miss√£o ser√° dividida em fases:"
  },
  {
    "objectID": "projeto/projeto_avaliativo.html#fase-1-a-reuni√£o-de-alinhamento",
    "href": "projeto/projeto_avaliativo.html#fase-1-a-reuni√£o-de-alinhamento",
    "title": "Projeto Avaliativo: An√°lise de churn em uma Empresa de Telecomunica√ß√µes",
    "section": "üó™ Fase 1: A Reuni√£o de Alinhamento",
    "text": "üó™ Fase 1: A Reuni√£o de Alinhamento\n\nParticipantes da Reuni√£o\n\nüìä L√≠der da Equipe de Neg√≥cios (Marketing e Vendas): Sra. Ana Paula, Diretora de Estrat√©gias de Clientes.\n\n‚ÄúSabemos que estamos perdendo clientes, mas n√£o sabemos exatamente o porqu√™. Precisamos de uma an√°lise s√≥lida, baseada em evid√™ncias, que nos ajude a agir com anteced√™ncia.‚Äù\n\nüíº Jo√£o Silva (Gerente de Marketing):\n\n‚ÄúJ√° testamos promo√ß√µes, descontos, campanhas de fideliza√ß√£o e novos planos. Mas algo ainda n√£o est√° funcionando. Ser√° que os clientes est√£o insatisfeitos e n√£o estamos enxergando os sinais a tempo?‚Äù\n\nüñ•Ô∏è Sr.¬†Mauro Bastos ‚Äì Especialista em Dados e Infraestrutura\n\n‚ÄúSou respons√°vel pelo banco de dados e pela governan√ßa das informa√ß√µes da ConecteMais. Organizei os dados de clientes com base em nosso sistema CRM e em registros de cobran√ßa, suporte e servi√ßos. Voc√™s ter√£o acesso √† base com todas as vari√°veis explicadas no dicion√°rio. Qualquer d√∫vida t√©cnica ou limita√ß√£o da base, falem comigo.‚Äù\n\nüíª L√≠der da Equipe de An√°lise de Dados: Dr.¬†Carlos Eduardo, Gerente de Ci√™ncia de Dados.\n\n‚ÄúNossa miss√£o √© identificar padr√µes nos dados que expliquem a rotatividade e desenvolver um modelo preditivo para ajudar a ConecteMais a antecipar e prevenir perdas de clientes. Para isso, precisamos primeiro entender o neg√≥cio, o contexto e a estrutura dos dados dispon√≠veis.‚Äù\n\n\nProblema Discutido\nüéô A Sra. Ana Paula inicia a reuni√£o expressando a preocupa√ß√£o da diretoria com o aumento do churn. Ela apresenta alguns dados gerais:\n\nA taxa de churn mensal passou de 15% para 26.5% nos √∫ltimos 6 meses.\nFoi observado um aumento de reclama√ß√µes em m√≠dias sociais e no atendimento ao cliente.\nH√° uma percep√ß√£o de que a concorr√™ncia est√° lan√ßando pacotes mais agressivos.\n\nEla enfatiza que a empresa precisa identificar proativamente os clientes com maior probabilidade de churn para que a equipe de marketing possa intervir com ofertas personalizadas, suporte proativo ou outras estrat√©gias de reten√ß√£o.\nüéô Sr.¬†Mauro Bastos (abrindo o notebook e conectando ao projetor):\n‚ÄúPara ajudar voc√™s, preparei uma base limpa com os principais dados de clientes. Voc√™s ter√£o acesso ao hist√≥rico de contratos, tipos de servi√ßo, cobran√ßas, reclama√ß√µes e se o cliente deixou ou n√£o a empresa. O dicion√°rio de dados est√° dispon√≠vel e cada vari√°vel j√° foi brevemente descrita. Refor√ßo que voc√™s devem validar a qualidade dos dados ‚Äì h√° campos com aus√™ncias e outras poss√≠veis inconsist√™ncias.‚Äù\nüéô Sr.¬†Mauro Bastos (acrescenta):\n‚ÄúCaso precisem de vari√°veis adicionais, posso consultar os sistemas internos para ver o que pode ser exportado. Mas comecem com este conjunto. Ele cobre 99% das sa√≠das registradas no √∫ltimo trimestre.‚Äù\n\n\nObjetivos Definidos\nVoc√™ e seu grupo participar√£o de uma reuni√£o com o gerente de ci√™ncia de dados da empresa. Nessa etapa, voc√™s dever√£o:\n\nCompreender o problema de neg√≥cios;\nLevantar hip√≥teses sobre fatores que influenciam a sa√≠da de clientes;\nIdentificar a vari√°vel-alvo: Churn Label\n\n\n\nBase de dados\nA base de dados de rotatividade de clientes da ConecteMais cont√™m informa√ß√µes sobre servi√ßos de telefone residencial e internet fornecidos a mais de 6 mil clientes na Calif√≥rnia no terceiro trimestre. Ela indica quais clientes sa√≠ram, permaneceram ou se inscreveram para nesses servi√ßos. V√°rias informa√ß√µes demogr√°ficas importantes s√£o inclu√≠das para cada cliente, bem como um score de satisfa√ß√£o, score de rotatividade e um √çndice de Valor de Tempo de Vida do Cliente (CLTV).\n\nDescri√ß√£o das vari√°veis:\n\nCustomerID: Um ID exclusivo que identifica cada cliente.\nGender: O g√™nero do cliente: Male (Masculino), Female (Feminino).\nAge: A idade atual do cliente, em anos, no momento em que o trimestre fiscal terminou.\nUnder 30: Indica se o cliente tem menos de 30 anos: Yes (Sim), No (N√£o).\nSenior Citizen: Indica se o cliente tem 65 anos ou mais: Yes (Sim), No (N√£o).\nMarried: Indica se o cliente √© casado: Yes (Sim), No (N√£o).\nDependents: Indica se o cliente mora com algum dependente: Yes (Sim), No (N√£o). Dependentes podem ser filhos, pais, av√≥s, etc.\nNumber of Dependents: Indica o n√∫mero de dependentes que moram com o cliente.\nCount: Um valor usado em relat√≥rios/dashboards para somar o n√∫mero de clientes em um conjunto filtrado.\nCountry: O pa√≠s de resid√™ncia principal do cliente.\nState: O estado de resid√™ncia principal do cliente.\nCity: A cidade de resid√™ncia principal do cliente.\nZip Code: O c√≥digo postal de resid√™ncia principal do cliente.\nLatitude: A latitude de resid√™ncia principal do cliente.\nLongitude: A longitude de resid√™ncia principal do cliente.\nPopulation: Uma estimativa populacional atual para toda a √°rea do c√≥digo postal.\nQuarter: O trimestre fiscal do qual os dados foram derivados (por exemplo, Q3).\nReferred a Friend: Indica se o cliente j√° indicou um amigo ou membro da fam√≠lia para esta empresa: Yes (Sim), No (N√£o).\nNumber of Referrals: Indica o n√∫mero de indica√ß√µes que o cliente fez at√© o momento.\nTenure in Months: Indica o n√∫mero total de meses que o cliente esteve com a empresa at√© o final do trimestre especificado acima.\nOffer: Identifica a √∫ltima oferta de marketing que o cliente aceitou, se aplic√°vel. Os valores incluem None (Nenhum), Offer A, Offer B, Offer C, Offer D e Offer E.\nPhone Service: Indica se o cliente assina o servi√ßo de telefone residencial com a empresa: Yes (Sim), No (N√£o).\nAvg Monthly Long Distance Charges: Indica as cobran√ßas m√©dias de chamadas de longa dist√¢ncia do cliente, calculadas at√© o final do trimestre especificado acima.\nMultiple Lines: Indica se o cliente assina v√°rias linhas telef√¥nicas com a empresa: Yes (Sim), No (N√£o).\nInternet Service: Indica se o cliente assina o servi√ßo de Internet com a empresa: Yes (Sim), No (N√£o).\nInternet Type: Indica o servi√ßo de Internet contradado junto a empresa: None (Nenhum), DSL, Fiber Optic (Fibra √ìptica), Cable (Cabo).\nAvg Monthly GB Download: Indica o volume m√©dio de download do cliente em gigabytes, calculado at√© o final do trimestre especificado acima.\nOnline Security: Indica se o cliente assina um servi√ßo adicional de seguran√ßa online fornecido pela empresa: Yes (Sim), No (N√£o).\nOnline Backup: Indica se o cliente assina um servi√ßo adicional de backup online fornecido pela empresa: Yes (Sim), No (N√£o).\nDevice Protection Plan: Indica se o cliente assina um plano adicional de prote√ß√£o de dispositivo para seu equipamento de Internet fornecido pela empresa: Yes (Sim), No (N√£o).\nPremium Tech Support: Indica se o cliente assina um plano adicional de suporte t√©cnico da empresa com tempos de espera reduzidos: Yes (Sim), No (N√£o).\nStreaming TV: Indica se o cliente usa seu servi√ßo de Internet para transmitir programa√ß√£o de televis√£o de um provedor terceirizado: Yes (Sim), No (N√£o). A empresa n√£o cobra uma taxa adicional por este servi√ßo.\nStreaming Movies: Indica se o cliente usa seu servi√ßo de Internet para transmitir filmes de um provedor terceirizado: Yes (Sim), No (N√£o). A empresa n√£o cobra uma taxa adicional por este servi√ßo.\nStreaming Music: Indica se o cliente usa seu servi√ßo de Internet para transmitir m√∫sica de um provedor terceirizado: Yes (Sim), No (N√£o). A empresa n√£o cobra uma taxa adicional por este servi√ßo.\nUnlimited Data: Indica se o cliente pagou uma taxa mensal adicional para ter downloads/uploads de dados ilimitados: Yes (Sim), No (N√£o).\nContract: Indica o tipo de contrato atual do cliente: Month-to-Month (M√™s a M√™s), One Year (Um Ano), Two Year (Dois Anos).\nPaperless Billing: Indica se o cliente escolheu o faturamento sem papel: Yes (Sim), No (N√£o).\nPayment Method: Indica como o cliente paga sua conta: Bank Withdrawal (D√©bito em Conta), Credit Card (Cart√£o de Cr√©dito), Mailed Check (Cheque Enviado pelo Correio).\nMonthly Charge: Indica a cobran√ßa mensal total atual do cliente para todos os seus servi√ßos da empresa.\nTotal Charges: Indica o total de cobran√ßas do cliente, calculado at√© o final do trimestre especificado acima.\nTotal Refunds: Indica o total de reembolsos do cliente, calculado at√© o final do trimestre especificado acima.\nTotal Extra Data Charges: Indica o total de cobran√ßas do cliente por downloads de dados extras acima dos especificados em seu plano, at√© o final do trimestre especificado acima.\nTotal Long Distance Charges: Indica o total de cobran√ßas do cliente por chamadas de longa dist√¢ncia acima das especificadas em seu plano, at√© o final do trimestre especificado acima.\nSatisfaction Score: Uma classifica√ß√£o geral de satisfa√ß√£o do cliente com a empresa de 1 (Muito Insatisfeito) a 5 (Muito Satisfeito).\nCustomer Status: Indica o status do cliente no final do trimestre: Churned (Saiu), Stayed (Permaneceu) ou Joined (Entrou).\nChurn Label: Yes (Sim) = o cliente deixou a empresa neste trimestre. No (N√£o) = o cliente permaneceu com a empresa.\nChurn Score: Um valor de 0 a 100 que √© calculado usando a ferramenta preditiva IBM SPSS Modeler. O modelo incorpora m√∫ltiplos fatores conhecidos por causar rotatividade. Quanto maior o score, maior a probabilidade de o cliente sair.\nCLTV: Customer Lifetime Value (Valor de Tempo de Vida do Cliente). Um CLTV previsto √© calculado usando f√≥rmulas corporativas e dados existentes. Quanto maior o valor, mais valioso √© o cliente. Clientes de alto valor devem ser monitorados para evitar a rotatividade.\nChurn Category: Uma categoria de alto n√≠vel para o motivo de rotatividade do cliente: Attitude (Atitude), Competitor (Concorrente), Dissatisfaction (Insatisfa√ß√£o), Other (Outro), Price (Pre√ßo). Quando deixam a empresa, todos os clientes s√£o questionados sobre os motivos de sua sa√≠da. Diretamente relacionado ao Churn Reason.\nChurn Reason: Um motivo espec√≠fico do cliente para deixar a empresa. Diretamente relacionado ao Churn Category.\n\n\n\n\nüß† Tarefas e Desafios Propostos\nCom base na descri√ß√£o do ‚ÄúConjunto de Dados de Rotatividade de Clientes ConecteMais‚Äù, respondam √†s seguintes perguntas, assumindo o papel de analistas de dados durante a reuni√£o:\n\nQual √© o principal problema de neg√≥cio que a ConecteMais Corp.¬†est√° enfrentando, conforme indicado pelo conjunto de dados? Expliquem em suas pr√≥prias palavras as implica√ß√µes desse problema para a empresa.\nQual vari√°vel do conjunto de dados √© a mais relevante para identificar o problema de rotatividade de clientes? Justifiquem sua escolha.\nConsiderando o problema de churn, a equipe de Neg√≥cios expressa o desejo de ‚Äúentender melhor por que os clientes est√£o saindo e quem s√£o esses clientes para que possamos agir‚Äù. Com base nisso, qual seria o objetivo principal do projeto de minera√ß√£o de dados que voc√™s proporiam? Formulem-no de forma clara e mensur√°vel, pensando em um problema de classifica√ß√£o.\nAl√©m do objetivo principal, quais outros objetivos secund√°rios (mas importantes) poderiam ser alcan√ßados com a an√°lise deste conjunto de dados para auxiliar a equipe de Neg√≥cios na luta contra o churn? Pensem em informa√ß√µes que seriam valiosas para decis√µes estrat√©gicas."
  },
  {
    "objectID": "projeto/projeto_avaliativo.html#fase-2-o-diagn√≥stico-t√©cnico",
    "href": "projeto/projeto_avaliativo.html#fase-2-o-diagn√≥stico-t√©cnico",
    "title": "Projeto Avaliativo: An√°lise de churn em uma Empresa de Telecomunica√ß√µes",
    "section": "üõ†Ô∏è Fase 2: O Diagn√≥stico T√©cnico",
    "text": "üõ†Ô∏è Fase 2: O Diagn√≥stico T√©cnico\n\nüîç Objetivo\nO objetivo desta etapa √© permitir que a equipe de an√°lise compreenda profundamente o conjunto de dados da ConecteMais Corp., identifique potenciais problemas de qualidade dos dados e proponha um plano de pr√©-processamento para tornar os dados adequados √† modelagem preditiva.\n\n\nüß† Tarefas e Desafios Propostos\nAp√≥s a reuni√£o inicial, a equipe de An√°lise de Dados se re√∫ne internamente para discutir a base de dados fornecida e planejar as pr√≥ximas etapas. Nesta etapa, voc√™s devem identificar poss√≠veis problemas nos dados que podem dificultar a constru√ß√£o de um modelo de classifica√ß√£o e sugerir as t√©cnicas de pr√©-processamento adequadas para resolv√™-los. Agora √© hora de:\n\nAvaliar a qualidade dos dados\n\nIdentificar valores faltantes, vari√°veis irrelevantes e problemas de codifica√ß√£o;\nIdentificar vari√°veis com baixa variabilidade: que n√£o agregam informa√ß√£o ao modelo;\nIdentificar vari√°veis altamente correlacionadas: redund√¢ncia que pode prejudicar modelos lineares;\nAvaliar distribui√ß√µes assim√©tricas ou outliers extremos em vari√°veis num√©ricas;\nAvaliar se existe desbalanceamento da vari√°vel-alvo (Churn Label).\n\nPropor um plano de pr√©-processamento para tornar os dados prontos para uso em modelos de classifica√ß√£o supervisionada.\n\nA diretoria precisa de resultados acion√°veis. Sua an√°lise poder√° influenciar a decis√£o de milh√µes de d√≥lares em estrat√©gias de reten√ß√£o.\n\n\nü§î Quest√µes para Discuss√£o e Resolu√ß√£o:\n\nIdentifique no m√≠nimo 5 vari√°veis do conjunto de dados que, potencialmente, podem apresentar problemas que necessitem de pr√©-processamento antes de serem usadas em um modelo de classifica√ß√£o. Para cada vari√°vel identificada, descrevam o problema esperado.\nPara cada um dos problemas identificados na quest√£o anterior, sugiram uma ou mais t√©cnicas de pr√©-processamento que poderiam ser aplicadas para resolv√™-los. Justifiquem a escolha da t√©cnica. Por exemplo: Se o problema for ‚Äúvalores ausentes na vari√°vel Idade, a t√©cnica sugerida poderia ser‚Äùimputa√ß√£o pela m√©dia/mediana‚Äù ou ‚Äúremo√ß√£o das linhas com valores ausentes‚Äù.\nConsiderando o objetivo de construir um modelo de classifica√ß√£o para prever o churn, quais atributos do conjunto de dados voc√™s consideram que provavelmente n√£o ser√£o √∫teis como features preditivas (ou seja, n√£o trar√£o valor para o modelo)? Justifiquem sua resposta."
  },
  {
    "objectID": "projeto/projeto_avaliativo.html#fase-3-constru√ß√£o-e-avalia√ß√£o-de-modelos-preditivos",
    "href": "projeto/projeto_avaliativo.html#fase-3-constru√ß√£o-e-avalia√ß√£o-de-modelos-preditivos",
    "title": "Projeto Avaliativo: An√°lise de churn em uma Empresa de Telecomunica√ß√µes",
    "section": "üéØ Fase 3: Constru√ß√£o e Avalia√ß√£o de Modelos Preditivos",
    "text": "üéØ Fase 3: Constru√ß√£o e Avalia√ß√£o de Modelos Preditivos"
  },
  {
    "objectID": "projeto/projeto_avaliativo.html#fase-4-reconhecimento-de-padr√µes-e-segmenta√ß√£o-de-clientes",
    "href": "projeto/projeto_avaliativo.html#fase-4-reconhecimento-de-padr√µes-e-segmenta√ß√£o-de-clientes",
    "title": "Projeto Avaliativo: An√°lise de churn em uma Empresa de Telecomunica√ß√µes",
    "section": "üõçÔ∏è Fase 4: Reconhecimento de Padr√µes e Segmenta√ß√£o de Clientes",
    "text": "üõçÔ∏è Fase 4: Reconhecimento de Padr√µes e Segmenta√ß√£o de Clientes"
  },
  {
    "objectID": "projeto/projeto_avaliativo.html#fase-5-constru√ß√£o-de-um-sistema-de-recomenda√ß√£o",
    "href": "projeto/projeto_avaliativo.html#fase-5-constru√ß√£o-de-um-sistema-de-recomenda√ß√£o",
    "title": "Projeto Avaliativo: An√°lise de churn em uma Empresa de Telecomunica√ß√µes",
    "section": "üß∫ Fase 5: Constru√ß√£o de um Sistema de Recomenda√ß√£o",
    "text": "üß∫ Fase 5: Constru√ß√£o de um Sistema de Recomenda√ß√£o"
  },
  {
    "objectID": "programacao/semana-7.html#material-complementar",
    "href": "programacao/semana-7.html#material-complementar",
    "title": "Semana 07",
    "section": "",
    "text": "Vazamento de dados: O inimigo oculto da predi√ß√£o confi√°vel"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#redes-neurais-artificiais",
    "href": "aulas/Analise_preditiva/preditiva.html#redes-neurais-artificiais",
    "title": "An√°lise Preditiva",
    "section": "Redes Neurais Artificiais",
    "text": "Redes Neurais Artificiais\n\n\nRedes neurais artificiais s√£o modelos matem√°ticos e computacionais inspirados no funcionamento do c√©rebro humano."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#inspira√ß√£o-biol√≥gica",
    "href": "aulas/Analise_preditiva/preditiva.html#inspira√ß√£o-biol√≥gica",
    "title": "An√°lise Preditiva",
    "section": "Inspira√ß√£o biol√≥gica",
    "text": "Inspira√ß√£o biol√≥gica\nOs neur√¥nios se comunicam atrav√©s de sinapses. Sinapse √© a regi√£o onde dois neur√¥nios entram em contato e atrav√©s da qual os impulsos nervosos s√£o transmitidos entre eles"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#neur√¥nio-artificial",
    "href": "aulas/Analise_preditiva/preditiva.html#neur√¥nio-artificial",
    "title": "An√°lise Preditiva",
    "section": "Neur√¥nio Artificial",
    "text": "Neur√¥nio Artificial\n\n\n\n\n\n\n\n\n\n\n\\[ \\nu_k = \\sum_{i=1}^p w_{ki}x_i +b_k\\]\n\\[ y_k = \\varphi(\\nu_k)\\]"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#fun√ß√µes-de-ativa√ß√£o",
    "href": "aulas/Analise_preditiva/preditiva.html#fun√ß√µes-de-ativa√ß√£o",
    "title": "An√°lise Preditiva",
    "section": "Fun√ß√µes de ativa√ß√£o",
    "text": "Fun√ß√µes de ativa√ß√£o\n\nA fun√ß√£o de ativa√ß√£o, denotada por \\(\\varphi(\\nu)\\), define sua sa√≠da ou ativa√ß√£o em termos do sinal \\(\\nu\\).\n\nEssa fun√ß√£o √© respons√°vel por introduzir n√£o-linearidades nas opera√ß√µes realizadas pela rede neural, permitindo que ela aprenda e modele rela√ß√µes complexas nos dados."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-threshold",
    "href": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-threshold",
    "title": "An√°lise Preditiva",
    "section": "Fun√ß√£o Threshold",
    "text": "Fun√ß√£o Threshold\n\n\n\n\\[\\varphi(\\nu) = \\begin{cases}1 & \\text{se } \\nu \\geq 0, \\\\\n0 & \\text{se }\\nu &lt; 0\\end{cases}\\]\n\n\n\n\n\n\n\n\n\nüëâ √ötil em problemas onde se deseja atribuir uma sa√≠da bin√°ria, como 0 ou 1, com base em um limite."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-sigmoidal",
    "href": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-sigmoidal",
    "title": "An√°lise Preditiva",
    "section": "Fun√ß√£o Sigmoidal",
    "text": "Fun√ß√£o Sigmoidal\n\n\n\n\\[\\varphi(\\nu) = \\dfrac{1}{1 + \\exp(-a\\nu)}\\]\n\n\n\n\n\n\n\n\n\n\nüëâ √ötil em problemas onde a sa√≠da representa a probabilidade de pertencer a uma das duas classes."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-sinal",
    "href": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-sinal",
    "title": "An√°lise Preditiva",
    "section": "Fun√ß√£o Sinal",
    "text": "Fun√ß√£o Sinal\n\n\n\n\\[\\varphi(\\nu) = \\begin{cases}1 & \\text{se } \\nu &gt; 0, \\\\\n0 & \\text{se }\\nu = 0, \\\\\n-1 & \\text{se } \\nu &lt; 0 \\end{cases}\\]\n\n\n\n\n\n\n\n\n\nüëâ √ötil em problemas onde se deseja atribuir uma sa√≠da discreta de -1 ou 1 com base na polaridade do valor de entrada."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-tangente-hiperb√≥lica",
    "href": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-tangente-hiperb√≥lica",
    "title": "An√°lise Preditiva",
    "section": "Fun√ß√£o Tangente Hiperb√≥lica",
    "text": "Fun√ß√£o Tangente Hiperb√≥lica\n\n\n\n\\[\\varphi(\\nu) = \\dfrac{1-\\exp(-\\beta \\nu)}{1+\\exp(-\\beta \\nu)}\\]\n\n\n\n\n\n\n\n\n\nüëâ √â comumente usada em redes neurais para classifica√ß√£o bin√°ria ou regress√£o."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-relu",
    "href": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-relu",
    "title": "An√°lise Preditiva",
    "section": "Fun√ß√£o ReLU",
    "text": "Fun√ß√£o ReLU\n\n\n\n\\[\\varphi(\\nu) = \\max(0, \\nu)\\]\n\n\n\n\n\n\n\n\n\nüëâ Amplamente usada em camadas ocultas de redes neurais devido √† sua simplicidade computacional"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-softmax",
    "href": "aulas/Analise_preditiva/preditiva.html#fun√ß√£o-softmax",
    "title": "An√°lise Preditiva",
    "section": "Fun√ß√£o Softmax",
    "text": "Fun√ß√£o Softmax\n\n\n\n\\[\\varphi(\\nu) = \\dfrac{\\exp({\\nu})}{\\sum_{k=1}^K \\exp(\\nu_k) }\\]\n\n\n\n\n\n\n\n\n\nüëâ Usada para classifica√ß√£o multiclasse. As sa√≠das s√£o normalizadas, representando probabilidades."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#arquiteturas-de-redes",
    "href": "aulas/Analise_preditiva/preditiva.html#arquiteturas-de-redes",
    "title": "An√°lise Preditiva",
    "section": "Arquiteturas de redes",
    "text": "Arquiteturas de redes\n\nExistem v√°rias arquiteturas de redes neurais que foram desenvolvidas ao longo dos anos para atender a diferentes necessidades e desafios.\n\nA arquitetura de uma RNA, define a sua especialidade e qual tipo de problema poder√° ser utilizada para resolv√™-lo.\n\n\nO que define a arquitetura de uma RNA basicamente s√£o as camadas (camada √∫nica ou m√∫ltiplas camadas), n√∫mero de neur√¥nios em cada camada e o tipo de conex√£o entre os neur√¥nios (FeedForward ou feedback)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#perceptron",
    "href": "aulas/Analise_preditiva/preditiva.html#perceptron",
    "title": "An√°lise Preditiva",
    "section": "Perceptron",
    "text": "Perceptron\nO perceptron √© a arquitetura mais b√°sica de rede neural, composta por um √∫nico neur√¥nio com conex√µes diretas (feedforward) de entrada e uma fun√ß√£o de ativa√ß√£o.\n\n\n\n\n\n\n\n\n\n\n\n\n\nbias: ajusta a influ√™ncia das entradas\naprimora a flexibilidade do modelo."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#perceptron-1",
    "href": "aulas/Analise_preditiva/preditiva.html#perceptron-1",
    "title": "An√°lise Preditiva",
    "section": "Perceptron",
    "text": "Perceptron\nEssa arquitetura √© usada principalmente para problemas de classifica√ß√£o linearmente separ√°veis."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#multi-layer-perceptron-mlp",
    "href": "aulas/Analise_preditiva/preditiva.html#multi-layer-perceptron-mlp",
    "title": "An√°lise Preditiva",
    "section": "Multi-Layer Perceptron (MLP)",
    "text": "Multi-Layer Perceptron (MLP)\n\nO MLP √© uma arquitetura de rede neural feedforward com v√°rias camadas ocultas entre a camada de entrada e a camada de sa√≠da.\n\n\n\nCamada de Entrada: Recebe os dados de entrada.\n\n\n\n\n\nCamadas Ocultas: Realizam o processamento das entradas.\n\n\n\n\n\nCamada de Sa√≠da: Fornece a sa√≠da do modelo.\n\n\n\nCada camada possui v√°rios neur√¥nios interconectados."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#multi-layer-perceptron-mlp-1",
    "href": "aulas/Analise_preditiva/preditiva.html#multi-layer-perceptron-mlp-1",
    "title": "An√°lise Preditiva",
    "section": "Multi-Layer Perceptron (MLP)",
    "text": "Multi-Layer Perceptron (MLP)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#multi-layer-perceptron-mlp-2",
    "href": "aulas/Analise_preditiva/preditiva.html#multi-layer-perceptron-mlp-2",
    "title": "An√°lise Preditiva",
    "section": "Multi-Layer Perceptron (MLP)",
    "text": "Multi-Layer Perceptron (MLP)\n\n\n\n\n\n\nO MLP √© capaz de aprender e modelar rela√ß√µes complexas nos dados e √© amplamente usado em problemas de classifica√ß√£o e regress√£o."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#treinamento-das-redes-neurais",
    "href": "aulas/Analise_preditiva/preditiva.html#treinamento-das-redes-neurais",
    "title": "An√°lise Preditiva",
    "section": "Treinamento das Redes Neurais",
    "text": "Treinamento das Redes Neurais\n\nO treinamento de redes neurais √© o processo de ajustar os pesos e bias (par√¢metros) da rede neural para que ela seja capaz de aprender a partir dos dados de treinamento e gerar previs√µes precisas para novos dados.\n\nO treinamento envolve a minimiza√ß√£o de uma fun√ß√£o de custo ou perda, que mede a diferen√ßa entre as sa√≠das previstas pela rede neural e as sa√≠das desejadas."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#treinamento-das-redes-neurais-1",
    "href": "aulas/Analise_preditiva/preditiva.html#treinamento-das-redes-neurais-1",
    "title": "An√°lise Preditiva",
    "section": "Treinamento das Redes Neurais",
    "text": "Treinamento das Redes Neurais\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüëâ Treinamento supervisionado"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#treinamento-do-perceptron",
    "href": "aulas/Analise_preditiva/preditiva.html#treinamento-do-perceptron",
    "title": "An√°lise Preditiva",
    "section": "Treinamento do perceptron",
    "text": "Treinamento do perceptron\nPara cada exemplo de treinamento, o perceptron realiza uma propaga√ß√£o direta dos dados de entrada atrav√©s da fun√ß√£o de ativa√ß√£o.\n\nO resultado da propaga√ß√£o direta √© comparado com a sa√≠da desejada.\n\n\nSe a sa√≠da do perceptron corresponder √† sa√≠da desejada, nenhuma atualiza√ß√£o nos pesos e bias √© necess√°ria e o pr√≥ximo exemplo de treinamento √© processado.\n\n\nSe a sa√≠da do perceptron for diferente da sa√≠da desejada, os pesos e o bias s√£o atualizados para ajustar o perceptron."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#treinamento-do-perceptron-1",
    "href": "aulas/Analise_preditiva/preditiva.html#treinamento-do-perceptron-1",
    "title": "An√°lise Preditiva",
    "section": "Treinamento do perceptron",
    "text": "Treinamento do perceptron\nA atualiza√ß√£o dos pesos √© dada por:\n\\[\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\eta[\\hat{y}(n)-y(n)]\\mathbf{x}(n)\\]\n\nE a atualiza√ß√£o do bias:\n\\[b(n+1) = b(n) + \\eta y(n)\\]\nem que:\n\n\n\\(\\hat{y}(n)\\) √© a sa√≠da alvo\n\n\\(y(n)\\) √© a sa√≠da da rede\n\n\\(\\eta\\) √© a taxa de aprendizado"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#treinamento-do-perceptron-2",
    "href": "aulas/Analise_preditiva/preditiva.html#treinamento-do-perceptron-2",
    "title": "An√°lise Preditiva",
    "section": "Treinamento do perceptron",
    "text": "Treinamento do perceptron\n\nA taxa de aprendizagem (\\(\\eta\\)) √© um hiperpar√¢metro cr√≠tico no treinamento do perceptron:\n\n\nSe for muito grande, pode levar a oscila√ß√µes e a n√£o converg√™ncia do algoritmo.\n\n\n\n\nSe for muito pequena, o treinamento pode ser lento.\n\n\n\nGeralmente seu valor varia de 0.1 a 1.0"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#treinamento-de-redes-mlp",
    "href": "aulas/Analise_preditiva/preditiva.html#treinamento-de-redes-mlp",
    "title": "An√°lise Preditiva",
    "section": "Treinamento de redes MLP",
    "text": "Treinamento de redes MLP\n\nO algoritmo de aprendizado da MLP √© chamado backpropagation e √© composto, basicamente, de duas etapas\n\n\n\nPropaga√ß√£o: Recebimento dos est√≠mulos que √© aplicado aos neur√¥nios da rede, onde seu efeito se propaga camada por camada at√© produzir uma sa√≠da como resposta da rede. Neste passo n√£o h√° altera√ß√£o nos pesos sin√°pticos."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#treinamento-de-redes-mlp-1",
    "href": "aulas/Analise_preditiva/preditiva.html#treinamento-de-redes-mlp-1",
    "title": "An√°lise Preditiva",
    "section": "Treinamento de redes MLP",
    "text": "Treinamento de redes MLP\n\nA sa√≠da prevista pela rede MLP √© comparada com a sa√≠da desejada usando uma fun√ß√£o de custo. A fun√ß√£o de custo mede o qu√£o bem a rede MLP est√° performando em rela√ß√£o ao objetivo desejado.\n\n\n\nRetropropaga√ß√£o: Ap√≥s a sa√≠da, os pesos sin√°pticos s√£o ajustados de acordo com a regra de corre√ß√£o de erro. Este sinal √© propagado ent√£o para toda rede da sa√≠da para o entrada (caminho inverso), ou seja, o erro √© retropropagado."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#treinamento-de-redes-mlp-2",
    "href": "aulas/Analise_preditiva/preditiva.html#treinamento-de-redes-mlp-2",
    "title": "An√°lise Preditiva",
    "section": "Treinamento de redes MLP",
    "text": "Treinamento de redes MLP"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#ajuste-de-hiperpar√¢metros",
    "href": "aulas/Analise_preditiva/preditiva.html#ajuste-de-hiperpar√¢metros",
    "title": "An√°lise Preditiva",
    "section": "Ajuste de Hiperpar√¢metros",
    "text": "Ajuste de Hiperpar√¢metros\n\n\n\nN√∫mero de Camadas Ocultas: Determina a complexidade da rede.\n\n\nMais camadas: Maior capacidade de modelar rela√ß√µes complexas. Contudo, pode resultar em redes mais lentas para treinar e maior risco de overfitting se n√£o controlado.\n\nMenos camadas: Menor capacidade de abstra√ß√£o, podendo falhar em aprender padr√µes complexos dos dados."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#ajuste-de-hiperpar√¢metros-1",
    "href": "aulas/Analise_preditiva/preditiva.html#ajuste-de-hiperpar√¢metros-1",
    "title": "An√°lise Preditiva",
    "section": "Ajuste de Hiperpar√¢metros",
    "text": "Ajuste de Hiperpar√¢metros\n\n\n\nN√∫mero de Neur√¥nios: Controla a capacidade de aprendizagem da rede.\n\n\nMais neur√¥nios: Maior capacidade de aprendizagem e representa√ß√£o dos dados, mas pode levar ao overfitting (modelo ‚Äúmemorizando‚Äù os dados de treinamento em vez de generalizar bem).\n\nMenos neur√¥nios: Pode resultar em um modelo muito simples, incapaz de capturar as rela√ß√µes complexas dos dados."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#ajuste-de-hiperpar√¢metros-2",
    "href": "aulas/Analise_preditiva/preditiva.html#ajuste-de-hiperpar√¢metros-2",
    "title": "An√°lise Preditiva",
    "section": "Ajuste de Hiperpar√¢metros",
    "text": "Ajuste de Hiperpar√¢metros\n\n\n\nTaxa de Aprendizado: Define o quanto os pesos s√£o atualizados a cada itera√ß√£o.\n\n\nTaxa muito alta: O modelo pode ‚Äúsaltar‚Äù a solu√ß√£o √≥tima, resultando em um treinamento inst√°vel.\n\nTaxa muito baixa: O treinamento ser√° mais lento, podendo at√© ficar preso em um √≥timo local (m√°ximo ou m√≠nimo)."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#evitando-overfitting",
    "href": "aulas/Analise_preditiva/preditiva.html#evitando-overfitting",
    "title": "An√°lise Preditiva",
    "section": "Evitando Overfitting",
    "text": "Evitando Overfitting\n\n\nDropout √© uma t√©cnica de regulariza√ß√£o onde, durante o treinamento, neur√¥nios s√£o aleatoriamente ‚Äúdesconectados‚Äù (ou desligados) de cada vez. Isso impede que o modelo dependa demais de neur√¥nios espec√≠ficos.\n\nüëâ Reduz a co-adapta√ß√£o dos neur√¥nios, for√ßando o modelo a aprender representa√ß√µes mais robustas. Aumenta a generaliza√ß√£o do modelo e melhora o desempenho em dados n√£o vistos.\n\n\nDurante o treinamento, uma fra√ß√£o dos neur√¥nios (tipicamente 20%-50%) √© removida aleatoriamente a cada itera√ß√£o, dificultando a adapta√ß√£o excessiva a um √∫nico conjunto de caracter√≠sticas."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#evitando-overfitting-1",
    "href": "aulas/Analise_preditiva/preditiva.html#evitando-overfitting-1",
    "title": "An√°lise Preditiva",
    "section": "Evitando Overfitting",
    "text": "Evitando Overfitting\n\n\nA regulariza√ß√£o L2 penaliza os pesos grandes ao adicionar uma soma dos quadrados dos pesos √† fun√ß√£o de custo do modelo. Isso evita que os neur√¥nios se especializem demais em dados espec√≠ficos.\n\nüëâ Ao for√ßar os pesos a permanecerem pequenos, o modelo tende a ter uma capacidade de generaliza√ß√£o maior, reduzindo o risco de overfitting. Penaliza fortemente pesos grandes, incentivando uma solu√ß√£o mais ‚Äúsuave‚Äù com menos varia√ß√£o nos par√¢metros.\n\n\nA regulariza√ß√£o L2 √© muito eficaz para problemas com muitos atributos e ajuda a melhorar a estabilidade do modelo."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#evitando-overfitting-dropout",
    "href": "aulas/Analise_preditiva/preditiva.html#evitando-overfitting-dropout",
    "title": "An√°lise Preditiva",
    "section": "Evitando Overfitting: Dropout",
    "text": "Evitando Overfitting: Dropout\n\n\nDropout √© uma t√©cnica de regulariza√ß√£o onde, durante o treinamento, neur√¥nios s√£o aleatoriamente ‚Äúdesconectados‚Äù (ou desligados) de cada vez. Isso impede que o modelo dependa demais de neur√¥nios espec√≠ficos.\n\nüëâ Reduz a co-adapta√ß√£o dos neur√¥nios, for√ßando o modelo a aprender representa√ß√µes mais robustas. Aumenta a generaliza√ß√£o do modelo e melhora o desempenho em dados n√£o vistos.\n\n\nDurante o treinamento, uma fra√ß√£o dos neur√¥nios (tipicamente 20%-50%) √© removida aleatoriamente a cada itera√ß√£o, dificultando a adapta√ß√£o excessiva a um √∫nico conjunto de caracter√≠sticas."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#evitando-overfitting-regulariza√ß√£o-l2",
    "href": "aulas/Analise_preditiva/preditiva.html#evitando-overfitting-regulariza√ß√£o-l2",
    "title": "An√°lise Preditiva",
    "section": "Evitando Overfitting: Regulariza√ß√£o L2",
    "text": "Evitando Overfitting: Regulariza√ß√£o L2\n\n\nA regulariza√ß√£o L2 penaliza os pesos grandes ao adicionar uma soma dos quadrados dos pesos √† fun√ß√£o de custo do modelo. Isso evita que os neur√¥nios se especializem demais em dados espec√≠ficos.\n\nüëâ Ao for√ßar os pesos a permanecerem pequenos, o modelo tende a ter uma capacidade de generaliza√ß√£o maior, reduzindo o risco de overfitting. Penaliza fortemente pesos grandes, incentivando uma solu√ß√£o mais ‚Äúsuave‚Äù com menos varia√ß√£o nos par√¢metros.\n\n\nA regulariza√ß√£o L2 √© muito eficaz para problemas com muitos atributos e ajuda a melhorar a estabilidade do modelo."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-das-redes-neurais-artificiais-rna",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-das-redes-neurais-artificiais-rna",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens das Redes Neurais Artificiais (RNA)",
    "text": "Vantagens e Desvantagens das Redes Neurais Artificiais (RNA)"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-9",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-9",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\n\nüëç\n\n\nCapacidade de Modelar Rela√ß√µes Complexas: As RNA s√£o poderosas para capturar padr√µes n√£o lineares e rela√ß√µes complexas nos dados.\nAdaptabilidade: Podem ser aplicadas a uma ampla gama de problemas, tanto para classifica√ß√£o quanto para regress√£o, al√©m de tarefas como vis√£o computacional e processamento de linguagem natural.\nMelhoria com Dados Abundantes: Seu desempenho tende a melhorar conforme a quantidade de dados aumenta, sendo uma escolha excelente em problemas com grandes volumes de dados.\nDesempenho em Dados N√£o Estruturados: Muito eficazes em dados como imagens, √°udio e texto, onde outras t√©cnicas de modelagem podem n√£o ser t√£o eficientes."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-10",
    "href": "aulas/Analise_preditiva/preditiva.html#vantagens-e-desvantagens-10",
    "title": "An√°lise Preditiva",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\n\n\n\nüëé\n\n\nNecessidade de Grande Quantidade de Dados: As RNA tendem a ter um desempenho inferior quando a quantidade de dados √© pequena. Elas precisam de muitos exemplos para generalizar corretamente.\nCusto Computacional Alto: O treinamento de redes neurais pode ser extremamente intensivo em termos de tempo e recursos computacionais, especialmente para redes grandes e profundas.\nFalta de Interpretabilidade: As RNA s√£o frequentemente descritas como ‚Äúcaixas pretas‚Äù, j√° que pode ser dif√≠cil entender como a rede toma decis√µes. Isso pode ser um problema em aplica√ß√µes onde a explica√ß√£o das decis√µes √© crucial (por exemplo, na √°rea da sa√∫de ou finan√ßas).\nOverfitting: As RNA s√£o propensas a overfitting, especialmente quando o n√∫mero de camadas e neur√¥nios √© muito alto, ou quando n√£o h√° dados suficientes para treinar adequadamente o modelo."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#rna-para-classifica√ß√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#rna-para-classifica√ß√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "RNA para classifica√ß√£o no R e Python",
    "text": "RNA para classifica√ß√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(nnet)\nlibrary(Metrics)\n\nset.seed(12345)\ndata(iris)\niris &lt;- iris %&gt;% filter(Species != \"setosa\") %&gt;% mutate(Species = factor(Species))\niris_split &lt;- initial_split(iris, strata = Species)\ntrain_data &lt;- training(iris_split)\ntest_data &lt;- testing(iris_split)\n\nnn_model &lt;- mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"classification\")\n\nnn_fit &lt;- workflow() %&gt;%\n  add_model(nn_model) %&gt;%\n  add_formula(Species ~ .) %&gt;%\n  fit(data = train_data)\n\nsp_predict &lt;- predict(nn_fit, new_data = test_data)\n\nMetrics::accuracy(test_data$Species, sp_predict$.pred_class)\n\n[1] 0.8846154\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data[iris.target != 0, :2]\ny = iris.target[iris.target != 0]\n\n# Split dos dados em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n\nnn_model = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=12345)\nnn_model.fit(X_train, y_train)\n\n\n\n\nMLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=12345)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nMLPClassifierMLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=12345)\n\n\n\ny_pred = nn_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.43333333333333335"
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#pr√©-processamento-para-redes-neurais-artificiais",
    "href": "aulas/Analise_preditiva/preditiva.html#pr√©-processamento-para-redes-neurais-artificiais",
    "title": "An√°lise Preditiva",
    "section": "Pr√©-processamento para Redes Neurais Artificiais",
    "text": "Pr√©-processamento para Redes Neurais Artificiais\n\nLimpeza de Dados: Remover ou imputar valores ausentes e corrigir erros nos dados.\nTransforma√ß√£o de Vari√°veis Categ√≥ricas: Usar One-Hot Encoding ou Label Encoding para converter vari√°veis categ√≥ricas em num√©ricas.\nNormaliza√ß√£o/Padroniza√ß√£o: Escalonar as vari√°veis para que tenham a mesma escala.\nNormaliza√ß√£o: Ajuste para o intervalo [0, 1].\nPadroniza√ß√£o: Ajuste para m√©dia 0 e desvio padr√£o 1.\nDivis√£o dos Dados: Separar os dados em conjuntos de treinamento e teste (ex: 80/20).\nBalanceamento de Classes (se necess√°rio): Usar t√©cnicas como SMOTE ou subamostragem/sobreamostragem."
  },
  {
    "objectID": "aulas/Analise_preditiva/preditiva.html#rna-para-regress√£o-no-r-e-python",
    "href": "aulas/Analise_preditiva/preditiva.html#rna-para-regress√£o-no-r-e-python",
    "title": "An√°lise Preditiva",
    "section": "RNA para regress√£o no R e Python",
    "text": "RNA para regress√£o no R e Python\n\n\nR\nPython\n\n\n\n\nlibrary(tidymodels)\nlibrary(Metrics)\n\ndata(\"mtcars\")\nset.seed(12345)\nmtcars_split &lt;- initial_split(mtcars, strata = mpg)\ntrain_data &lt;- training(mtcars_split)\ntest_data &lt;- testing(mtcars_split)\n\nnn_reg &lt;- mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nnn_reg_fit &lt;- workflow() %&gt;%\n  add_model(nn_reg) %&gt;%\n  add_formula(mpg ~ .) %&gt;%\n  fit(data = train_data)\n\nmpg_predict &lt;- predict(nn_reg_fit, test_data)\n\nMetrics::mse(test_data$mpg, mpg_predict$.pred) # MSE\n\n[1] 15.50235\n\n\n\n\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, random_state=12345\n)\n\nnn_reg = MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=12345)\nnn_reg.fit(X_train, y_train)\n\n\n\n\nMLPRegressor(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=12345)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nMLPRegressorMLPRegressor(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=12345)\n\n\n\ny_pred = nn_reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=True)  # MSE\n\n0.7468737160368192"
  },
  {
    "objectID": "programacao/semana-8.html",
    "href": "programacao/semana-8.html",
    "title": "Semana 08",
    "section": "",
    "text": "An√°lise preditiva"
  },
  {
    "objectID": "programacao/semana-8.html#slides",
    "href": "programacao/semana-8.html#slides",
    "title": "Semana 08",
    "section": "",
    "text": "An√°lise preditiva"
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html",
    "href": "analise_dados/space_titanic_workflow.html",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "",
    "text": "Imagine que voc√™ recebeu um conjunto de dados misterioso sobre passageiros de uma espa√ßonave e que alguns deles desapareceram em uma outra dimens√£o! A miss√£o? Construir um modelo preditivo para identificar quem foi ‚Äútransportado‚Äù com base em caracter√≠sticas como gastos, cabine, companhia de viagem, etc.\nPara essa jornada, vamos usar o pacote tidymodels do R, que nos ajuda a organizar todo o fluxo de an√°lise preditiva de maneira limpa, modular e reprodut√≠vel. Al√©m disso, vamos comparar seis modelos diferentes e ajustar automaticamente os seus hiperpar√¢metros para alcan√ßar a melhor performance poss√≠vel."
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#pacotes-utilizados",
    "href": "analise_dados/space_titanic_workflow.html#pacotes-utilizados",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Pacotes utilizados",
    "text": "Pacotes utilizados\n\nload &lt;- function(pkg){\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg))\n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n} \n\n## Pacotes utilizados nessa an√°lise\n\npackages &lt;- c(\"tidyverse\",'tidymodels', 'janitor', 'ggpubr', 'funModeling', 'ggalluvial', 'vip', 'skimr', 'VIM', 'patchwork', 'klaR', 'kknn', 'parsnip', 'discrim', 'colino', 'FSelectorRcpp','praznik', 'kernelshap', 'shapviz', \"DALEX\", \"DALEXtra\", 'doParallel', 'tictoc', 'tidyr', 'purrr')\nload(packages)\n\n    tidyverse    tidymodels       janitor        ggpubr   funModeling \n         TRUE          TRUE          TRUE          TRUE          TRUE \n   ggalluvial           vip         skimr           VIM     patchwork \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         klaR          kknn       parsnip       discrim        colino \n         TRUE          TRUE          TRUE          TRUE          TRUE \nFSelectorRcpp       praznik    kernelshap       shapviz         DALEX \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     DALEXtra    doParallel        tictoc         tidyr         purrr \n         TRUE          TRUE          TRUE          TRUE          TRUE"
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#leitura-dos-dados",
    "href": "analise_dados/space_titanic_workflow.html#leitura-dos-dados",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Leitura dos dados",
    "text": "Leitura dos dados\n\ndados = rio::import(\"https://raw.githubusercontent.com/tiagomartin/est125/refs/heads/main/dados/space_titanic.csv\")\ndados %&gt;% \n  glimpse()\n\nRows: 8,693\nColumns: 14\n$ PassengerId  &lt;chr&gt; \"0001_01\", \"0002_01\", \"0003_01\", \"0003_02\", \"0004_01\", \"0‚Ä¶\n$ HomePlanet   &lt;chr&gt; \"Europa\", \"Earth\", \"Europa\", \"Europa\", \"Earth\", \"Earth\", ‚Ä¶\n$ CryoSleep    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FA‚Ä¶\n$ Cabin        &lt;chr&gt; \"B/0/P\", \"F/0/S\", \"A/0/S\", \"A/0/S\", \"F/1/S\", \"F/0/P\", \"F/‚Ä¶\n$ Destination  &lt;chr&gt; \"TRAPPIST-1e\", \"TRAPPIST-1e\", \"TRAPPIST-1e\", \"TRAPPIST-1e‚Ä¶\n$ Age          &lt;dbl&gt; 39, 24, 58, 33, 16, 44, 26, 28, 35, 14, 34, 45, 32, 48, 2‚Ä¶\n$ VIP          &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FA‚Ä¶\n$ RoomService  &lt;dbl&gt; 0, 109, 43, 0, 303, 0, 42, 0, 0, 0, 0, 39, 73, 719, 8, 32‚Ä¶\n$ FoodCourt    &lt;dbl&gt; 0, 9, 3576, 1283, 70, 483, 1539, 0, 785, 0, 0, 7295, 0, 1‚Ä¶\n$ ShoppingMall &lt;dbl&gt; 0, 25, 0, 371, 151, 0, 3, 0, 17, 0, NA, 589, 1123, 65, 12‚Ä¶\n$ Spa          &lt;dbl&gt; 0, 549, 6715, 3329, 565, 291, 0, 0, 216, 0, 0, 110, 0, 0,‚Ä¶\n$ VRDeck       &lt;dbl&gt; 0, 44, 49, 193, 2, 0, 0, NA, 0, 0, 0, 124, 113, 24, 7, 0,‚Ä¶\n$ Name         &lt;chr&gt; \"Maham Ofracculy\", \"Juanna Vines\", \"Altark Susent\", \"Sola‚Ä¶\n$ Transported  &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n\n\n\ndados %&gt;% df_status()\n\n       variable q_zeros p_zeros q_na p_na q_inf p_inf      type unique\n1   PassengerId       0    0.00    0 0.00     0     0 character   8693\n2    HomePlanet       0    0.00    0 0.00     0     0 character      4\n3     CryoSleep    5439   62.57  217 2.50     0     0   logical      2\n4         Cabin       0    0.00    0 0.00     0     0 character   6561\n5   Destination       0    0.00    0 0.00     0     0 character      4\n6           Age     178    2.05  179 2.06     0     0   numeric     80\n7           VIP    8291   95.38  203 2.34     0     0   logical      2\n8   RoomService    5577   64.16  181 2.08     0     0   numeric   1273\n9     FoodCourt    5456   62.76  183 2.11     0     0   numeric   1507\n10 ShoppingMall    5587   64.27  208 2.39     0     0   numeric   1115\n11          Spa    5324   61.24  183 2.11     0     0   numeric   1327\n12       VRDeck    5495   63.21  188 2.16     0     0   numeric   1306\n13         Name       0    0.00    0 0.00     0     0 character   8474\n14  Transported    4315   49.64    0 0.00     0     0   logical      2\n\n\nNa an√°lise anterior, fizemos a explora√ß√£o da base de dados e atrav√©s dela, contru√≠mos uma receita de pr√©-processamento utilizando o pacote recipes do ecossistema tidymodels.\n\ndados = dados %&gt;%\n  separate(PassengerId, c(\"PassGroup\", \"nGroup\"), sep = \"_\", remove = FALSE) %&gt;% \n  mutate(PassGroup = as.numeric(PassGroup))\n\n\ndados = dados %&gt;%\n  separate(Cabin, c(\"CabinDeck\", \"CabinNum\", \"CabinSide\"), sep = \"/\") %&gt;% \n  mutate(CabinDeck = as.factor(CabinDeck),\n         CabinSide = as.factor(CabinSide))"
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#divis√£o-dos-dados",
    "href": "analise_dados/space_titanic_workflow.html#divis√£o-dos-dados",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Divis√£o dos dados",
    "text": "Divis√£o dos dados\n\nset.seed(12345)\nsplits = dados %&gt;% initial_split(strata = Transported)\n\ndados_treino = training(splits)\ndados_teste = testing(splits)"
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#receita-de-pr√©-processamento",
    "href": "analise_dados/space_titanic_workflow.html#receita-de-pr√©-processamento",
    "title": "An√°lise preditiva com tidymodels: Experimento com Space Titanic",
    "section": "Receita de pr√©-processamento",
    "text": "Receita de pr√©-processamento\n\nSTRecipe = recipe(Transported ~ ., data = dados_treino) %&gt;% \n  update_role(PassengerId, Name, new_role = \"ID\") %&gt;%\n  step_mutate(\n    IsAlone = ifelse(GroupSize==1, 1, 0) %&gt;% as.factor(),\n    CabinNum = as.numeric(CabinNum),\n    nGroup = as.numeric(nGroup),\n    VIP = as.factor(VIP),\n    CryoSleep = ifelse((RoomService &gt; 0 | FoodCourt &gt; 0 | ShoppingMall &gt; 0 | Spa &gt; 0 | VRDeck &gt; 0), replace_na(CryoSleep, FALSE),TRUE),\n    CryoSleep = as.factor(CryoSleep)) %&gt;% \n  step_impute_knn(CryoSleep, neighbors = 2, impute_with = imp_vars(PassGroup, CabinDeck, CabinNum, CabinSide)) %&gt;% \n  step_mutate(\n    RoomService = ifelse(CryoSleep==TRUE, replace_na(RoomService, 0),RoomService),\n    FoodCourt = ifelse(CryoSleep==TRUE, replace_na(FoodCourt, 0),FoodCourt),\n    ShoppingMall = ifelse(CryoSleep==TRUE, replace_na(ShoppingMall, 0), ShoppingMall),\n    Spa = ifelse(CryoSleep==TRUE, replace_na(Spa, 0), Spa),\n    VRDeck = ifelse(CryoSleep==TRUE, replace_na(VRDeck, 0), VRDeck)\n  ) %&gt;% \n  step_impute_knn(RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, neighbors = 2, \n                  impute_with = imp_vars(PassGroup, CabinDeck, CabinNum, CabinSide)) %&gt;%\n  step_impute_bag(Age, VIP, CryoSleep, Destination, HomePlanet, CabinDeck, CabinNum, \n                  CabinSide) %&gt;%\n  step_mutate_at(RoomService:VRDeck, fn= ~log10(. + 1)) %&gt;% \n  step_mutate(despesasSuperfluas = RoomService + Spa + VRDeck,\n              despesasBasicas = FoodCourt + ShoppingMall,\n              Isearth = ifelse(HomePlanet=='Earth', 1, 0) %&gt;% as.factor(),\n              Iskid = ifelse(Age &lt; 14, 1, 0) %&gt;% as.factor()) %&gt;% \n  prep(verbose=TRUE)\n\noper 1 step mutate [training] \noper 2 step impute knn [training] \noper 3 step mutate [training] \noper 4 step impute knn [training] \noper 5 step impute bag [training] \noper 6 step mutate at [training] \noper 7 step mutate [training] \nThe retained training set is ~ 1.74 Mb  in memory.\n\ndados_prep = bake(STRecipe, new_data = NULL)"
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#receita-principal-de-pr√©-processamento",
    "href": "analise_dados/space_titanic_workflow.html#receita-principal-de-pr√©-processamento",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Receita principal de pr√©-processamento",
    "text": "Receita principal de pr√©-processamento\n\nSTRecipe = recipe(Transported ~ ., data = dados_treino) %&gt;% \n  update_role(PassengerId, Name, new_role = \"ID\") %&gt;%\n  step_mutate(\n    GroupSize = as.integer(purrr::map_dbl(PassGroup, ~ sum(PassGroup == .x))),\n    IsAlone = ifelse(GroupSize==1, 1, 0) %&gt;% as.factor(),\n    CabinNum = as.numeric(CabinNum),\n    nGroup = as.numeric(nGroup),\n    VIP = as.factor(VIP),\n    CryoSleep = ifelse((RoomService &gt; 0 | FoodCourt &gt; 0 | ShoppingMall &gt; 0 | Spa &gt; 0 | VRDeck &gt; 0), tidyr::replace_na(CryoSleep, FALSE),TRUE),\n    CryoSleep = as.factor(CryoSleep)) %&gt;% \n  step_impute_knn(CryoSleep, neighbors = 2, impute_with = imp_vars(PassGroup, CabinDeck, CabinNum, CabinSide)) %&gt;% \n  step_mutate(\n    RoomService = ifelse(CryoSleep==TRUE, tidyr::replace_na(RoomService, 0),RoomService),\n    FoodCourt = ifelse(CryoSleep==TRUE, tidyr::replace_na(FoodCourt, 0),FoodCourt),\n    ShoppingMall = ifelse(CryoSleep==TRUE, tidyr::replace_na(ShoppingMall, 0), ShoppingMall),\n    Spa = ifelse(CryoSleep==TRUE, tidyr::replace_na(Spa, 0), Spa),\n    VRDeck = ifelse(CryoSleep==TRUE, tidyr::replace_na(VRDeck, 0), VRDeck)\n  ) %&gt;% \n  step_impute_knn(RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, neighbors = 2, \n                  impute_with = imp_vars(PassGroup, CabinDeck, CabinNum, CabinSide)) %&gt;%\n  step_impute_bag(Age, VIP, CryoSleep, Destination, HomePlanet, CabinDeck, CabinNum, \n                  CabinSide) %&gt;%\n  step_mutate_at(RoomService:VRDeck, fn= ~log10(. + 1)) %&gt;% \n  step_mutate(despesasSuperfluas = RoomService + Spa + VRDeck,\n              despesasBasicas = FoodCourt + ShoppingMall,\n              Isearth = ifelse(HomePlanet=='Earth', 1, 0) %&gt;% as.factor(),\n              Iskid = ifelse(Age &lt; 14, 1, 0) %&gt;% as.factor(), \n              Transported = as.factor(Transported)) %&gt;% \n  step_select_infgain(all_predictors(), outcome = \"Transported\", threshold = 0.5)\n\nNeste momento chegou a hora de prepararmos a modelagem dos dados. Vamos comparar seis modelos diferentes de classifica√ß√£o, KNN, Naive Bayes, √Årvore de Decis√£o, Random Forest, SVM e Rede Neural, todos aplicados ao conjunto de dados Space Titanic. Cada modelo ser√° treinado e validado com t√©cnicas de valida√ß√£o cruzada, e seus hiperpar√¢metros ser√£o ajustados automaticamente para obter o melhor desempenho poss√≠vel. Ao final, avaliaremos as m√©tricas obtidas e selecionaremos o modelo que apresentar os melhores resultados para os dados analisados, considerando como crit√©rio de sele√ß√£o a acur√°cia dos modelos."
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#receita-adicional-de-pr√©-processamento",
    "href": "analise_dados/space_titanic_workflow.html#receita-adicional-de-pr√©-processamento",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Receita adicional de pr√©-processamento",
    "text": "Receita adicional de pr√©-processamento\nAlguns algoritmos de machine learning (como KNN, SVM e Redes Neurais) s√£o sens√≠veis √† escala dos dados e necessitam que todas as vari√°veis sejam num√©ricas. Se uma vari√°vel tem valores muito maiores do que outra, ela pode dominar a dist√¢ncia ou as pondera√ß√µes do modelo, mesmo que n√£o seja a mais importante. Portanto, normalizar evita esse desequil√≠brio e garante que todas as vari√°veis num√©ricas contribuam igualmente para a an√°lise. Al√©m disso, vari√°veis com vari√¢ncia zero n√£o contribuem com informa√ß√£o √∫til para os modelos preditivos. Como todos os seus valores s√£o iguais, elas n√£o ajudam a diferenciar as classes que queremos prever. Somado a isso, podem at√© atrapalhar o desempenho do modelo ou causar erros em algoritmos que exigem variabilidade nos dados.\n\nrecipe_normalizado = STRecipe %&gt;% \n  step_normalize(all_numeric()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\nPara os demais algoritmos, apenas incluiremos o passo de verifica√ß√£o de vari√°veis com zero vari√¢ncia na nossa receita principal.\n\nzv_recipe = STRecipe %&gt;%\n  step_zv(all_predictors())"
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#defini√ß√£o-dos-modelos",
    "href": "analise_dados/space_titanic_workflow.html#defini√ß√£o-dos-modelos",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Defini√ß√£o dos modelos",
    "text": "Defini√ß√£o dos modelos\nA seguir, definimos os modelos e hiperpar√¢metros que ser√£o tunados:\n\nknn_model = nearest_neighbor(neighbors = tune(), weight_func = tune(), dist_power = tune()) %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\") \n\nnb_model = naive_Bayes(smoothness = tune(), Laplace = tune()) %&gt;% \n  set_engine('klaR') %&gt;% \n  set_mode('classification') \n\ndt_model = decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;%\n  set_engine('rpart') %&gt;% \n  set_mode('classification') \n\nrf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% \n  set_engine('ranger') %&gt;% \n  set_mode('classification') \n\nsvm_model = svm_linear(cost = tune(), margin = tune()) %&gt;% \n  set_engine('kernlab') %&gt;% \n  set_mode('classification')\n\nnn_model = mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% \n  set_engine('nnet') %&gt;% \n  set_mode('classification') \n\nTodos os modelos foram configurados para realizar tarefas de classifica√ß√£o e tiveram seus hiperpar√¢metros definidos com tune(), o que permite que esses valores sejam ajustados automaticamente durante o processo de tuning com valida√ß√£o cruzada. Cada modelo est√° pronto para ser integrado a um workflow do tidymodels, utilizando fun√ß√µes como tune_grid() ou tune_bayes() para encontrar as melhores combina√ß√µes de par√¢metros. Com isso, conseguimos comparar o desempenho entre os diferentes algoritmos de forma justa e escolher aquele que apresenta os melhores resultados para o conjunto de dados analisado."
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#juntando-tudo",
    "href": "analise_dados/space_titanic_workflow.html#juntando-tudo",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Juntando tudo",
    "text": "Juntando tudo\nVamos utilizar o pacote workflowsets do ecossistema tidymodels para organizar e comparar v√°rios modelos e pr√©-processamentos ao mesmo tempo, de forma clara e automatizada. A fun√ß√£o workflow_set() cria uma cole√ß√£o de workflows, combinando diferentes receitas de pr√©-processamento (recipes) com diferentes modelos (models). Ela √© √∫til quando queremos testar diversas combina√ß√µes de forma sistem√°tica.\n\nPrimeira parte: workflows com pr√©-processamento simples\n\nsimples = workflow_set(\n  preproc = list(simples = zv_recipe),\n  models = list(arvore_decisao = dt_model,\n                randon_forest = rf_model,\n                naiveBayes = nb_model))\n\nAqui estamos criando um conjunto de workflows chamado simples, onde usamos a receita zv_recipe, que executa todo o pr√©-processamento definido anteriormente em STRecipe adicionando o passo de verifica√ß√£o de vari√°veis com zero vari√¢ncia e a combinamos com tr√™s modelos diferentes:\n\n√Årvore de decis√£o (dt_model)\nRandom Forest (rf_model)\nNaive Bayes (nb_model)\n\nIsso resultar√° em tr√™s workflows com o mesmo pr√©-processamento e modelos distintos.\n\n\nSegunda parte: workflows com pr√©-processamento mais elaborado\n\nnormalizados = workflow_set(\n  preproc = list(normalizados = recipe_normalizado),\n  models = list(redes_neurais = nn_model,\n                svm = svm_model,\n                knn = knn_model))\n\nNeste bloco, criamos outra cole√ß√£o de workflows chamada normalizados, que usa a receita recipe_normalizado, que realiza todo o pr√©-processamento definido em STRecipe adicionando as etapas step_normalize() e step_dummy(), que s√£o essenciais para modelos sens√≠veis √† escala. Essa receita √© combinada com:\n\nRede Neural (nn_model)\nSVM (svm_model)\nKNN (knn_model)\n\nTamb√©m resultar√° em tr√™s workflows diferentes.\n\n\nUnindo tudo\n\ncomplete_workflows = bind_rows(simples,normalizados) %&gt;% \n  mutate(wflow_id = gsub(\"(simples_)|(normalizados_)\", \"\", wflow_id))\n\nJuntamos todos os workflows (total de 6) em um √∫nico objeto complete_workflows e removemos os prefixos \"simples_\" e \"normalizados_\" dos nomes dos workflows, deixando o identificador do modelo mais limpo (ex: \"arvore_decisao\" ao inv√©s de \"simples_arvore_decisao\")."
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#hora-de-come√ßar-o-processo-de-treinamento",
    "href": "analise_dados/space_titanic_workflow.html#hora-de-come√ßar-o-processo-de-treinamento",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Hora de come√ßar o processo de treinamento",
    "text": "Hora de come√ßar o processo de treinamento\nPrimeiro, vamos criar os conjuntos de valida√ß√£o cruzada que ser√£o usados para avaliar os modelos. Note que faremos uma valida√ß√£o cruzada com 10 dobras (folds), ou seja, o conjunto de dados ser√° dividido em 10 partes, garantindo que a propor√ß√£o de classes seja mantida em cada uma das dobras, o que √© importante quando a vari√°vel resposta est√° desbalanceada. Em cada rodada, uma parte √© usada como valida√ß√£o e as outras 9 como treino. As informa√ß√µes sobre esse processo s√£o armazenadas no objeto cv_splits.\n\ncv_splits = vfold_cv(\n  dados_treino, v = 10, strata = Transported\n)\n\nA seguir, configuramos o comportamento do processo de tuning quando usamos tune_grid() (ou tune_bayes()) e salvamos no objeto grid_ctrl.\n\ngrid_ctrl = control_grid(\n  save_pred = TRUE,\n  parallel_over = 'everything',\n  save_workflow = TRUE\n)\n\n\nsave_pred = TRUE: salva as predi√ß√µes feitas durante o tuning em cada dobra, o que permite gerar gr√°ficos como curva ROC, matriz de confus√£o e an√°lise de erro depois.\nparallel_over = 'everything': habilita o uso de paraleliza√ß√£o completa, ou seja, o R vai tentar usar m√∫ltiplos n√∫cleos do processador para acelerar o tuning, paralelizando sobre modelos, dobras e combina√ß√µes de hiperpar√¢metros.\nsave_workflow = TRUE : salva o workflow completo para cada combina√ß√£o de hiperpar√¢metros testada, o que permite depois recuperar o modelo treinado com uma configura√ß√£o espec√≠fica.\n\nNeste momento, vamos executar o tuning dos modelos em paralelo, ou seja, usando todos os n√∫cleos de CPU dispon√≠veis no seu computador menos um, para evitar travar o sistema operacional, o que pode acelerar bastante o processo quando estamos testando muitos modelos e hiperpar√¢metros.\n\ndoParallel::registerDoParallel(cores = detectCores()-1)\n\ntic()\n\ngrid_resultados = complete_workflows %&gt;% \n  workflow_map(\n    resamples = cv_splits,\n    grid = 15,\n    control = grid_ctrl\n  )\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntoc()\n\n836.19 sec elapsed\n\ndoParallel::stopImplicitCluster()\n\nNote que a etapa principal, onde o tidymodels executa todos os workflows definidos, com tuning e valida√ß√£o cruzada est√° armazenada no objeto grid_resultados. Aqui, a fun√ß√£o workflow_map() aplica tune_grid() com 15 combina√ß√µes aleat√≥rias de hiperpar√¢metros a cada workflow listado em complete_workflows (que cont√™m diferentes combina√ß√µes de modelos e receitas), usando os 10 folds definidos para valida√ß√£o cruzada e as configura√ß√µes definidas com control_grid() (como salvar predi√ß√µes, paralelizar, etc.). Por fim, finalizamos a paraleliza√ß√£o, liberando os n√∫cleos que estavam sendo usados.\nAs fun√ß√µes tic() e toc() s√£o do pacote tictoc e servem para medir o tempo de execu√ß√£o de um bloco de c√≥digo.\n\ntic() inicia o cron√¥metro;\ntoc() exibe quanto tempo se passou desde o tic().\n\n\nVisualizando resultados\nVamos agora visualizar e explorar os resultados do tuning de v√°rios modelos no tidymodels, utilizando o objeto grid_resultados gerado com workflow_map().\n\nautoplot(grid_resultados)\n\n\n\n\n\n\n\n\n\nEsse gr√°fico apresenta os resultados do tuning de m√∫ltiplos modelos de classifica√ß√£o, avaliados com base em tr√™s m√©tricas: acur√°cia (painel da esquerda), brier score (painel do meio) e AUC da curva ROC (painel da direita). Cada ponto representa uma combina√ß√£o de hiperpar√¢metros testada durante a valida√ß√£o cruzada, e as barras verticais indicam a variabilidade do desempenho entre as dobras da valida√ß√£o cruzada.\n\n\nPainel da esquerda: Acur√°cia\n\nOs modelos Random Forest (rand_forest, cor azul) est√£o entre os melhores colocados, com valores de acur√°cia acima de 0,80 e baixa variabilidade, indicando desempenho est√°vel e alto.\nRedes Neurais (MLP) e Naive Bayes t√™m desempenho intermedi√°rio, com acur√°cias entre 0,75 e 0,78.\nO modelo KNN (nearest_neighbor) tem desempenho mais inst√°vel e geralmente inferior (acur√°cia &lt; 0,73), com grande varia√ß√£o entre as dobras.\nAs √Årvores de decis√£o (decision_tree) aparece no meio do gr√°fico, com desempenho moderado e alguma variabilidade.\n\n\n\nPainel do meio: Brier Score\n\nMede a calibra√ß√£o das probabilidades preditas: quanto menor, melhor.\nRandom Forest novamente se destaca com os menores valores de Brier Score (√≥tima calibra√ß√£o).\nNaive Bayes e KNN apresentam valores mais altos, indicando que as probabilidades previstas s√£o menos confi√°veis.\nA separa√ß√£o √© clara: os melhores modelos est√£o no extremo esquerdo (baixo Brier Score), os piores, no direito.\n\n\n\nPainel da direita: ROC AUC\n\nO padr√£o se repete: Random Forest lidera com maior AUC (&gt; 0,88), mostrando excelente capacidade de separa√ß√£o das classes.\nSVM e Redes Neurais tamb√©m apresentam bom desempenho com AUC entre 0,83 e 0,86.\nNovamente, KNN aparece entre os piores desempenhos, com AUC abaixo de 0,75 em v√°rios casos.\n\nPodemos gerar um gr√°fico mais espec√≠fico que o anterior, focando apenas na m√©trica de acur√°cia, exibindo apenas a melhor combina√ß√£o de hiperpar√¢metros para cada modelo, tornando a visualiza√ß√£o mais limpa.\n\nautoplot(\n  grid_resultados,\n  rank_metric = \"accuracy\",  \n  metric = \"accuracy\",       \n  select_best = TRUE \n)\n\n\n\n\n\n\n\n\n\nNovamente podemos observar que,\n\nO Random Forest se destaca como o melhor modelo em termos de acur√°cia e estabilidade.\nModelos simples, como Naive Bayes e √Årvore de Decis√£o, ainda entregam resultados razo√°veis e est√°veis.\nO KNN, por outro lado, tem desempenho inferior e alta incerteza, indicando que n√£o √© a melhor escolha para esse problema.\nRedes Neurais e SVM s√£o competitivos, mas com varia√ß√£o nos resultados entre os folds.\n\n\n\nAvaliando o melhor modelo no conjunto de testes\nAp√≥s analisarmos resultados do processo de treinamento, vamos extrair o melhor conjunto de hiperpar√¢metros para um modelo espec√≠fico (no caso, o Random Forest) com base nos resultados de tuning.\n\nmelhor_modelo = grid_resultados %&gt;% \n  extract_workflow_set_result(\"randon_forest\") %&gt;% \n  select_best(metric = \"accuracy\")\n\nO objeto melhor_modelo armazena os melhores hiperpar√¢metros do Random Forest, segundo a acur√°cia. Resta-nos agora realizar a etapa final do processo de modelagem preditiva com o modelo Random Forest ajustado, ou seja, finalizar o melhor workflow encontrado durante o tuning e avaliar seu desempenho no conjunto de teste.\n\nrf_ajustado = grid_resultados %&gt;% \n  extract_workflow(\"randon_forest\") %&gt;% \n  finalize_workflow(melhor_modelo) %&gt;% \n  last_fit(split = splits)\n\nCome√ßamos com o objeto grid_resultados, que cont√©m os resultados de todos os workflows avaliados (modelos + receitas + tuning) e extra√≠mos o workflow do modelo espec√≠fico chamado \"randon_forest\". Com os melhores hiperpar√¢metros encontrados (armazenados no objeto melhor_modelo), usamos a fun√ß√£o finalize_workflow() para inseri-los no workflow. O resultado √© um workflow ‚Äúcompleto‚Äù, pronto para ser treinado e testado usando a fun√ß√£o last_fit(split = splits), que garante que o modelo final seja ajustado com todos os dados de treino dispon√≠veis, sem valida√ß√£o cruzada, e avaliado diretamente no conjunto de teste.\nO objeto rf_ajustado √© um objeto de classe fit_resamples com os seguintes elementos:\n\nO modelo Random Forest treinado com os melhores hiperpar√¢metros;\nAs m√©tricas obtidas no conjunto de teste (como acur√°cia, AUC, sensibilidade, etc.);\nAs predi√ß√µes feitas no conjunto de teste (√∫til para gerar curva ROC, matriz de confus√£o, etc.).\n\nPodemos extrair as predi√ß√µes feitas pelo modelo Random Forest no conjunto de teste, ap√≥s o ajuste final com last_fit().\n\nrf_teste_resultados = rf_ajustado %&gt;% \n  collect_predictions()\n\nrf_teste_resultados\n\n# A tibble: 2,174 √ó 7\n   .pred_class .pred_FALSE .pred_TRUE id                .row Transported .config\n   &lt;fct&gt;             &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;fct&gt;       &lt;chr&gt;  \n 1 TRUE             0.0251     0.975  train/test split     1 FALSE       Prepro‚Ä¶\n 2 FALSE            0.890      0.110  train/test split     4 FALSE       Prepro‚Ä¶\n 3 TRUE             0.0251     0.975  train/test split    11 TRUE        Prepro‚Ä¶\n 4 FALSE            0.935      0.0653 train/test split    18 FALSE       Prepro‚Ä¶\n 5 TRUE             0.0152     0.985  train/test split    19 TRUE        Prepro‚Ä¶\n 6 TRUE             0.375      0.625  train/test split    20 TRUE        Prepro‚Ä¶\n 7 TRUE             0.296      0.704  train/test split    24 FALSE       Prepro‚Ä¶\n 8 TRUE             0.296      0.704  train/test split    27 FALSE       Prepro‚Ä¶\n 9 TRUE             0.0152     0.985  train/test split    35 TRUE        Prepro‚Ä¶\n10 FALSE            0.950      0.0497 train/test split    40 FALSE       Prepro‚Ä¶\n# ‚Ñπ 2,164 more rows\n\n\nO resultado armazenado no objeto rf_teste_resultados √© uma tabela com informa√ß√µes √∫teis para avalia√ß√£o. Com esse objeto, voc√™ pode:\n\nGerar uma matriz de confus√£o:\n\n\nrf_teste_resultados %&gt;% \n  conf_mat(truth=Transported, estimate=.pred_class)\n\n          Truth\nPrediction FALSE TRUE\n     FALSE   830  191\n     TRUE    249  904\n\n\nObservamos que:\n\nVerdadeiro Negativo (TN) = 830 ‚Üí o modelo previu FALSE, e de fato era FALSE.\nFalso Negativo (FN) = 191 ‚Üí o modelo previu FALSE, mas era TRUE.\nFalso Positivo (FP) = 249 ‚Üí o modelo previu TRUE, mas era FALSE.\nVerdadeiro Positivo (TP) = 904 ‚Üí o modelo previu TRUE, e de fato era TRUE.\n\n\n\nCalcular m√©tricas de avalia√ß√£o:\n\n\nrf_teste_resultados %&gt;% \n  conf_mat(truth=Transported, estimate=.pred_class) %&gt;% \n  summary()\n\n# A tibble: 13 √ó 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.798\n 2 kap                  binary         0.595\n 3 sens                 binary         0.769\n 4 spec                 binary         0.826\n 5 ppv                  binary         0.813\n 6 npv                  binary         0.784\n 7 mcc                  binary         0.596\n 8 j_index              binary         0.595\n 9 bal_accuracy         binary         0.797\n10 detection_prevalence binary         0.470\n11 precision            binary         0.813\n12 recall               binary         0.769\n13 f_meas               binary         0.790\n\n\nObservamos que:\n\nO modelo apresenta um desempenho robusto e equilibrado, com sensibilidade e especificidade pr√≥ximas (77% e 83%, respectivamente), o que √© positivo quando ambos os tipos de erro (falsos positivos e falsos negativos) t√™m import√¢ncia.\nO √≠ndice de Kappa (0,595) e o MCC (0,596) confirmam que o modelo est√° performando de forma significativamente melhor do que o acaso.\nA precis√£o (PPV) e o valor preditivo negativo (NPV) tamb√©m est√£o acima de 78%, indicando confian√ßa nas predi√ß√µes positivas e negativas.\nO Youden‚Äôs J index (~0,595) sugere que o modelo √© bom em separar as duas classes.\nA acur√°cia balanceada √© quase id√™ntica √† acur√°cia simples, o que indica que as classes est√£o relativamente equilibradas ou que o modelo n√£o est√° tendencioso.\n\n\n\nConstruir uma curva ROC:\n\n\nrf_teste_resultados %&gt;%\n  roc_curve(.pred_FALSE,truth = Transported) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\nrf_teste_resultados %&gt;%\n  roc_auc(.pred_FALSE,truth = Transported) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.871\n\n\nNote que a curva ROC do seu modelo est√° bem acima da linha diagonal, o que indica que ele tem bom poder discriminativo. O valor da m√©trica roc_auc (√Årea sob a Curva ROC) apresentado √© 0,871. Isso indica que o modelo possui excelente capacidade discriminativa, ou seja, ele consegue distinguir com alta precis√£o entre as classes positivas (transportados) e negativas (n√£o transportados).\nNa etapa final da nossa an√°lise, utilizamos as curvas de ganho acumulado e curva lift para avaliar a capacidade do modelo Random Forest em identificar corretamente os passageiros que seriam transportados, priorizando aqueles com maior probabilidade prevista. A curva de ganho acumulado nos mostra, de forma intuitiva, o quanto o modelo consegue capturar dos casos positivos √† medida que aumentamos a propor√ß√£o da amostra analisada. J√° a curva lift indica o quanto o modelo √© mais eficaz do que uma sele√ß√£o aleat√≥ria em identificar corretamente os casos de interesse nos percentis superiores da base. Essas curvas s√£o especialmente √∫teis em contextos de prioriza√ß√£o, como marketing, cr√©dito e sa√∫de, pois evidenciam o valor pr√°tico do modelo ao permitir a√ß√µes mais eficazes com base nas previs√µes geradas.\n\nrf_teste_resultados %&gt;%\n  gain_curve(truth = Transported, .pred_FALSE) %&gt;%\n  autoplot() +\n  labs(title = \"Curva de Ganho Acumulado - Random Forest\",\n       x = \"Propor√ß√£o da Amostra\",\n       y = \"Propor√ß√£o de Positivos Acumulados\")\n\n\n\n\n\n\n\n\nA curva de ganho acumulado mostra o desempenho do modelo Random Forest em identificar corretamente os passageiros transportados (classe positiva) ao longo de diferentes propor√ß√µes da amostra classificada com maior probabilidade. No in√≠cio do gr√°fico, a curva cresce de forma ligeiramente superior √† linha de refer√™ncia aleat√≥ria (a diagonal da √°rea sombreada), indicando que o modelo consegue capturar mais passageiros transportados do que o acaso. √Ä medida que mais indiv√≠duos s√£o inclu√≠dos na amostra (avan√ßando no eixo X), a curva mant√©m um crescimento constante, mas sem uma inclina√ß√£o muito acentuada. Isso sugere que o modelo consegue priorizar positivamente os casos, mas sem uma separa√ß√£o muito forte entre as classes. Ao atingir cerca de 75% da amostra, o modelo j√° identificou aproximadamente 90% dos passageiros realmente transportados, o que representa uma vantagem sobre uma sele√ß√£o aleat√≥ria, embora n√£o seja uma curva altamente eficiente (ela n√£o se destaca muito da diagonal). Nos percentis finais (acima de 90% da amostra), a curva se aproxima de 100% dos positivos, como esperado, j√° que toda a amostra √© considerada. A regi√£o acima da linha diagonal (no topo da √°rea sombreada) representa a performance m√°xima poss√≠vel que um modelo ideal poderia atingir.\n\nrf_teste_resultados %&gt;%\n  lift_curve(truth = Transported, .pred_FALSE) %&gt;%\n  autoplot() +\n  labs(title = \"Curva Lift - Random Forest\",\n       x = \"Propor√ß√£o da Amostra\",\n       y = \"Lift\")\n\n\n\n\n\n\n\n\nO gr√°fico mostra o desempenho do modelo Random Forest em termos de lift, ou seja, o quanto o modelo melhora a identifica√ß√£o de passageiros transportados (classe positiva) em rela√ß√£o a uma sele√ß√£o aleat√≥ria. Podemos observar que, nos primeiros percentis (at√© aproximadamente 5% da amostra), o modelo apresenta um lift superior a 2,0, o que significa que, entre os primeiros 5% dos indiv√≠duos com maior probabilidade prevista de transporte, o modelo est√° encontrando mais que o dobro de positivos em compara√ß√£o ao acaso. H√° uma pequena oscila√ß√£o inicial, t√≠pica de amostras muito pequenas, mas ainda dentro de um bom desempenho. A medida que s√£o apresentados mais indiv√≠duos ao modelo, o lift permanece acima de 1,5, indicando que o modelo continua sendo significativamente melhor que o aleat√≥rio. A curva mostra uma queda suave e cont√≠nua, indicando que o modelo perde gradualmente poder de discrimina√ß√£o ao incluir observa√ß√µes com menor probabilidade prevista. Observamos ainda que, ap√≥s 75% da amostra ser apresentada, o lift se aproxima de 1,0, o que indica que, nesse ponto, o modelo n√£o est√° mais distinguindo bem entre positivos e negativos. Esse comportamento √© esperado, pois quando toda a base √© considerada, a propor√ß√£o de positivos reflete a preval√™ncia real e o modelo se equivale ao acaso.\nCom base nas curvas de ganho acumulado e curva lift apresentadas, √© poss√≠vel concluir que o modelo Random Forest demonstra um desempenho razo√°vel e consistente, especialmente quando o objetivo √© priorizar corretamente os passageiros que seriam transportados (classe positiva) entre os primeiros percentis da amostra. O Random Forest apresenta um bom poder de prioriza√ß√£o nos percentis iniciais, o que o torna √∫til para aplica√ß√µes que exigem foco em uma parte mais restrita da popula√ß√£o (por exemplo, campanhas, triagens ou aloca√ß√£o de recursos). Embora n√£o seja um modelo perfeito, ele supera o acaso com folga e fornece valor preditivo pr√°tico, especialmente em contextos onde √© importante identificar rapidamente os indiv√≠duos mais prov√°veis de serem positivos."
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#como-as-vari√°veis-influenciam-as-decis√µes",
    "href": "analise_dados/space_titanic_workflow.html#como-as-vari√°veis-influenciam-as-decis√µes",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "Como as vari√°veis influenciam as decis√µes?",
    "text": "Como as vari√°veis influenciam as decis√µes?\nAp√≥s treinarmos e avaliarmos o modelo Random Forest, uma etapa essencial √© entender como ele est√° tomando suas decis√µes. Para isso, utilizamos os valores SHAP (SHapley Additive exPlanations), uma t√©cnica de interpretabilidade que nos permite quantificar a contribui√ß√£o de cada vari√°vel para a predi√ß√£o de cada observa√ß√£o individual.\nA ideia √© responder perguntas como:\n\nQuais vari√°veis mais influenciaram a previs√£o de que um passageiro seria transportado?\nEm que medida cada caracter√≠stica aumentou ou diminuiu essa probabilidade?\n\nNeste exemplo, utilizamos o pacote shapviz, que facilita o uso de SHAP em modelos do tidymodels, e a fun√ß√£o kernelshap(), que √© compat√≠vel com modelos como o Random Forest. Aplicamos o pr√©-processamento usado no treinamento, extra√≠mos o modelo ajustado e selecionamos uma amostra aleat√≥ria dos dados como base de refer√™ncia para calcular os valores SHAP.\nCom isso, conseguimos produzir visualiza√ß√µes que mostram tanto a import√¢ncia global das vari√°veis quanto explica√ß√µes locais, observa√ß√£o por observa√ß√£o. Isso nos ajuda a confiar mais no modelo, a identificar padr√µes importantes e at√© a comunicar melhor os resultados para p√∫blicos n√£o t√©cnicos.\nA seguir, aplicamos o c√≥digo que prepara e calcula os valores SHAP para nosso modelo Random Forest.\n\nrf_fit = extract_fit_parsnip(rf_ajustado)\ndadosP = bake(prep(zv_recipe), dados_treino)%&gt;% dplyr::select(-c('Transported'))\nbackground_data = dadosP[sample(nrow(dadosP), 50),]\n\n\ndoParallel::registerDoParallel(cores = detectCores()-1)\n\ntic()\n\nshap_values = kernelshap(rf_fit,dadosP, bg_X = background_data, type = \"prob\", parallel = TRUE)\n\nKernel SHAP values by the hybrid strategy of degree 2\n\ntoc()\n\n1451.75 sec elapsed\n\ndoParallel::stopImplicitCluster()\n\nA fun√ß√£o shapviz() transforma os valores SHAP calculados com kernelshap() em um objeto que pode ser facilmente explorado e visualizado.\n\nsv = shapviz(shap_values, which_class = 1)\n\nA fun√ß√£o sv_importance() faz parte do pacote shapviz e √© usada para extrair a import√¢ncia das vari√°veis com base nos valores SHAP.\n\nsv_importance(sv)\n\n\n\n\n\n\n\n\nEsse gr√°fico exibe a import√¢ncia global das vari√°veis para o modelo Random Forest com base nos valores SHAP, medidos pela m√©dia dos valores absolutos de SHAP de cada vari√°vel. Note que despesasSuperfluas √©, de longe, a vari√°vel mais influente no modelo, com um valor m√©dio de SHAP superior a 0.15. Isso indica que ela tem grande poder discriminativo para prever se um passageiro foi transportado ou n√£o. despesasBasicas, HomePlanet e IsEarth tamb√©m t√™m papel importante, mas com impacto consideravelmente menor.\nVari√°veis como PassengerId e Name apresentam import√¢ncia quase nula, sugerindo que n√£o contribuem significativamente para as decis√µes do modelo (o que √© esperado, pois s√£o mais identificadores do que vari√°veis explicativas). Vari√°veis relacionadas ao conforto da viagem (RoomService, Spa, ShoppingMall, etc.) t√™m impacto moderado, o que faz sentido em um contexto onde o perfil de consumo pode indicar o comportamento do passageiro.\n\nsv_importance(sv, kind = \"bee\")\n\n\n\n\n\n\n\n\nUma alternativa ao gr√°fico √© o beeswarm plot de valores SHAP, gerado a partir da fun√ß√£o shapviz() com kind = \"beeswarm\". Ele fornece uma vis√£o detalhada sobre como cada vari√°vel influenciou as previs√µes do modelo Random Forest para diferentes observa√ß√µes. Podemos observar que quanto maiores os gastos com despesas sup√©rfluas (pontos mais claros), maior a contribui√ß√£o positiva para a previs√£o da classe positiva. Isso sugere que passageiros com altos gastos t√™m maior chance de serem transportados. A vari√°vel despesasBasicas tamb√©m influencia positivamente, mas com menor impacto e menor dispers√£o. Al√©m disso,\n\nHomePlanet e IsEarth: mostram varia√ß√£o relevante, com valores altos (ou baixos, dependendo da codifica√ß√£o) puxando o SHAP para dire√ß√µes opostas.\nCryoSleep, CabinDeck, VRDeck, ShoppingMall, Spa, etc.: t√™m efeitos mais variados. Em alguns indiv√≠duos aumentam a probabilidade de transporte, em outros diminuem. Isso indica intera√ß√µes complexas com o desfecho.\nPassengerId e Name: praticamente n√£o afetam a predi√ß√£o (como esperado), com valores SHAP pr√≥ximos de zero.\n\nA fun√ß√£o sv_dependence(sv, \"despesasSuperfluas\") gera um gr√°fico de depend√™ncia SHAP para a vari√°vel despesasSuperfluas. Esse tipo de gr√°fico ajuda a visualizar como os valores dessa vari√°vel impactam as previs√µes do modelo em termos dos valores SHAP.\n\nsv_dependence(sv, \"despesasSuperfluas\")\n\n\n\n\n\n\n\n\nO gr√°fico apresentado √© uma curva de depend√™ncia SHAP para a vari√°vel despesasSuperfluas, com os pontos coloridos de acordo com a vari√°vel categ√≥rica IsEarth. Ele nos permite entender:\n\nCorrela√ß√£o positiva geral: Conforme o valor de despesasSuperfluas aumenta, os valores SHAP tamb√©m aumentam. Isso indica que gastos maiores com itens sup√©rfluos aumentam a probabilidade de o modelo prever que o passageiro foi transportado.\nIntera√ß√£o com a vari√°vel IsEarth: Passageiros que n√£o estavam na Terra (roxos) apresentam, em geral, valores SHAP mais altos para os mesmos n√≠veis de despesasSuperfluas, especialmente para valores m√©dios e altos (acima de 2,5). J√° os passageiros vindos da Terra (amarelos) t√™m valores SHAP consistentemente mais baixos, mesmo quando gastam bastante. Isso sugere que a origem do passageiro influencia a interpreta√ß√£o do modelo sobre o efeito do gasto sup√©rfluo: Gastar com sup√©rfluos tem mais peso positivo para prever transporte quando o passageiro n√£o vem da Terra.\nRegi√£o de baixo gasto (pr√≥xima a 0): Para valores baixos de despesasSuperfluas, os valores SHAP s√£o geralmente negativos (efeito desfavor√°vel na previs√£o). A distin√ß√£o entre IsEarth = 0 e IsEarth = 1 √© menos clara nesse intervalo, indicando que o impacto da origem do passageiro √© mais evidente ap√≥s certo n√≠vel de gasto.\n\nSendo assim, o gr√°fico mostra que o modelo Random Forest est√° capturando uma intera√ß√£o relevante entre o local de origem (IsEarth) e o padr√£o de gasto com sup√©rfluos. Em termos pr√°ticos:\n\nGastos sup√©rfluos s√£o um indicativo importante de transporte, mas o peso dessa informa√ß√£o depende da origem do passageiro."
  },
  {
    "objectID": "analise_dados/space_titanic_workflow.html#e-se-quisermos-saber-sobre-uma-predi√ß√£o-espec√≠fica",
    "href": "analise_dados/space_titanic_workflow.html#e-se-quisermos-saber-sobre-uma-predi√ß√£o-espec√≠fica",
    "title": "Construindo workflows preditivos usando a base de dados Space Titanic",
    "section": "E se quisermos saber sobre uma predi√ß√£o espec√≠fica?",
    "text": "E se quisermos saber sobre uma predi√ß√£o espec√≠fica?\nDiscutir a import√¢ncia local das vari√°veis em modelos preditivos √© fundamental para entender por que o modelo tomou determinada decis√£o para um passageiro espec√≠fico da base de dados Space Titanic.\nEnquanto a import√¢ncia global nos mostra, em m√©dia, quais vari√°veis mais influenciam o modelo em todo o conjunto de dados, a import√¢ncia local revela, por exemplo, por que o modelo classificou um determinado passageiro como ‚Äútransportado‚Äù ou ‚Äún√£o transportado‚Äù. Isso permite identificar quais fatores individuais, como gastos em servi√ßos de bordo, planeta de origem ou cabine, foram decisivos para a predi√ß√£o daquele passageiro.\nEssa an√°lise √© especialmente √∫til para investigar casos espec√≠ficos, avaliar poss√≠veis vieses e construir explica√ß√µes mais transparentes e confi√°veis, fundamentais quando se deseja interpretar o comportamento do modelo de forma personalizada.\nUma maneira eficaz de explorar a import√¢ncia local √© utilizando a fun√ß√£o sv_force(), do pacote shapviz. Esse comando gera uma visualiza√ß√£o conhecida como gr√°fico de for√ßa (force plot), que mostra, de forma intuitiva, como cada vari√°vel contribuiu para aumentar ou diminuir a probabilidade de o modelo prever que esse passageiro seria transportado. Suponha que desejamos entender por que o indiv√≠duo de n√∫mero 10 em nossa amostra foi classificado como transportado (Transported = TRUE).\n\nsv_force(sv[10])\n\n\n\n\n\n\n\n\nPodemos observar que:\n\nE[f(x)] = 0,523: √© o valor de predi√ß√£o m√©dio do modelo, ou seja, a probabilidade m√©dia de transporte considerando todos os passageiros.\nf(x) = 0,892: √© a probabilidade prevista pelo modelo para este passageiro em espec√≠fico. Isso significa que o modelo prev√™ que esse indiv√≠duo tem 89,2% de chance de ser transportado.\nAs setas amarelas representam o impacto de cada vari√°vel no deslocamento do valor m√©dio (E[f(x)]) at√© o valor final previsto (f(x)):\n\ndespesasSuperfluas = 3 ‚Üí +0,126. A principal contribui√ß√£o positiva: o gasto com despesas sup√©rfluas elevou significativamente a previs√£o.\nSpa = 3 ‚Üí +0,0766. Gastos com o spa tamb√©m influenciaram fortemente para aumentar a probabilidade de transporte.\n8 other features ‚Üí +0,0502. Um conjunto de outras vari√°veis teve um pequeno efeito somado.\ndespesasBasicas = 2,35 ‚Üí +0,0483. Essa vari√°vel contribuiu positivamente, aumentando um pouco a probabilidade.\nIsEarth = 1 ‚Üí +0,0412. Indica que o fato do passageiro ser da Terra teve efeito positivo na previs√£o.\nHomePlanet = Earth ‚Üí +0,0265. Aparece como fator categ√≥rico refor√ßando a influ√™ncia acima.\n\n\nO gr√°fico abaixo representa a mesma ideia, com visual mais sequencial: da base m√©dia do modelo at√© a previs√£o individual.\n\nsv_waterfall(sv[10])\n\n\n\n\n\n\n\n\nO modelo considera que esse passageiro tem fortes ind√≠cios de ter sido transportado, principalmente devido a altos gastos com despesas sup√©rfluas e uso do Spa, o que pode estar associado a um perfil mais voltado ao conforto e luxo, possivelmente correlacionado com a experi√™ncia de transporte no espa√ßo. A explica√ß√£o √© individualizada, o que permite entender com transpar√™ncia por que esse caso espec√≠fico foi classificado dessa forma."
  }
]