---
title: "Reconhecimento de Padrões"
format: 
  revealjs:
    width: 1600
    height: 900
    footer: ""
    theme: quartomonothemer.scss
    slide-number: c/t
    show-slide-number: all
incremental: false
code-link: true
bibliography: references.bib
title-slide-attributes:
    data-background-image: /images/back.jpeg
    data-background-size: cover
    data-background-opacity: "0.3"
---

## Clusterização

<br>

<br>

Uma das habilidades mais básicas dos organismos vivos é a capacidade de agrupar objetos similares para produzir uma **taxonomia**, uma **classificação**, ou um **agrupamento**.

## **Humanos** se interessam por **categorizações**...

<br>


:::: {.columns}

::: {.column width="50%"}

<table>
<tr>
<th>Música</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/music.jpg){.nostretch fig-align="center" width="500px"}
</td>
<td>
<ul>
  <li>Rock</li>
  <li>Erudita</li>
  <li>Popular</li>
  <li>Jazz</li>
  <li>etc...</li>
</ul>
</td>
</tr>
</table>
:::

::: {.column width="50%" .fragment}


<table>
<tr>
<th>Filmes</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/movies.jpg){.nostretch fig-align="center" width="500px"}
</td>
<td>
<ul>
  <li>Animação</li>
  <li>Aventura</li>
  <li>Comédia</li>
  <li>Drama</li>
  <li>etc...</li>
</ul>
</td>
</tr>
</table>

:::

::::


## **Humanos** se interessam por **categorizações**...

<p align="center">  
Diversas **ciências** se baseiam na organização de objetos de acordo com suas similaridades
</p>

. . .

<table>
<tr>
<th>Biologia</th>
<th></th>
</tr>
<tr>
<td>

![](/images/cluster/macaco_homem.jpg){.nostretch fig-align="center" width="600px"}
</td>
<td>
<dl>
  <dt>Reino: Animalia</dt>
  <dt>Ramo: Chordata</dt>
  <dt>Classe: Mammalia</dt>
  <dt>Ordem: Primatas</dt>
  <dt>Família: <em>Hominidae</em></dt>
  <dt>Gênero: <em>Homo</em></dt>
  <dt>Espécie: <em>Homo sapiens</em></dt>
</dl>
</td>
</tr>
</table>


## O que é Clusterização?

<br>

Existem muitas situações nas quais não sabemos de **antemão** uma maneira **apropriada** de agrupar uma coleção de objetos de acordo com suas **similaridades**

. . .

Imagine que você tem uma cesta cheia de frutas diferentes. Como você poderia agrupá-las?


## O que é Clusterização?


<table>
<tr>
<th>Como agrupá-las?</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/cesta.jpg){.nostretch fig-align="center" width="420px"}
</td>
<td>
<ul>
  <li>Pela cor?</li>
  <li>Pelo tamanho?</li>
  <li>Pelo sabor?</li>
  <li>Ou algum outro critério?</li>
</ul>
</td>
</tr>
</table>


## O que é Clusterização?

A **clusterização** é uma técnica de mineração de dados que nos permite descobrir **padrões** e estruturas ocultas em conjuntos de dados. Ela agrupa objetos **similares** e os **separa** dos demais, formando clusters.


- Sem rótulos previamente conhecidos.


. . .


Os grupos são formados de maneira a **maximizar** a similaridade entre os elementos de um grupo (**similaridade intra-grupo**) e **minimizar** a similaridade entre elementos de grupos diferentes (**similaridade inter-grupos**)


## O que é Clusterização?


![](/images/cluster/relacoes.png){.nostretch fig-align="center" width="1000px"}


## Aplicações reais



<table>
<tr>
<th>Segmentação de clientes</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/cliente.png){.nostretch fig-align="center" width="1000px"}

<br>

<br>
</td>
<td>
<ul>
  <li>Identificar grupos de clientes com características semelhantes</li>
  <li>Analisar comportamentos e preferências de consumo</li>
  <li>Personalizar estratégias de marketing e comunicação</li>
  <li>Otimizar produtos e serviços para diferentes perfis</li>
  <li>Melhorar a experiência e fidelização dos clientes</li>
</ul>
</td>
</tr>
</table>


## Aplicações reais

<br>

<table>
<tr>
<th>Análise de perfis de usuários</th>
<th></th>
</tr>
<tr>
<td>

![](/images/cluster/perfil_usuario.png){.nostretch fig-align="center" width="500px"}
</td>
<td>
<ul>
  <li>Identificar diferentes perfis de usuários</li>
  <li>Analisar comportamentos e preferências</li>
  <li>Detectar padrões de interação</li>
  <li>Apoiar decisões estratégicas</li>
  <li>Personalizar serviços ou produtos</li>
</ul>
</td>
</tr>
</table>



## Aplicações reais

<table>
<tr>
<th>Agrupamento de genes</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/gene.png){.nostretch fig-align="center" width="1000px"}
</td>
<td>
<ul>
  <li>Identificar grupos de genes com padrões de expressão semelhantes</li>
  <li>Inferir funções biológicas compartilhadas entre os genes do mesmo grupo</li>
  <li>Detectar relações de co-regulação entre genes</li>
  <li>Revelar possíveis vias metabólicas ou processos celulares associados</li>
  <li>Apoiar a compreensão da organização funcional do genoma</li>
</ul>
</td>
</tr>
</table>




## Aplicações reais

<table>
<tr>
<th>Agrupamento de genes</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/gene.png){.nostretch fig-align="center" width="1000px"}
</td>
<td>
<ul>
  <li>Identificar grupos de genes com padrões de expressão semelhantes</li>
  <li>Inferir funções biológicas compartilhadas entre os genes do mesmo grupo</li>
  <li>Detectar relações de co-regulação entre genes</li>
  <li>Revelar possíveis vias metabólicas ou processos celulares associados</li>
  <li>Apoiar a compreensão da organização funcional do genoma</li>
</ul>
</td>
</tr>
</table>


## Aplicações reais

<table>
<tr>
<th>Agrupamento de regiões com base em indicadores sociais</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/regioes.png){.nostretch fig-align="center" width="1000px"}
</td>
<td>
<ul>
  <li>Identificar regiões com características sociais parecidas</li>
  <li>Entender melhor as desigualdades entre diferentes áreas</li>
  <li>Ajudar na criação de políticas públicas mais justas</li>
  <li>Facilitar a análise de grandes quantidades de dados sociais</li>
  <li>Planejar melhor os investimentos em infraestrutura e serviços</li>
</ul>
</td>
</tr>
</table>


## Métodos hierárquicos de clusterização

<br>

- Agrupamento hierárquico é uma técnica de clusterização baseada em uma estrutura de "árvore"

. . .

- Permite identificar relações de similaridade entre observações

. . .

- Pode ser aglomerativo (bottom-up) ou divisivo (top-down)

. . .

- Não é necessário especificar o número de clusters a priori


## Tipos de Agrupamento Hierárquico


- Aglomerativo (HAC - Hierarchical Agglomerative Clustering)
  - Começa com cada elemento em um cluster separado
  - Iterativamente une os pares de clusters mais semelhantes
  - Resulta em um dendrograma

. . .


- Divisivo
  - Começa com todos os dados em um único cluster
  - Divide recursivamente em subgrupos mais distintos
  - Menos comum na prática devido à complexidade computacional
  
  
## Medidas de Similaridade

<br>

- Baseadas em distância entre observações

  - Euclidiana (mais comum)
  - Manhattan
  - Correlação (Pearson)

. . .

- Resultam em uma matriz de distância (ou dissimilaridade)


## Critérios de Ligação

<br>

- Single linkage (vizinho mais próximo)

. . .

- Complete linkage (vizinho mais distante)


. . .


- Average linkage (média das distâncias)

. . .

- Ward (minimiza a variância dentro dos grupos)


## Dendrogramas

<br>

:::: {.columns}

::: {.column width="50%"}

<br>

- Representação visual do processo de fusão

- Permite explorar diferentes cortes para definir o número de clusters

- Altura dos ramos reflete a dissimilaridade entre grupos
:::

::: {.column width="50%" .fragment}

![](/images/cluster/dendro.png){.nostretch fig-align="center" width="800px"}


:::

::::


## Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

- Intuitivo e visual

- Não exige escolha prévia do número de grupos

- Aplica-se a dados qualitativos e quantitativos (com dissimilaridade adequada)
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}



<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}


- Sensível a outliers

- Complexidade computacional 

- Dificuldade em modificar estruturas já formadas

:::

::::


## Métodos não hierárquicos

<br>

- Agrupamento não hierárquico é uma abordagem de clusterização que não utiliza estrutura em árvore

. . .

- Mais eficiente para conjuntos de dados grandes

. . .

- Foco em descobrir grupos com base em similaridade de atributos


## Principais Métodos

<br>

- K-means
  - Requer a definição do número de clusters (k)
  - Agrupa observações minimizando a variância dentro dos clusters
  - Iterativo:
    - Inicialização dos centróides
    - Atribuição dos pontos ao cluster mais próximo
    - Atualização dos centróides


## Principais Métodos

<br>

- K-medoids (PAM)
  - Usa objetos reais como centróides (medoids)
  - Mais robusto a outliers
  - Pode ser usado com qualquer métrica de dissimilaridade
  

## K-means: Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

<p></p>

- Simples e rápido

- Escalável para grandes conjuntos de dados

- Fácil de interpretar e implementar
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}



<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}

<p></p>

- Sensível a outliers

- Requer especificar o número de clusters k

- Funciona melhor com clusters esféricos de tamanho semelhante

:::

::::



## K-medoids: Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

<p></p>

- Mais robusto a outliers

- Pode usar qualquer métrica de dissimilaridade

- Resultados mais estáveis em presença de ruído
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}

<p></p>

- Mais lento que K-means em grandes conjuntos de dados

- Pode exigir mais ajustes de parâmetros

:::

::::

# Métodos baseados em densidade

## Métodos baseados em densidade

São técnicas utilizadas na mineração de dados para identificar agrupamentos ou padrões em conjuntos de dados com base na **densidade dos elementos amostrais**.

. . .

Esses métodos levam em consideração a **proximidade** entre os indivíduos, agrupando aqueles que estão próximos uns dos outros em **regiões densas**.

. . .

Uma **região densa** é uma região onde cada ponto tem **muitos pontos** em sua **vizinhança**.


## Métodos baseados em densidade

![](/images/cluster/dens.jpg){.nostretch fig-align="center" width="800px"}

# Método DBSCAN

## Método DBSCAN

<br>

O método **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) é um dos algoritmos baseados em **densidade** mais conhecidos.

. . .

Vamos definir **densidade** como sendo o número de pontos dentro de um **raio específico** 

. . .

- Dois principais parâmetros:
  - `Eps`: raio de vizinhança.
  - `MinPts`: número mínimo de pontos para formar um cluster.
  
  
## Método DBSCAN

<br>

- Um ponto é um **ponto de núcleo** (`core point`) se ele tem mais que um número especificado de pontos (`MinPts`) dentro de `Eps`
  - Estes são os pontos que estão no **interior** de um grupo
  
. . .

- Um **ponto de fronteira** (`border point`) tem menos que `MinPts` dentro de `Eps` mas está na vizinhança de um ponto núcleo

. . .

- Um **ponto de ruído** (`noise point`) é um ponto que não é nem um ponto núcleo nem um ponto de fronteira.



## Método DBSCAN


![](/images/cluster/points2.png){.nostretch fig-align="center" width="1000px"}

## Método DBSCAN

<br>

Para encontrar os agrupamentos, o algoritmo DBSCAN faz uma **varredura** nas observações determinando **todos os pontos núcleo**. 

. . .

Faz-se a seguir uma varredura dos pontos núcleo fazendo as conexões a todos os pontos que estejam a uma distância menor do que (`Eps`).

. . .

Cada subconjunto de pontos conectados entre si (conectividade), forma um cluster.


## Método DBSCAN

![](/images/cluster/DBSCAN_tutorial.gif){.nostretch fig-align="center" width="800px"}

## Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

<p></p>

- Eficiente em tratar grandes bases de dados
- Menos sensível a ruídos
- Forma clusters de formato arbitrário
- Usuário não precisa determinar a quantidade de clusters
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}



<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}



- Sensível aos parâmetros de entrada (`Eps` e `MinPts`)
![](/images/cluster/tab_par_dbscan.png){.nostretch fig-align="center" width="1000px"}


:::

::::



## Determinação dos valores de Eps e MinPts

A escolha adequada dos parâmetros `Eps` (epsilon) e `MinPts` (mínimo de pontos) é crucial para o bom desempenho do DBSCAN, já que esses parâmetros definem o que é considerado uma região densa.

. . .

Regra prática para escolha de `MinPts`:

- Para dados com dimensão *d*, recomenda-se:
  - `MinPts` ≥ *d* + 1
  - Exemplo: para dados 2D, escolha pelo menos `MinPts` = 3 ou mais.

- Em geral, valores entre 4 e 10 funcionam bem para a maioria dos casos.


## Determinação dos valores de Eps e MinPts

A melhor prática para determinação de `Eps` é usar o gráfico da distância do k-vizinho mais próximo:

. . .

- Etapas:
  - Para cada ponto, calcule a distância ao seu k-ésimo vizinho mais próximo (geralmente k = `MinPts` - 1).
  - Ordene essas distâncias em ordem decrescente.
  - Plote as distâncias.
  - Procure o "cotovelo" no gráfico — o ponto onde a curva começa a subir rapidamente.

. . .

Esse valor de distância costuma ser um bom `Eps`.


## Determinação dos valores de Eps e MinPts

![](/images/cluster/minpts_eps.jpg){.nostretch fig-align="center" width="1300px"}

## Problema: Clusters com densidades diferentes

A Figura abaixo ilustra um desafio importante enfrentado pelo DBSCAN: a sensibilidade à variação na densidade dos dados.


![](/images/cluster/problema_dbscan.jpg){.nostretch fig-align="center" width="1300px"}


## Problema: Clusters com densidades diferentes

Observamos dois conjuntos de pontos (agrupamentos), um à esquerda (com os pontos A e B) e outro à direita (com os pontos C e D). Ambos os conjuntos parecem representar clusters distintos, mas com densidades diferentes:

. . .

🔴 **Cluster à esquerda (A e B):**
Denso, com muitos pontos próximos. Aqui, os pontos A e B provavelmente atendem ao critério de serem pontos núcleo, possuem ao menos `MinPts` vizinhos dentro de uma distância `Eps`.

. . .

🟡 **Cluster à direita (C e D):**
Mais esparso, ou seja, os pontos estão mais afastados uns dos outros. Mesmo que C e D estejam relativamente próximos, é possível que não atendam aos critérios de densidade definidos por `MinPts` e `Eps`.



## Problema: Clusters com densidades diferentes

<br>

- O algoritmo DBSCAN utiliza critérios fixos para formar agrupamentos:
  - Um raio (`Eps`) que define o que é "perto".
  - Um número mínimo de pontos (`MinPts`) para considerar uma região densa.

. . .
  
- Se `Eps` é **alto suficiente** para que C e D sejam detectados como clusters então A e B e a região a sua volta se tornarão um **único cluster**

. . .

- Se `Eps` é **baixo suficiente** para que A e B sejam detectados como clusters separados então C e D (e os objetos a seu redor) serão considerados **outliers**!


## Consequência prática

<br>

- DBSCAN não se adapta bem a dados com clusters de densidades muito diferentes.

. . .

- Um único par de valores de `Eps` e `MinPts` não consegue capturar simultaneamente regiões densas e esparsas.

. . .

- Em aplicações reais (por exemplo, agrupamento geográfico ou comportamento de usuários), isso pode levar à subdetecção de padrões legítimos em regiões menos densas.


## Soluções possíveis

<br>

- Ajustar manualmente `Eps` e `MinPts` para diferentes partes dos dados.

. . .

- Usar algoritmos mais flexíveis:
    - HDBSCAN: detecta clusters com diferentes densidades.
    - OPTICS: evita a necessidade de definir `Eps` fixo.

. . .

- Aplicar técnicas de redução de dimensionalidade para uniformizar densidades.


# Método Mean Shift

## Método Mean Shift

O método **Mean Shift** é um algoritmo baseado em **janela deslizante** que tenta encontrar áreas densas no conjunto de dados.

. . .

A ideia desse algoritmo é criar uma **região circular**, obtida através da **média ponderada** dos elementos presentes dentro dessa área, considerando a **distância** de cada ponto em relação ao ponto atual.

. . .

A região de maior concentração em de objetos determina a **direção de deslocamento** deste círculo na base de dados.


. . .

Quando o círculo **não se movimentar** mais na base de dados a iteração termina e o algoritmo é encerrado.


## Método Mean Shift

![](/images/cluster/gif02.gif){.nostretch fig-align="center" width="800px"}


## Método Mean Shift

![](/images/cluster/fig01.png){.nostretch fig-align="center" width="1100px"}

## Método Mean Shift

<br>

A ideia do algoritmo é calcular a **função densidade de probabilidade** para um conjunto de dados. Isso é feito através do conceito de **kernel density estimation (KDE)**

. . .

A **média ponderada** é calculada utilizando o kernel, que atribui pesos aos pontos de dados com base em sua proximidade ao ponto central.


## Método Mean Shift

![](/images/cluster/fig02.png){.nostretch fig-align="center" width="900px"}


## Método Mean Shift

![](/images/cluster/fig03.png){.nostretch fig-align="center" width="1100px"}


## Método Mean Shift

![](/images/cluster/gif01.gif){.nostretch fig-align="center" width="1100px"}


## Método Mean Shift

<br>

A **largura de banda (bandwidth)** define o raio da **vizinhança** que é considerado para calcular a média ponderada.

. . .

Se o valor do bandwidth for **muito grande**, os pontos de dados de diferentes clusters podem ser misturados, levando a um agrupamento **menos discriminativo**.

. . .

Se o valor do bandwidth for **muito pequeno**, podem surgir **clusters pequenos e insignificantes** ou até mesmo apenas um **único cluster** que englobe todos os pontos.


## Método Mean Shift

<br>

A **escolha** adequada do valor do bandwidth é **crucial** para obter resultados significativos com o algoritmo Mean Shift. 


. . .

Geralmente, esse valor é **ajustado empiricamente** ou através de **métodos de validação cruzada** para encontrar o valor que melhor se adapte aos dados específicos em questão. 

. . .

Uma abordagem comum é **experimentar** diferentes valores e selecionar aquele que produzir os **clusters mais relevantes** e bem definidos para a tarefa em questão.



## Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

<p></p>

- Não exige número de clusters pré-definido

- Capaz de detectar formas arbitrárias

- Baseado em densidade
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}



<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}



- Sensível ao parâmetro de `bandwidth`

- Alto custo computacional

- Muitos clusters se `bandwidth` for pequeno


:::

::::




# Métodos baseados em modelos

## Redes de Kohonen

**Redes de Kohonen**, também conhecidas como **Mapas Auto-Organizáveis (SOM - Self-Organizing Maps)**, são uma classe especial de algoritmos de aprendizado de máquina **não supervisionado**, desenvolvidas por Teuvo Kohonen em 1982.

. . .

Essas redes têm como objetivo **organizar** e **visualizar** dados complexos em um **espaço bidimensional** ou tridimensional, de forma que **padrões** e **estruturas ocultas** possam ser facilmente **identificados**.

. . .

O funcionamento das redes de Kohonen é inspirado na forma como o **cérebro humano** organiza informações e aprende a partir do **ambiente**.


## Redes de Kohonen

![Cérebro respondendo a estímulos](/images/cluster/cerebro2.gif){.nostretch fig-align="center" width="1400px"}


## Redes de Kohonen

<br>

A rede é composta por um **conjunto de neurônios** artificialmente conectados em uma estrutura **bidimensional**, que representa o mapa. Cada neurônio é associado a um **vetor de pesos**, com mesma dimensão da entrada, que inicialmente é **aleatório**.

. . .

Preserva a topologia: observações similares são mapeadas próximas


. . .

O treinamento das redes de Kohonen ocorre em **duas etapas:** Competição e Cooperação

. . .

Princípio fundamental: **Aprendizagem competitiva**


## Redes de Kohonen: Competição

<br>

Durante essa fase, um dado de entrada é apresentado à rede, e cada neurônio calcula a sua **distância** em relação ao dado.

. . .

O neurônio que possui os pesos mais próximos do dado de entrada é o **vencedor (BMU: Best Matching Unit)**, ou seja, o neurônio com menor distância euclidiana.


## Redes de Kohonen: Cooperação

<br>

O neurônio vencedor, juntamente com seus neurônios vizinhos próximos, **atualiza seus pesos** para ficarem mais **similares** ao dado de entrada.

. . .

Essa **atualização** ajuda a agrupar os dados de entrada **similares** em **regiões próximas** do mapa.

. . .

Essas duas etapas de competição e cooperação são repetidas várias vezes durante o treinamento, e gradualmente, os neurônios se **organizam** em regiões distintas, formando agrupamentos e **revelando padrões** presentes nos dados.


## Redes de Kohonen

<table>
<tr>
<th>Competição</th>
<th>Cooperação</th>
</tr>
<tr>
<td>
![](/images/cluster/kohonen.jpeg){.nostretch fig-align="center" width="1000px"}
</td>
<td>
![](/images/cluster/SOM.gif){.nostretch fig-align="center" width="600px"}
</td>
</tr>
</table>


## Redes de Kohonen

A **vizinhança** de cada neurônio pode ser definida de acordo com a **forma geométrica** utilizada para representar os neurônios da rede

![](/images/cluster/topologia.png){.nostretch fig-align="center" width="1000px"}


## Redes de Kohonen

- **Hexagonal:** cada neurônio possui 6 vizinhos diretos

. . .

- **Retangular:** cada neurônio possui 4 vizinhos diretos

. . .

- Os pesos são **atualizados** segundo a **distância** ao neurônio **ganhador**

![](/images/cluster/eq.jpg){.nostretch fig-align="center" width="1200px"}

![](/images/cluster/eq2.jpg){.nostretch fig-align="center" width="1200px"}

## Redes de Kohonen

<br>

- É uma rede de **aprendizado não supervisionado**

. . .

- Os exemplos de entrada são comparados a **todos** os neurônios e o **mais próximo** é considerado o neurônio **vencedor (BMU)**

. . .

- Os pesos do nerônio vencedor são **atualizados** para **aproximar-se** mais do padrão de dados que representa


. . .


- É uma rede ideal para **detecção de agrupamentos**

. . .

- O **vetor de pesos** para um cluster serve como um **exemplar dos padrões** de entrada associados com esse cluster.




# Avaliação dos grupos

## Por que Avaliar Clusters?

<br>
 
- Clusterização é uma tarefa **não supervisionada**: não há rótulos corretos.
- Avaliar para:
  - Comparar algoritmos.
  - Ajustar hiperparâmetros.
  - Validar interpretação e coerência dos grupos.
  
  
## Tipos de Avaliação

<br>

| Tipo     | Requer rótulo? | Exemplo                  |
| -------- | -------------- | ------------------------ |
| Interna  | ❌ Não          | Silhueta, Davies-Bouldin |
| Externa  | ✅ Sim          | ARI, Homogeneidade, Completude| 
| Relativa | ❌ Não       | Gap Statistic, Elbow |


## Avaliação Interna: Índice de Silhueta

- Mede a **coerência** interna dos clusters.
- Intervalo: **[-1, 1]**

$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$

- $a(i)$: distância média no cluster  
- $b(i)$: distância ao cluster mais próximo



## Avaliação Interna: Índice de Silhueta

![](/images/cluster/silhueta.png){.nostretch fig-align="center" width="1600px"}


## Davies-Bouldin Index (DBI)

- Mede similaridade entre clusters

$$
DBI = \frac{1}{k} \sum_{i=1}^k \max_{j \ne i} \left( \frac{s_i + s_j}{d_{ij}} \right)
$$

- **Menor DBI** indica **melhor separação** e compacidade.


## Avaliação Externa: Adjusted Rand Index (ARI)

<br>

- Compara agrupamento com rótulos reais.
- Intervalo: [-1, 1]
  - 1: agrupamento perfeito
  - 0: aleatório
  - <0: pior que aleatório


## Avaliação Externa: Homogeneidade

- Mede se cada cluster contém apenas membros de uma única classe.

- Intervalo: [0, 1] (quanto mais próximo de 1, mais puro).


$$\text{Homogeneidade }(h) = 1 - \dfrac{H(C|K)}{H(C)}$$

em que:

$C$ = classes verdadeiras, $K$ = clusters atribuídos, $H(C|K)$ = entropia condicional das classes dado os clusters, $H(C)$ = entropia das classes


- **Alta homogeneidade** → cada cluster é formado majoritariamente por uma única classe real.




## Avaliação Externa: Completude

- Mede se todos os membros de uma mesma classe real estão alocados no mesmo cluster.

- Intervalo: [0, 1] (quanto mais próximo de 1, mais completo).


$$\text{Completude }(c) = 1 - \dfrac{H(K|C)}{H(K)}$$

em que:

$C$ = classes verdadeiras, $K$ = clusters atribuídos, $H(C|K)$ = entropia condicional dos clusters dado as classes, $H(C)$ = entropia dos clusters


- **Alta completude** → cada classe real está concentrada em um cluster.




## Avaliação Relativa



- **Elbow Method**: ponto de "cotovelo" indica número ideal de clusters.


![](/images/cluster/elbow.png){.nostretch fig-align="center" width="600px"}

## Avaliação Relativa


- **Gap Statistic**: compara a soma das distâncias intra-cluster (WCSS) com valor esperado sob aleatoriedade.

$$Gap(k) = E[\log(W_k)] - \log(W_k)$$

em que:

-$W_k:$  soma intra-cluster ($WCSS$) para $k$ clusters e $E[\log(W_k)]$ é valor esperado sob dados aleatórios (referência).


- O melhor número de clusters é aquele com maior $Gap(k)$


## Avaliação Visual

<br>

- PCA (Principal Component Analysis)
    - Técnica linear de redução de dimensionalidade
    - Projeta dados em componentes principais que explicam maior variância
    - Mantém estrutura global dos dados


- **Limitação:** não captura relações não lineares entre variáveis


## Avaliação Visual

- t-SNE (t-Distributed Stochastic Neighbor Embedding)
    - Preserva relações de vizinhança local
    - Muito bom para visualizar clusters complexos em 2D ou 3D
    - Mais lento e sensível a hiperparâmetros

. . .

- UMAP (Uniform Manifold Approximation and Projection)
    - Preserva estrutura global e local
    - Mais rápido e escalável que t-SNE 
    - Indicado para redução de dimensionalidade antes de clusterização


## Avaliação Visual

<br>

- Uso típico:
    - Reduzir dados de alta dimensão para 2D/3D
    - Plotar clusters coloridos para inspeção visual 
    - Avaliar separabilidade e sobreposição entre grupos
