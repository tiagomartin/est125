---
title: "Reconhecimento de Padr√µes"
format: 
  revealjs:
    width: 1600
    height: 900
    footer: ""
    theme: quartomonothemer.scss
    slide-number: c/t
    show-slide-number: all
incremental: false
code-link: true
bibliography: references.bib
title-slide-attributes:
    data-background-image: /images/back.jpeg
    data-background-size: cover
    data-background-opacity: "0.3"
---

## Clusteriza√ß√£o

<br>

<br>

Uma das habilidades mais b√°sicas dos organismos vivos √© a capacidade de agrupar objetos similares para produzir uma **taxonomia**, uma **classifica√ß√£o**, ou um **agrupamento**.

## **Humanos** se interessam por **categoriza√ß√µes**...

<br>


:::: {.columns}

::: {.column width="50%"}

<table>
<tr>
<th>M√∫sica</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/music.jpg){.nostretch fig-align="center" width="500px"}
</td>
<td>
<ul>
  <li>Rock</li>
  <li>Erudita</li>
  <li>Popular</li>
  <li>Jazz</li>
  <li>etc...</li>
</ul>
</td>
</tr>
</table>
:::

::: {.column width="50%" .fragment}


<table>
<tr>
<th>Filmes</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/movies.jpg){.nostretch fig-align="center" width="500px"}
</td>
<td>
<ul>
  <li>Anima√ß√£o</li>
  <li>Aventura</li>
  <li>Com√©dia</li>
  <li>Drama</li>
  <li>etc...</li>
</ul>
</td>
</tr>
</table>

:::

::::


## **Humanos** se interessam por **categoriza√ß√µes**...

<p align="center">  
Diversas **ci√™ncias** se baseiam na organiza√ß√£o de objetos de acordo com suas similaridades
</p>

. . .

<table>
<tr>
<th>Biologia</th>
<th></th>
</tr>
<tr>
<td>

![](/images/cluster/macaco_homem.jpg){.nostretch fig-align="center" width="600px"}
</td>
<td>
<dl>
  <dt>Reino: Animalia</dt>
  <dt>Ramo: Chordata</dt>
  <dt>Classe: Mammalia</dt>
  <dt>Ordem: Primatas</dt>
  <dt>Fam√≠lia: <em>Hominidae</em></dt>
  <dt>G√™nero: <em>Homo</em></dt>
  <dt>Esp√©cie: <em>Homo sapiens</em></dt>
</dl>
</td>
</tr>
</table>


## O que √© Clusteriza√ß√£o?

<br>

Existem muitas situa√ß√µes nas quais n√£o sabemos de **antem√£o** uma maneira **apropriada** de agrupar uma cole√ß√£o de objetos de acordo com suas **similaridades**

. . .

Imagine que voc√™ tem uma cesta cheia de frutas diferentes. Como voc√™ poderia agrup√°-las?


## O que √© Clusteriza√ß√£o?


<table>
<tr>
<th>Como agrup√°-las?</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/cesta.jpg){.nostretch fig-align="center" width="420px"}
</td>
<td>
<ul>
  <li>Pela cor?</li>
  <li>Pelo tamanho?</li>
  <li>Pelo sabor?</li>
  <li>Ou algum outro crit√©rio?</li>
</ul>
</td>
</tr>
</table>


## O que √© Clusteriza√ß√£o?

A **clusteriza√ß√£o** √© uma t√©cnica de minera√ß√£o de dados que nos permite descobrir **padr√µes** e estruturas ocultas em conjuntos de dados. Ela agrupa objetos **similares** e os **separa** dos demais, formando clusters.


- Sem r√≥tulos previamente conhecidos.


. . .


Os grupos s√£o formados de maneira a **maximizar** a similaridade entre os elementos de um grupo (**similaridade intra-grupo**) e **minimizar** a similaridade entre elementos de grupos diferentes (**similaridade inter-grupos**)


## O que √© Clusteriza√ß√£o?


![](/images/cluster/relacoes.png){.nostretch fig-align="center" width="1000px"}


## Aplica√ß√µes reais



<table>
<tr>
<th>Segmenta√ß√£o de clientes</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/cliente.png){.nostretch fig-align="center" width="1000px"}

<br>

<br>
</td>
<td>
<ul>
  <li>Identificar grupos de clientes com caracter√≠sticas semelhantes</li>
  <li>Analisar comportamentos e prefer√™ncias de consumo</li>
  <li>Personalizar estrat√©gias de marketing e comunica√ß√£o</li>
  <li>Otimizar produtos e servi√ßos para diferentes perfis</li>
  <li>Melhorar a experi√™ncia e fideliza√ß√£o dos clientes</li>
</ul>
</td>
</tr>
</table>


## Aplica√ß√µes reais

<br>

<table>
<tr>
<th>An√°lise de perfis de usu√°rios</th>
<th></th>
</tr>
<tr>
<td>

![](/images/cluster/perfil_usuario.png){.nostretch fig-align="center" width="500px"}
</td>
<td>
<ul>
  <li>Identificar diferentes perfis de usu√°rios</li>
  <li>Analisar comportamentos e prefer√™ncias</li>
  <li>Detectar padr√µes de intera√ß√£o</li>
  <li>Apoiar decis√µes estrat√©gicas</li>
  <li>Personalizar servi√ßos ou produtos</li>
</ul>
</td>
</tr>
</table>



## Aplica√ß√µes reais

<table>
<tr>
<th>Agrupamento de genes</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/gene.png){.nostretch fig-align="center" width="1000px"}
</td>
<td>
<ul>
  <li>Identificar grupos de genes com padr√µes de express√£o semelhantes</li>
  <li>Inferir fun√ß√µes biol√≥gicas compartilhadas entre os genes do mesmo grupo</li>
  <li>Detectar rela√ß√µes de co-regula√ß√£o entre genes</li>
  <li>Revelar poss√≠veis vias metab√≥licas ou processos celulares associados</li>
  <li>Apoiar a compreens√£o da organiza√ß√£o funcional do genoma</li>
</ul>
</td>
</tr>
</table>




## Aplica√ß√µes reais

<table>
<tr>
<th>Agrupamento de genes</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/gene.png){.nostretch fig-align="center" width="1000px"}
</td>
<td>
<ul>
  <li>Identificar grupos de genes com padr√µes de express√£o semelhantes</li>
  <li>Inferir fun√ß√µes biol√≥gicas compartilhadas entre os genes do mesmo grupo</li>
  <li>Detectar rela√ß√µes de co-regula√ß√£o entre genes</li>
  <li>Revelar poss√≠veis vias metab√≥licas ou processos celulares associados</li>
  <li>Apoiar a compreens√£o da organiza√ß√£o funcional do genoma</li>
</ul>
</td>
</tr>
</table>


## Aplica√ß√µes reais

<table>
<tr>
<th>Agrupamento de regi√µes com base em indicadores sociais</th>
<th></th>
</tr>
<tr>
<td>
![](/images/cluster/regioes.png){.nostretch fig-align="center" width="1000px"}
</td>
<td>
<ul>
  <li>Identificar regi√µes com caracter√≠sticas sociais parecidas</li>
  <li>Entender melhor as desigualdades entre diferentes √°reas</li>
  <li>Ajudar na cria√ß√£o de pol√≠ticas p√∫blicas mais justas</li>
  <li>Facilitar a an√°lise de grandes quantidades de dados sociais</li>
  <li>Planejar melhor os investimentos em infraestrutura e servi√ßos</li>
</ul>
</td>
</tr>
</table>


## M√©todos hier√°rquicos de clusteriza√ß√£o

<br>

- Agrupamento hier√°rquico √© uma t√©cnica de clusteriza√ß√£o baseada em uma estrutura de "√°rvore"

. . .

- Permite identificar rela√ß√µes de similaridade entre observa√ß√µes

. . .

- Pode ser aglomerativo (bottom-up) ou divisivo (top-down)

. . .

- N√£o √© necess√°rio especificar o n√∫mero de clusters a priori


## Tipos de Agrupamento Hier√°rquico


- Aglomerativo (HAC - Hierarchical Agglomerative Clustering)
  - Come√ßa com cada elemento em um cluster separado
  - Iterativamente une os pares de clusters mais semelhantes
  - Resulta em um dendrograma

. . .


- Divisivo
  - Come√ßa com todos os dados em um √∫nico cluster
  - Divide recursivamente em subgrupos mais distintos
  - Menos comum na pr√°tica devido √† complexidade computacional
  
  
## Medidas de Similaridade

<br>

- Baseadas em dist√¢ncia entre observa√ß√µes

  - Euclidiana (mais comum)
  - Manhattan
  - Correla√ß√£o (Pearson)

. . .

- Resultam em uma matriz de dist√¢ncia (ou dissimilaridade)


## Crit√©rios de Liga√ß√£o

<br>

- Single linkage (vizinho mais pr√≥ximo)

. . .

- Complete linkage (vizinho mais distante)


. . .


- Average linkage (m√©dia das dist√¢ncias)

. . .

- Ward (minimiza a vari√¢ncia dentro dos grupos)


## Dendrogramas

<br>

:::: {.columns}

::: {.column width="50%"}

<br>

- Representa√ß√£o visual do processo de fus√£o

- Permite explorar diferentes cortes para definir o n√∫mero de clusters

- Altura dos ramos reflete a dissimilaridade entre grupos
:::

::: {.column width="50%" .fragment}

![](/images/cluster/dendro.png){.nostretch fig-align="center" width="800px"}


:::

::::


## Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

- Intuitivo e visual

- N√£o exige escolha pr√©via do n√∫mero de grupos

- Aplica-se a dados qualitativos e quantitativos (com dissimilaridade adequada)
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}



<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}


- Sens√≠vel a outliers

- Complexidade computacional 

- Dificuldade em modificar estruturas j√° formadas

:::

::::


## M√©todos n√£o hier√°rquicos

<br>

- Agrupamento n√£o hier√°rquico √© uma abordagem de clusteriza√ß√£o que n√£o utiliza estrutura em √°rvore

. . .

- Mais eficiente para conjuntos de dados grandes

. . .

- Foco em descobrir grupos com base em similaridade de atributos


## Principais M√©todos

<br>

- K-means
  - Requer a defini√ß√£o do n√∫mero de clusters (k)
  - Agrupa observa√ß√µes minimizando a vari√¢ncia dentro dos clusters
  - Iterativo:
    - Inicializa√ß√£o dos centr√≥ides
    - Atribui√ß√£o dos pontos ao cluster mais pr√≥ximo
    - Atualiza√ß√£o dos centr√≥ides


## Principais M√©todos

<br>

- K-medoids (PAM)
  - Usa objetos reais como centr√≥ides (medoids)
  - Mais robusto a outliers
  - Pode ser usado com qualquer m√©trica de dissimilaridade
  

## K-means: Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

<p></p>

- Simples e r√°pido

- Escal√°vel para grandes conjuntos de dados

- F√°cil de interpretar e implementar
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}



<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}

<p></p>

- Sens√≠vel a outliers

- Requer especificar o n√∫mero de clusters k

- Funciona melhor com clusters esf√©ricos de tamanho semelhante

:::

::::



## K-medoids: Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

<p></p>

- Mais robusto a outliers

- Pode usar qualquer m√©trica de dissimilaridade

- Resultados mais est√°veis em presen√ßa de ru√≠do
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}

<p></p>

- Mais lento que K-means em grandes conjuntos de dados

- Pode exigir mais ajustes de par√¢metros

:::

::::

# M√©todos baseados em densidade

## M√©todos baseados em densidade

S√£o t√©cnicas utilizadas na minera√ß√£o de dados para identificar agrupamentos ou padr√µes em conjuntos de dados com base na **densidade dos elementos amostrais**.

. . .

Esses m√©todos levam em considera√ß√£o a **proximidade** entre os indiv√≠duos, agrupando aqueles que est√£o pr√≥ximos uns dos outros em **regi√µes densas**.

. . .

Uma **regi√£o densa** √© uma regi√£o onde cada ponto tem **muitos pontos** em sua **vizinhan√ßa**.


## M√©todos baseados em densidade

![](/images/cluster/dens.jpg){.nostretch fig-align="center" width="800px"}

# M√©todo DBSCAN

## M√©todo DBSCAN

<br>

O m√©todo **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) √© um dos algoritmos baseados em **densidade** mais conhecidos.

. . .

Vamos definir **densidade** como sendo o n√∫mero de pontos dentro de um **raio espec√≠fico** 

. . .

- Dois principais par√¢metros:
  - `Eps`: raio de vizinhan√ßa.
  - `MinPts`: n√∫mero m√≠nimo de pontos para formar um cluster.
  
  
## M√©todo DBSCAN

<br>

- Um ponto √© um **ponto de n√∫cleo** (`core point`) se ele tem mais que um n√∫mero especificado de pontos (`MinPts`) dentro de `Eps`
  - Estes s√£o os pontos que est√£o no **interior** de um grupo
  
. . .

- Um **ponto de fronteira** (`border point`) tem menos que `MinPts` dentro de `Eps` mas est√° na vizinhan√ßa de um ponto n√∫cleo

. . .

- Um **ponto de ru√≠do** (`noise point`) √© um ponto que n√£o √© nem um ponto n√∫cleo nem um ponto de fronteira.



## M√©todo DBSCAN


![](/images/cluster/points2.png){.nostretch fig-align="center" width="1000px"}

## M√©todo DBSCAN

<br>

Para encontrar os agrupamentos, o algoritmo DBSCAN faz uma **varredura** nas observa√ß√µes determinando **todos os pontos n√∫cleo**. 

. . .

Faz-se a seguir uma varredura dos pontos n√∫cleo fazendo as conex√µes a todos os pontos que estejam a uma dist√¢ncia menor do que (`Eps`).

. . .

Cada subconjunto de pontos conectados entre si (conectividade), forma um cluster.


## M√©todo DBSCAN

![](/images/cluster/DBSCAN_tutorial.gif){.nostretch fig-align="center" width="800px"}

## Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

<p></p>

- Eficiente em tratar grandes bases de dados
- Menos sens√≠vel a ru√≠dos
- Forma clusters de formato arbitr√°rio
- Usu√°rio n√£o precisa determinar a quantidade de clusters
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}



<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}



- Sens√≠vel aos par√¢metros de entrada (`Eps` e `MinPts`)
![](/images/cluster/tab_par_dbscan.png){.nostretch fig-align="center" width="1000px"}


:::

::::



## Determina√ß√£o dos valores de Eps e MinPts

A escolha adequada dos par√¢metros `Eps` (epsilon) e `MinPts` (m√≠nimo de pontos) √© crucial para o bom desempenho do DBSCAN, j√° que esses par√¢metros definem o que √© considerado uma regi√£o densa.

. . .

Regra pr√°tica para escolha de `MinPts`:

- Para dados com dimens√£o *d*, recomenda-se:
  - `MinPts` ‚â• *d* + 1
  - Exemplo: para dados 2D, escolha pelo menos `MinPts` = 3 ou mais.

- Em geral, valores entre 4 e 10 funcionam bem para a maioria dos casos.


## Determina√ß√£o dos valores de Eps e MinPts

A melhor pr√°tica para determina√ß√£o de `Eps` √© usar o gr√°fico da dist√¢ncia do k-vizinho mais pr√≥ximo:

. . .

- Etapas:
  - Para cada ponto, calcule a dist√¢ncia ao seu k-√©simo vizinho mais pr√≥ximo (geralmente k = `MinPts` - 1).
  - Ordene essas dist√¢ncias em ordem decrescente.
  - Plote as dist√¢ncias.
  - Procure o "cotovelo" no gr√°fico ‚Äî o ponto onde a curva come√ßa a subir rapidamente.

. . .

Esse valor de dist√¢ncia costuma ser um bom `Eps`.


## Determina√ß√£o dos valores de Eps e MinPts

![](/images/cluster/minpts_eps.jpg){.nostretch fig-align="center" width="1300px"}

## Problema: Clusters com densidades diferentes

A Figura abaixo ilustra um desafio importante enfrentado pelo DBSCAN: a sensibilidade √† varia√ß√£o na densidade dos dados.


![](/images/cluster/problema_dbscan.jpg){.nostretch fig-align="center" width="1300px"}


## Problema: Clusters com densidades diferentes

Observamos dois conjuntos de pontos (agrupamentos), um √† esquerda (com os pontos A e B) e outro √† direita (com os pontos C e D). Ambos os conjuntos parecem representar clusters distintos, mas com densidades diferentes:

. . .

üî¥ **Cluster √† esquerda (A e B):**
Denso, com muitos pontos pr√≥ximos. Aqui, os pontos A e B provavelmente atendem ao crit√©rio de serem pontos n√∫cleo, possuem ao menos `MinPts` vizinhos dentro de uma dist√¢ncia `Eps`.

. . .

üü° **Cluster √† direita (C e D):**
Mais esparso, ou seja, os pontos est√£o mais afastados uns dos outros. Mesmo que C e D estejam relativamente pr√≥ximos, √© poss√≠vel que n√£o atendam aos crit√©rios de densidade definidos por `MinPts` e `Eps`.



## Problema: Clusters com densidades diferentes

<br>

- O algoritmo DBSCAN utiliza crit√©rios fixos para formar agrupamentos:
  - Um raio (`Eps`) que define o que √© "perto".
  - Um n√∫mero m√≠nimo de pontos (`MinPts`) para considerar uma regi√£o densa.

. . .
  
- Se `Eps` √© **alto suficiente** para que C e D sejam detectados como clusters ent√£o A e B e a regi√£o a sua volta se tornar√£o um **√∫nico cluster**

. . .

- Se `Eps` √© **baixo suficiente** para que A e B sejam detectados como clusters separados ent√£o C e D (e os objetos a seu redor) ser√£o considerados **outliers**!


## Consequ√™ncia pr√°tica

<br>

- DBSCAN n√£o se adapta bem a dados com clusters de densidades muito diferentes.

. . .

- Um √∫nico par de valores de `Eps` e `MinPts` n√£o consegue capturar simultaneamente regi√µes densas e esparsas.

. . .

- Em aplica√ß√µes reais (por exemplo, agrupamento geogr√°fico ou comportamento de usu√°rios), isso pode levar √† subdetec√ß√£o de padr√µes leg√≠timos em regi√µes menos densas.


## Solu√ß√µes poss√≠veis

<br>

- Ajustar manualmente `Eps` e `MinPts` para diferentes partes dos dados.

. . .

- Usar algoritmos mais flex√≠veis:
    - HDBSCAN: detecta clusters com diferentes densidades.
    - OPTICS: evita a necessidade de definir `Eps` fixo.

. . .

- Aplicar t√©cnicas de redu√ß√£o de dimensionalidade para uniformizar densidades.


# M√©todo Mean Shift

## M√©todo Mean Shift

O m√©todo **Mean Shift** √© um algoritmo baseado em **janela deslizante** que tenta encontrar √°reas densas no conjunto de dados.

. . .

A ideia desse algoritmo √© criar uma **regi√£o circular**, obtida atrav√©s da **m√©dia ponderada** dos elementos presentes dentro dessa √°rea, considerando a **dist√¢ncia** de cada ponto em rela√ß√£o ao ponto atual.

. . .

A regi√£o de maior concentra√ß√£o em de objetos determina a **dire√ß√£o de deslocamento** deste c√≠rculo na base de dados.


. . .

Quando o c√≠rculo **n√£o se movimentar** mais na base de dados a itera√ß√£o termina e o algoritmo √© encerrado.


## M√©todo Mean Shift

![](/images/cluster/gif02.gif){.nostretch fig-align="center" width="800px"}


## M√©todo Mean Shift

![](/images/cluster/fig01.png){.nostretch fig-align="center" width="1100px"}

## M√©todo Mean Shift

<br>

A ideia do algoritmo √© calcular a **fun√ß√£o densidade de probabilidade** para um conjunto de dados. Isso √© feito atrav√©s do conceito de **kernel density estimation (KDE)**

. . .

A **m√©dia ponderada** √© calculada utilizando o kernel, que atribui pesos aos pontos de dados com base em sua proximidade ao ponto central.


## M√©todo Mean Shift

![](/images/cluster/fig02.png){.nostretch fig-align="center" width="900px"}


## M√©todo Mean Shift

![](/images/cluster/fig03.png){.nostretch fig-align="center" width="1100px"}


## M√©todo Mean Shift

![](/images/cluster/gif01.gif){.nostretch fig-align="center" width="1100px"}


## M√©todo Mean Shift

<br>

A **largura de banda (bandwidth)** define o raio da **vizinhan√ßa** que √© considerado para calcular a m√©dia ponderada.

. . .

Se o valor do bandwidth for **muito grande**, os pontos de dados de diferentes clusters podem ser misturados, levando a um agrupamento **menos discriminativo**.

. . .

Se o valor do bandwidth for **muito pequeno**, podem surgir **clusters pequenos e insignificantes** ou at√© mesmo apenas um **√∫nico cluster** que englobe todos os pontos.


## M√©todo Mean Shift

<br>

A **escolha** adequada do valor do bandwidth √© **crucial** para obter resultados significativos com o algoritmo Mean Shift. 


. . .

Geralmente, esse valor √© **ajustado empiricamente** ou atrav√©s de **m√©todos de valida√ß√£o cruzada** para encontrar o valor que melhor se adapte aos dados espec√≠ficos em quest√£o. 

. . .

Uma abordagem comum √© **experimentar** diferentes valores e selecionar aquele que produzir os **clusters mais relevantes** e bem definidos para a tarefa em quest√£o.



## Vantagens e Desvantagens

:::: {.columns}

::: {.column width="10%"}

<span style='font-size:160px;'>&#128077;</span>
:::

::: {.column width="90%"}

<p></p>

- N√£o exige n√∫mero de clusters pr√©-definido

- Capaz de detectar formas arbitr√°rias

- Baseado em densidade
:::

::::

. . .


:::: {.columns}

::: {.column width="10%"}



<span style='font-size:160px;'>&#128078;</span>
:::

::: {.column width="90%"}



- Sens√≠vel ao par√¢metro de `bandwidth`

- Alto custo computacional

- Muitos clusters se `bandwidth` for pequeno


:::

::::




# M√©todos baseados em modelos

## Redes de Kohonen

**Redes de Kohonen**, tamb√©m conhecidas como **Mapas Auto-Organiz√°veis (SOM - Self-Organizing Maps)**, s√£o uma classe especial de algoritmos de aprendizado de m√°quina **n√£o supervisionado**, desenvolvidas por Teuvo Kohonen em 1982.

. . .

Essas redes t√™m como objetivo **organizar** e **visualizar** dados complexos em um **espa√ßo bidimensional** ou tridimensional, de forma que **padr√µes** e **estruturas ocultas** possam ser facilmente **identificados**.

. . .

O funcionamento das redes de Kohonen √© inspirado na forma como o **c√©rebro humano** organiza informa√ß√µes e aprende a partir do **ambiente**.


## Redes de Kohonen

![C√©rebro respondendo a est√≠mulos](/images/cluster/cerebro2.gif){.nostretch fig-align="center" width="1400px"}


## Redes de Kohonen

<br>

A rede √© composta por um **conjunto de neur√¥nios** artificialmente conectados em uma estrutura **bidimensional**, que representa o mapa. Cada neur√¥nio √© associado a um **vetor de pesos**, com mesma dimens√£o da entrada, que inicialmente √© **aleat√≥rio**.

. . .

Preserva a topologia: observa√ß√µes similares s√£o mapeadas pr√≥ximas


. . .

O treinamento das redes de Kohonen ocorre em **duas etapas:** Competi√ß√£o e Coopera√ß√£o

. . .

Princ√≠pio fundamental: **Aprendizagem competitiva**


## Redes de Kohonen: Competi√ß√£o

<br>

Durante essa fase, um dado de entrada √© apresentado √† rede, e cada neur√¥nio calcula a sua **dist√¢ncia** em rela√ß√£o ao dado.

. . .

O neur√¥nio que possui os pesos mais pr√≥ximos do dado de entrada √© o **vencedor (BMU: Best Matching Unit)**, ou seja, o neur√¥nio com menor dist√¢ncia euclidiana.


## Redes de Kohonen: Coopera√ß√£o

<br>

O neur√¥nio vencedor, juntamente com seus neur√¥nios vizinhos pr√≥ximos, **atualiza seus pesos** para ficarem mais **similares** ao dado de entrada.

. . .

Essa **atualiza√ß√£o** ajuda a agrupar os dados de entrada **similares** em **regi√µes pr√≥ximas** do mapa.

. . .

Essas duas etapas de competi√ß√£o e coopera√ß√£o s√£o repetidas v√°rias vezes durante o treinamento, e gradualmente, os neur√¥nios se **organizam** em regi√µes distintas, formando agrupamentos e **revelando padr√µes** presentes nos dados.


## Redes de Kohonen

<table>
<tr>
<th>Competi√ß√£o</th>
<th>Coopera√ß√£o</th>
</tr>
<tr>
<td>
![](/images/cluster/kohonen.jpeg){.nostretch fig-align="center" width="1000px"}
</td>
<td>
![](/images/cluster/SOM.gif){.nostretch fig-align="center" width="600px"}
</td>
</tr>
</table>


## Redes de Kohonen

A **vizinhan√ßa** de cada neur√¥nio pode ser definida de acordo com a **forma geom√©trica** utilizada para representar os neur√¥nios da rede

![](/images/cluster/topologia.png){.nostretch fig-align="center" width="1000px"}


## Redes de Kohonen

- **Hexagonal:** cada neur√¥nio possui 6 vizinhos diretos

. . .

- **Retangular:** cada neur√¥nio possui 4 vizinhos diretos

. . .

- Os pesos s√£o **atualizados** segundo a **dist√¢ncia** ao neur√¥nio **ganhador**

![](/images/cluster/eq.jpg){.nostretch fig-align="center" width="1200px"}

![](/images/cluster/eq2.jpg){.nostretch fig-align="center" width="1200px"}

## Redes de Kohonen

<br>

- √â uma rede de **aprendizado n√£o supervisionado**

. . .

- Os exemplos de entrada s√£o comparados a **todos** os neur√¥nios e o **mais pr√≥ximo** √© considerado o neur√¥nio **vencedor (BMU)**

. . .

- Os pesos do ner√¥nio vencedor s√£o **atualizados** para **aproximar-se** mais do padr√£o de dados que representa


. . .


- √â uma rede ideal para **detec√ß√£o de agrupamentos**

. . .

- O **vetor de pesos** para um cluster serve como um **exemplar dos padr√µes** de entrada associados com esse cluster.




# Avalia√ß√£o dos grupos

## Por que Avaliar Clusters?

<br>
 
- Clusteriza√ß√£o √© uma tarefa **n√£o supervisionada**: n√£o h√° r√≥tulos corretos.
- Avaliar para:
  - Comparar algoritmos.
  - Ajustar hiperpar√¢metros.
  - Validar interpreta√ß√£o e coer√™ncia dos grupos.
  
  
## Tipos de Avalia√ß√£o

<br>

| Tipo     | Requer r√≥tulo? | Exemplo                  |
| -------- | -------------- | ------------------------ |
| Interna  | ‚ùå N√£o          | Silhueta, Davies-Bouldin |
| Externa  | ‚úÖ Sim          | ARI, Homogeneidade, Completude| 
| Relativa | ‚ùå N√£o       | Gap Statistic, Elbow |


## Avalia√ß√£o Interna: √çndice de Silhueta

- Mede a **coer√™ncia** interna dos clusters.
- Intervalo: **[-1, 1]**

$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$

- $a(i)$: dist√¢ncia m√©dia no cluster  
- $b(i)$: dist√¢ncia ao cluster mais pr√≥ximo



## Avalia√ß√£o Interna: √çndice de Silhueta

![](/images/cluster/silhueta.png){.nostretch fig-align="center" width="1600px"}


## Davies-Bouldin Index (DBI)

- Mede similaridade entre clusters

$$
DBI = \frac{1}{k} \sum_{i=1}^k \max_{j \ne i} \left( \frac{s_i + s_j}{d_{ij}} \right)
$$

- **Menor DBI** indica **melhor separa√ß√£o** e compacidade.


## Avalia√ß√£o Externa: Adjusted Rand Index (ARI)

<br>

- Compara agrupamento com r√≥tulos reais.
- Intervalo: [-1, 1]
  - 1: agrupamento perfeito
  - 0: aleat√≥rio
  - <0: pior que aleat√≥rio


## Avalia√ß√£o Externa: Homogeneidade

- Mede se cada cluster cont√©m apenas membros de uma √∫nica classe.

- Intervalo: [0, 1] (quanto mais pr√≥ximo de 1, mais puro).


$$\text{Homogeneidade }(h) = 1 - \dfrac{H(C|K)}{H(C)}$$

em que:

$C$ = classes verdadeiras, $K$ = clusters atribu√≠dos, $H(C|K)$ = entropia condicional das classes dado os clusters, $H(C)$ = entropia das classes


- **Alta homogeneidade** ‚Üí cada cluster √© formado majoritariamente por uma √∫nica classe real.




## Avalia√ß√£o Externa: Completude

- Mede se todos os membros de uma mesma classe real est√£o alocados no mesmo cluster.

- Intervalo: [0, 1] (quanto mais pr√≥ximo de 1, mais completo).


$$\text{Completude }(c) = 1 - \dfrac{H(K|C)}{H(K)}$$

em que:

$C$ = classes verdadeiras, $K$ = clusters atribu√≠dos, $H(C|K)$ = entropia condicional dos clusters dado as classes, $H(C)$ = entropia dos clusters


- **Alta completude** ‚Üí cada classe real est√° concentrada em um cluster.




## Avalia√ß√£o Relativa



- **Elbow Method**: ponto de "cotovelo" indica n√∫mero ideal de clusters.


![](/images/cluster/elbow.png){.nostretch fig-align="center" width="600px"}

## Avalia√ß√£o Relativa


- **Gap Statistic**: compara a soma das dist√¢ncias intra-cluster (WCSS) com valor esperado sob aleatoriedade.

$$Gap(k) = E[\log(W_k)] - \log(W_k)$$

em que:

-$W_k:$  soma intra-cluster ($WCSS$) para $k$ clusters e $E[\log(W_k)]$ √© valor esperado sob dados aleat√≥rios (refer√™ncia).


- O melhor n√∫mero de clusters √© aquele com maior $Gap(k)$


## Avalia√ß√£o Visual

<br>

- PCA (Principal Component Analysis)
    - T√©cnica linear de redu√ß√£o de dimensionalidade
    - Projeta dados em componentes principais que explicam maior vari√¢ncia
    - Mant√©m estrutura global dos dados


- **Limita√ß√£o:** n√£o captura rela√ß√µes n√£o lineares entre vari√°veis


## Avalia√ß√£o Visual

- t-SNE (t-Distributed Stochastic Neighbor Embedding)
    - Preserva rela√ß√µes de vizinhan√ßa local
    - Muito bom para visualizar clusters complexos em 2D ou 3D
    - Mais lento e sens√≠vel a hiperpar√¢metros

. . .

- UMAP (Uniform Manifold Approximation and Projection)
    - Preserva estrutura global e local
    - Mais r√°pido e escal√°vel que t-SNE 
    - Indicado para redu√ß√£o de dimensionalidade antes de clusteriza√ß√£o


## Avalia√ß√£o Visual

<br>

- Uso t√≠pico:
    - Reduzir dados de alta dimens√£o para 2D/3D
    - Plotar clusters coloridos para inspe√ß√£o visual 
    - Avaliar separabilidade e sobreposi√ß√£o entre grupos
